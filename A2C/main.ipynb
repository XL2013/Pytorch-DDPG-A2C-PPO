{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "import gym\n",
    "from envs import make_env\n",
    "from subproc_vec_env import SubprocVecEnv\n",
    "from model import CNNPolicy\n",
    "from storage import RolloutStorage\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "#log args\n",
    "log_dir = './log'\n",
    "log_interval = 100\n",
    "\n",
    "# env args\n",
    "env_name = 'BreakoutNoFrameskip-v4'\n",
    "num_processes = 16\n",
    "seed = 1\n",
    "\n",
    "#stack 4 history frame\n",
    "num_stack = 4\n",
    "\n",
    "# RMSprop optimizer args\n",
    "lr = 7e-4\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "# training args\n",
    "num_steps = 5\n",
    "num_frames = 10e6\n",
    "num_updates = int(num_frames) // num_steps // num_processes\n",
    "\n",
    "# a2c algrithm args\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "max_grad_norm = 0.5\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000\n"
     ]
    }
   ],
   "source": [
    "print(num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-19 15:15:54,026] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,032] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,037] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,045] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,053] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,059] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,075] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,080] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,085] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,091] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,100] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,106] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,112] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,118] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,132] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,132] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,348] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,357] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,369] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,369] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,374] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,380] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,402] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,408] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,408] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,411] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,420] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,423] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,433] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,451] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,485] Making new env: BreakoutNoFrameskip-v4\n",
      "[2018-01-19 15:15:54,487] Making new env: BreakoutNoFrameskip-v4\n",
      "Process Process-8:\n",
      "Process Process-4:\n",
      "Process Process-12:\n",
      "Process Process-7:\n",
      "Process Process-9:\n",
      "Process Process-13:\n",
      "Process Process-1:\n",
      "Process Process-6:\n",
      "Process Process-10:\n",
      "Process Process-5:\n",
      "Process Process-14:\n",
      "Process Process-11:\n",
      "Process Process-16:\n",
      "Process Process-15:\n",
      "Process Process-2:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/Pytorch-DDPG-A2C-PPO/A2C/subproc_vec_env.py\", line 10, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/user2/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "envs = [make_env(env_name, seed, rank, log_dir) for rank in range(num_processes)]\n",
    "envs = SubprocVecEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_shape = envs.observation_space.shape\n",
    "obs_shape = [obs_shape[0]*num_stack, *obs_shape[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 84, 84]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor_critic = CNNPolicy(obs_shape[0], envs.action_space, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNPolicy(\n",
       "  (conv1): Conv2d (4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d (32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d (64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (linear1): Linear(in_features=1568, out_features=512)\n",
       "  (critic_linear): Linear(in_features=512, out_features=1)\n",
       "  (dist): Categorical(\n",
       "    (linear): Linear(in_features=512, out_features=4)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    actor_critic.cuda()\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollouts = RolloutStorage(num_steps, num_processes, obs_shape, envs.action_space, actor_critic.state_size)\n",
    "current_obs = torch.zeros(num_processes, *obs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 84, 84])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the oldest obs in current_obs by obs\n",
    "def update_current_obs(obs):\n",
    "    shape_dim0 = envs.observation_space.shape[0]\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    if num_stack > 1:\n",
    "        current_obs[:, :-shape_dim0] = current_obs[:, shape_dim0:]\n",
    "    current_obs[:, -shape_dim0:] = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs = envs.reset()\n",
    "update_current_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rollouts.observations[0].copy_(current_obs)\n",
    "episode_rewards = torch.zeros([num_processes,1])\n",
    "final_rewards = torch.zeros([num_processes,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    rollouts.cuda()\n",
    "    current_obs = current_obs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if envs.action_space.__class__.__name__ == \"Discrete\":\n",
    "    action_shape = 1\n",
    "else:\n",
    "    action_shape = envs.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 84, 84]) torch.Size([16, 1]) torch.Size([16, 1])\n",
      "<class 'torch.cuda.FloatTensor'> <class 'torch.cuda.FloatTensor'> <class 'torch.cuda.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "obs_test = rollouts.observations[0]\n",
    "state_test = rollouts.states[0]\n",
    "mask_test = rollouts.masks[0]\n",
    "print(obs_test.shape,state_test.shape, mask_test.shape)\n",
    "print(type(obs_test),type(state_test), type(mask_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user2/Pytorch-DDPG-A2C-PPO/A2C/distributions.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x)\n",
      "/home/user2/Pytorch-DDPG-A2C-PPO/A2C/distributions.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(x)\n",
      "/home/user2/Pytorch-DDPG-A2C-PPO/A2C/distributions.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[0], volatile=True),\n",
    "                                                          Variable(rollouts.states[0], volatile=True),\n",
    "                                                          Variable(rollouts.masks[0], volatile=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user2/Pytorch-DDPG-A2C-PPO/A2C/distributions.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x)\n",
      "/home/user2/Pytorch-DDPG-A2C-PPO/A2C/distributions.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(x)\n",
      "/home/user2/Pytorch-DDPG-A2C-PPO/A2C/distributions.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 80, FPS 757, mean/median reward 0.0/0.0, min/max reward 0.0/0.0, entropy 1.37727, value loss 0.00249, policy loss 0.04527\n",
      "Updates 100, num timesteps 8080, FPS 1475, mean/median reward 0.2/0.0, min/max reward 0.0/3.0, entropy 1.38524, value loss 0.00074, policy loss -0.01315\n",
      "Updates 200, num timesteps 16080, FPS 1429, mean/median reward 0.2/0.0, min/max reward 0.0/2.0, entropy 1.38576, value loss 0.02414, policy loss 0.02908\n",
      "Updates 300, num timesteps 24080, FPS 1404, mean/median reward 0.2/0.0, min/max reward 0.0/1.0, entropy 1.38293, value loss 0.04014, policy loss 0.05686\n",
      "Updates 400, num timesteps 32080, FPS 1394, mean/median reward 0.1/0.0, min/max reward 0.0/1.0, entropy 1.37919, value loss 0.07455, policy loss 0.13182\n",
      "Updates 500, num timesteps 40080, FPS 1379, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.37522, value loss 0.01414, policy loss -0.06204\n",
      "Updates 600, num timesteps 48080, FPS 1387, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.37817, value loss 0.01237, policy loss 0.04205\n",
      "Updates 700, num timesteps 56080, FPS 1388, mean/median reward 0.2/0.0, min/max reward 0.0/2.0, entropy 1.37190, value loss 0.01450, policy loss -0.09322\n",
      "Updates 800, num timesteps 64080, FPS 1388, mean/median reward 0.5/0.0, min/max reward 0.0/2.0, entropy 1.36414, value loss 0.01230, policy loss 0.03440\n",
      "Updates 900, num timesteps 72080, FPS 1393, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.35139, value loss 0.00485, policy loss -0.00337\n",
      "Updates 1000, num timesteps 80080, FPS 1387, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.33144, value loss 0.00188, policy loss 0.00376\n",
      "Updates 1100, num timesteps 88080, FPS 1396, mean/median reward 0.3/0.0, min/max reward 0.0/2.0, entropy 1.34184, value loss 0.00500, policy loss -0.00928\n",
      "Updates 1200, num timesteps 96080, FPS 1404, mean/median reward 0.6/0.0, min/max reward 0.0/2.0, entropy 1.37230, value loss 0.01008, policy loss -0.01746\n",
      "Updates 1300, num timesteps 104080, FPS 1410, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.35961, value loss 0.00094, policy loss 0.00642\n",
      "Updates 1400, num timesteps 112080, FPS 1420, mean/median reward 0.5/0.0, min/max reward 0.0/2.0, entropy 1.35095, value loss 0.00932, policy loss 0.00565\n",
      "Updates 1500, num timesteps 120080, FPS 1421, mean/median reward 0.7/0.0, min/max reward 0.0/2.0, entropy 1.37212, value loss 0.00094, policy loss -0.00356\n",
      "Updates 1600, num timesteps 128080, FPS 1419, mean/median reward 0.6/1.0, min/max reward 0.0/2.0, entropy 1.37093, value loss 0.00970, policy loss -0.01744\n",
      "Updates 1700, num timesteps 136080, FPS 1420, mean/median reward 0.3/0.0, min/max reward 0.0/2.0, entropy 1.37451, value loss 0.00453, policy loss 0.01166\n",
      "Updates 1800, num timesteps 144080, FPS 1421, mean/median reward 0.5/0.0, min/max reward 0.0/3.0, entropy 1.37054, value loss 0.00343, policy loss -0.01288\n",
      "Updates 1900, num timesteps 152080, FPS 1417, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.35318, value loss 0.00702, policy loss 0.01302\n",
      "Updates 2000, num timesteps 160080, FPS 1420, mean/median reward 0.8/1.0, min/max reward 0.0/2.0, entropy 1.33816, value loss 0.02879, policy loss 0.04423\n",
      "Updates 2100, num timesteps 168080, FPS 1420, mean/median reward 0.5/0.0, min/max reward 0.0/2.0, entropy 1.34373, value loss 0.00978, policy loss -0.01228\n",
      "Updates 2200, num timesteps 176080, FPS 1423, mean/median reward 0.7/0.0, min/max reward 0.0/2.0, entropy 1.33344, value loss 0.00680, policy loss -0.00947\n",
      "Updates 2300, num timesteps 184080, FPS 1428, mean/median reward 0.6/0.0, min/max reward 0.0/2.0, entropy 1.28469, value loss 0.00268, policy loss -0.02277\n",
      "Updates 2400, num timesteps 192080, FPS 1432, mean/median reward 0.9/0.0, min/max reward 0.0/3.0, entropy 1.35221, value loss 0.00900, policy loss -0.03518\n",
      "Updates 2500, num timesteps 200080, FPS 1431, mean/median reward 0.6/1.0, min/max reward 0.0/2.0, entropy 1.30289, value loss 0.00740, policy loss -0.04112\n",
      "Updates 2600, num timesteps 208080, FPS 1434, mean/median reward 0.8/1.0, min/max reward 0.0/2.0, entropy 1.33724, value loss 0.01776, policy loss -0.06068\n",
      "Updates 2700, num timesteps 216080, FPS 1440, mean/median reward 1.0/1.0, min/max reward 0.0/2.0, entropy 1.32887, value loss 0.01456, policy loss -0.03681\n",
      "Updates 2800, num timesteps 224080, FPS 1443, mean/median reward 0.8/1.0, min/max reward 0.0/2.0, entropy 1.32422, value loss 0.00854, policy loss -0.03378\n",
      "Updates 2900, num timesteps 232080, FPS 1448, mean/median reward 0.6/0.0, min/max reward 0.0/2.0, entropy 1.32182, value loss 0.00991, policy loss 0.02010\n",
      "Updates 3000, num timesteps 240080, FPS 1450, mean/median reward 0.9/1.0, min/max reward 0.0/4.0, entropy 1.28375, value loss 0.01462, policy loss -0.04997\n",
      "Updates 3100, num timesteps 248080, FPS 1455, mean/median reward 1.4/1.0, min/max reward 0.0/4.0, entropy 1.21771, value loss 0.02143, policy loss 0.00548\n",
      "Updates 3200, num timesteps 256080, FPS 1460, mean/median reward 0.8/1.0, min/max reward 0.0/3.0, entropy 1.21229, value loss 0.02402, policy loss -0.00076\n",
      "Updates 3300, num timesteps 264080, FPS 1469, mean/median reward 1.3/1.0, min/max reward 0.0/3.0, entropy 1.22308, value loss 0.07969, policy loss -0.01247\n",
      "Updates 3400, num timesteps 272080, FPS 1473, mean/median reward 1.4/1.0, min/max reward 0.0/3.0, entropy 1.15423, value loss 0.02342, policy loss 0.03041\n",
      "Updates 3500, num timesteps 280080, FPS 1480, mean/median reward 1.2/1.0, min/max reward 0.0/3.0, entropy 1.13981, value loss 0.02603, policy loss -0.02390\n",
      "Updates 3600, num timesteps 288080, FPS 1485, mean/median reward 1.1/1.0, min/max reward 0.0/2.0, entropy 1.13808, value loss 0.01069, policy loss 0.06203\n",
      "Updates 3700, num timesteps 296080, FPS 1491, mean/median reward 1.3/1.0, min/max reward 0.0/3.0, entropy 1.18264, value loss 0.02686, policy loss -0.03149\n",
      "Updates 3800, num timesteps 304080, FPS 1495, mean/median reward 1.2/1.0, min/max reward 0.0/4.0, entropy 1.23320, value loss 0.01906, policy loss -0.03287\n",
      "Updates 3900, num timesteps 312080, FPS 1501, mean/median reward 1.4/1.0, min/max reward 0.0/4.0, entropy 1.21254, value loss 0.04279, policy loss -0.02079\n",
      "Updates 4000, num timesteps 320080, FPS 1506, mean/median reward 1.9/2.0, min/max reward 0.0/5.0, entropy 1.24259, value loss 0.04839, policy loss 0.06505\n",
      "Updates 4100, num timesteps 328080, FPS 1514, mean/median reward 1.2/1.0, min/max reward 0.0/3.0, entropy 1.18746, value loss 0.01498, policy loss 0.01893\n",
      "Updates 4200, num timesteps 336080, FPS 1522, mean/median reward 1.9/1.0, min/max reward 0.0/4.0, entropy 1.17049, value loss 0.02830, policy loss 0.03154\n",
      "Updates 4300, num timesteps 344080, FPS 1529, mean/median reward 1.7/1.0, min/max reward 0.0/4.0, entropy 1.09244, value loss 0.06100, policy loss 0.01042\n",
      "Updates 4400, num timesteps 352080, FPS 1533, mean/median reward 1.8/1.0, min/max reward 0.0/5.0, entropy 1.10405, value loss 0.04345, policy loss -0.08029\n",
      "Updates 4500, num timesteps 360080, FPS 1537, mean/median reward 2.1/2.0, min/max reward 1.0/5.0, entropy 1.02724, value loss 0.05699, policy loss -0.04515\n",
      "Updates 4600, num timesteps 368080, FPS 1543, mean/median reward 2.0/2.0, min/max reward 1.0/6.0, entropy 1.19104, value loss 0.03669, policy loss -0.07182\n",
      "Updates 4700, num timesteps 376080, FPS 1549, mean/median reward 2.9/2.0, min/max reward 1.0/6.0, entropy 1.12991, value loss 0.05600, policy loss -0.07225\n",
      "Updates 4800, num timesteps 384080, FPS 1556, mean/median reward 1.3/1.0, min/max reward 0.0/4.0, entropy 1.08873, value loss 0.03744, policy loss -0.03747\n",
      "Updates 4900, num timesteps 392080, FPS 1562, mean/median reward 3.1/2.0, min/max reward 0.0/11.0, entropy 0.99719, value loss 0.04005, policy loss -0.01438\n",
      "Updates 5000, num timesteps 400080, FPS 1568, mean/median reward 1.8/2.0, min/max reward 1.0/4.0, entropy 1.20585, value loss 0.04014, policy loss 0.00321\n",
      "Updates 5100, num timesteps 408080, FPS 1573, mean/median reward 2.2/2.0, min/max reward 0.0/5.0, entropy 1.12536, value loss 0.07153, policy loss 0.01727\n",
      "Updates 5200, num timesteps 416080, FPS 1579, mean/median reward 2.1/2.0, min/max reward 1.0/5.0, entropy 1.09202, value loss 0.01813, policy loss 0.01671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 5300, num timesteps 424080, FPS 1584, mean/median reward 1.6/1.0, min/max reward 0.0/5.0, entropy 1.12668, value loss 0.02410, policy loss -0.05357\n",
      "Updates 5400, num timesteps 432080, FPS 1589, mean/median reward 2.1/2.0, min/max reward 0.0/6.0, entropy 0.99991, value loss 0.04498, policy loss -0.01688\n",
      "Updates 5500, num timesteps 440080, FPS 1597, mean/median reward 1.6/1.0, min/max reward 0.0/4.0, entropy 1.21710, value loss 0.03181, policy loss -0.04328\n",
      "Updates 5600, num timesteps 448080, FPS 1600, mean/median reward 1.8/1.0, min/max reward 0.0/5.0, entropy 1.17989, value loss 0.07478, policy loss -0.12069\n",
      "Updates 5700, num timesteps 456080, FPS 1606, mean/median reward 2.5/2.0, min/max reward 1.0/5.0, entropy 1.17957, value loss 0.07967, policy loss -0.13259\n",
      "Updates 5800, num timesteps 464080, FPS 1612, mean/median reward 2.8/2.0, min/max reward 0.0/7.0, entropy 1.10268, value loss 0.02526, policy loss -0.00654\n",
      "Updates 5900, num timesteps 472080, FPS 1619, mean/median reward 3.6/4.0, min/max reward 0.0/8.0, entropy 0.84163, value loss 0.04355, policy loss 0.01883\n",
      "Updates 6000, num timesteps 480080, FPS 1625, mean/median reward 3.3/2.0, min/max reward 1.0/12.0, entropy 0.93931, value loss 0.04410, policy loss -0.00177\n",
      "Updates 6100, num timesteps 488080, FPS 1630, mean/median reward 3.6/3.0, min/max reward 1.0/7.0, entropy 1.02313, value loss 0.02975, policy loss 0.06681\n",
      "Updates 6200, num timesteps 496080, FPS 1632, mean/median reward 2.4/2.0, min/max reward 0.0/6.0, entropy 1.04143, value loss 0.04582, policy loss -0.01770\n",
      "Updates 6300, num timesteps 504080, FPS 1638, mean/median reward 3.7/3.0, min/max reward 1.0/7.0, entropy 1.09225, value loss 0.05779, policy loss -0.09964\n",
      "Updates 6400, num timesteps 512080, FPS 1641, mean/median reward 2.6/2.0, min/max reward 1.0/7.0, entropy 1.09646, value loss 0.05344, policy loss -0.06657\n",
      "Updates 6500, num timesteps 520080, FPS 1646, mean/median reward 2.6/2.0, min/max reward 1.0/7.0, entropy 1.08228, value loss 0.05101, policy loss -0.00411\n",
      "Updates 6600, num timesteps 528080, FPS 1651, mean/median reward 2.9/2.0, min/max reward 1.0/6.0, entropy 1.02024, value loss 0.06032, policy loss 0.03938\n",
      "Updates 6700, num timesteps 536080, FPS 1652, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 0.91834, value loss 0.04781, policy loss -0.02124\n",
      "Updates 6800, num timesteps 544080, FPS 1654, mean/median reward 3.5/3.0, min/max reward 1.0/7.0, entropy 1.03060, value loss 0.04394, policy loss 0.01071\n",
      "Updates 6900, num timesteps 552080, FPS 1658, mean/median reward 4.1/3.0, min/max reward 1.0/9.0, entropy 1.06787, value loss 0.04475, policy loss 0.03092\n",
      "Updates 7000, num timesteps 560080, FPS 1662, mean/median reward 3.4/3.0, min/max reward 1.0/7.0, entropy 1.03135, value loss 0.02470, policy loss -0.01762\n",
      "Updates 7100, num timesteps 568080, FPS 1665, mean/median reward 3.1/2.0, min/max reward 1.0/7.0, entropy 0.97863, value loss 0.03080, policy loss 0.06370\n",
      "Updates 7200, num timesteps 576080, FPS 1668, mean/median reward 4.8/4.0, min/max reward 0.0/13.0, entropy 1.12645, value loss 0.13208, policy loss -0.02844\n",
      "Updates 7300, num timesteps 584080, FPS 1672, mean/median reward 2.2/2.0, min/max reward 0.0/6.0, entropy 1.11095, value loss 0.03314, policy loss -0.09748\n",
      "Updates 7400, num timesteps 592080, FPS 1676, mean/median reward 2.8/3.0, min/max reward 1.0/5.0, entropy 0.91970, value loss 0.04514, policy loss 0.00541\n",
      "Updates 7500, num timesteps 600080, FPS 1682, mean/median reward 3.4/2.0, min/max reward 1.0/7.0, entropy 1.03446, value loss 0.05663, policy loss 0.01133\n",
      "Updates 7600, num timesteps 608080, FPS 1685, mean/median reward 5.2/5.0, min/max reward 1.0/15.0, entropy 0.96983, value loss 0.05202, policy loss -0.02210\n",
      "Updates 7700, num timesteps 616080, FPS 1689, mean/median reward 3.6/2.0, min/max reward 1.0/18.0, entropy 1.11349, value loss 0.05202, policy loss -0.03157\n",
      "Updates 7800, num timesteps 624080, FPS 1692, mean/median reward 3.4/3.0, min/max reward 0.0/9.0, entropy 1.05732, value loss 0.06126, policy loss -0.03694\n",
      "Updates 7900, num timesteps 632080, FPS 1699, mean/median reward 4.3/3.0, min/max reward 0.0/14.0, entropy 1.05903, value loss 0.05445, policy loss -0.02266\n",
      "Updates 8000, num timesteps 640080, FPS 1702, mean/median reward 4.4/4.0, min/max reward 0.0/13.0, entropy 1.08215, value loss 0.05056, policy loss 0.07275\n",
      "Updates 8100, num timesteps 648080, FPS 1708, mean/median reward 3.8/4.0, min/max reward 0.0/9.0, entropy 0.96337, value loss 0.05031, policy loss 0.03035\n",
      "Updates 8200, num timesteps 656080, FPS 1712, mean/median reward 4.6/4.0, min/max reward 1.0/13.0, entropy 0.91846, value loss 0.07986, policy loss -0.02347\n",
      "Updates 8300, num timesteps 664080, FPS 1717, mean/median reward 4.4/4.0, min/max reward 0.0/14.0, entropy 0.91369, value loss 0.01756, policy loss 0.03963\n",
      "Updates 8400, num timesteps 672080, FPS 1721, mean/median reward 3.6/3.0, min/max reward 1.0/13.0, entropy 1.18327, value loss 0.01572, policy loss -0.02659\n",
      "Updates 8500, num timesteps 680080, FPS 1725, mean/median reward 5.2/5.0, min/max reward 0.0/12.0, entropy 0.99893, value loss 0.05384, policy loss -0.08789\n",
      "Updates 8600, num timesteps 688080, FPS 1729, mean/median reward 5.6/5.0, min/max reward 0.0/15.0, entropy 0.98429, value loss 0.03532, policy loss -0.02733\n",
      "Updates 8700, num timesteps 696080, FPS 1732, mean/median reward 4.8/4.0, min/max reward 1.0/12.0, entropy 1.06942, value loss 0.02316, policy loss -0.03289\n",
      "Updates 8800, num timesteps 704080, FPS 1735, mean/median reward 3.6/3.0, min/max reward 1.0/7.0, entropy 1.01806, value loss 0.04252, policy loss -0.00505\n",
      "Updates 8900, num timesteps 712080, FPS 1737, mean/median reward 3.9/4.0, min/max reward 1.0/7.0, entropy 0.92697, value loss 0.03067, policy loss -0.03575\n",
      "Updates 9000, num timesteps 720080, FPS 1740, mean/median reward 5.8/4.0, min/max reward 0.0/15.0, entropy 0.98139, value loss 0.13721, policy loss -0.14851\n",
      "Updates 9100, num timesteps 728080, FPS 1743, mean/median reward 4.7/3.0, min/max reward 0.0/18.0, entropy 0.97729, value loss 0.01429, policy loss 0.02326\n",
      "Updates 9200, num timesteps 736080, FPS 1748, mean/median reward 6.1/5.0, min/max reward 1.0/14.0, entropy 1.03089, value loss 0.07814, policy loss 0.05830\n",
      "Updates 9300, num timesteps 744080, FPS 1751, mean/median reward 6.2/6.0, min/max reward 0.0/13.0, entropy 0.98854, value loss 0.01786, policy loss -0.02860\n",
      "Updates 9400, num timesteps 752080, FPS 1754, mean/median reward 4.9/4.0, min/max reward 1.0/18.0, entropy 0.97506, value loss 0.08690, policy loss 0.07583\n",
      "Updates 9500, num timesteps 760080, FPS 1757, mean/median reward 4.9/4.0, min/max reward 1.0/16.0, entropy 0.95848, value loss 0.01844, policy loss -0.01536\n",
      "Updates 9600, num timesteps 768080, FPS 1760, mean/median reward 6.1/5.0, min/max reward 2.0/13.0, entropy 1.00711, value loss 0.10151, policy loss -0.04940\n",
      "Updates 9700, num timesteps 776080, FPS 1764, mean/median reward 5.4/4.0, min/max reward 0.0/21.0, entropy 0.99305, value loss 0.02496, policy loss 0.01526\n",
      "Updates 9800, num timesteps 784080, FPS 1766, mean/median reward 5.5/4.0, min/max reward 0.0/14.0, entropy 1.12136, value loss 0.31310, policy loss -0.08339\n",
      "Updates 9900, num timesteps 792080, FPS 1768, mean/median reward 5.2/4.0, min/max reward 0.0/14.0, entropy 0.94644, value loss 0.03173, policy loss 0.01127\n",
      "Updates 10000, num timesteps 800080, FPS 1772, mean/median reward 5.4/4.0, min/max reward 1.0/14.0, entropy 0.88136, value loss 0.04010, policy loss 0.03435\n",
      "Updates 10100, num timesteps 808080, FPS 1774, mean/median reward 4.6/4.0, min/max reward 0.0/13.0, entropy 0.88228, value loss 0.03294, policy loss 0.06285\n",
      "Updates 10200, num timesteps 816080, FPS 1778, mean/median reward 4.2/4.0, min/max reward 1.0/10.0, entropy 0.93116, value loss 0.06547, policy loss 0.08978\n",
      "Updates 10300, num timesteps 824080, FPS 1781, mean/median reward 7.3/6.0, min/max reward 2.0/20.0, entropy 1.05945, value loss 0.02772, policy loss -0.03191\n",
      "Updates 10400, num timesteps 832080, FPS 1784, mean/median reward 5.4/5.0, min/max reward 1.0/15.0, entropy 1.06790, value loss 0.06553, policy loss 0.02725\n",
      "Updates 10500, num timesteps 840080, FPS 1787, mean/median reward 3.9/3.0, min/max reward 1.0/12.0, entropy 1.02193, value loss 0.00889, policy loss 0.00490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 10600, num timesteps 848080, FPS 1789, mean/median reward 5.2/4.0, min/max reward 1.0/13.0, entropy 0.96034, value loss 0.09102, policy loss 0.01129\n",
      "Updates 10700, num timesteps 856080, FPS 1792, mean/median reward 4.1/4.0, min/max reward 0.0/10.0, entropy 1.01030, value loss 0.15047, policy loss -0.05358\n",
      "Updates 10800, num timesteps 864080, FPS 1794, mean/median reward 6.5/4.0, min/max reward 1.0/16.0, entropy 0.92909, value loss 0.02393, policy loss 0.02686\n",
      "Updates 10900, num timesteps 872080, FPS 1797, mean/median reward 4.5/4.0, min/max reward 1.0/9.0, entropy 0.95681, value loss 0.03261, policy loss -0.03601\n",
      "Updates 11000, num timesteps 880080, FPS 1800, mean/median reward 5.6/5.0, min/max reward 1.0/15.0, entropy 1.04082, value loss 0.05250, policy loss -0.00796\n",
      "Updates 11100, num timesteps 888080, FPS 1803, mean/median reward 4.8/3.0, min/max reward 1.0/12.0, entropy 0.96239, value loss 0.04218, policy loss 0.00977\n",
      "Updates 11200, num timesteps 896080, FPS 1806, mean/median reward 5.8/5.0, min/max reward 1.0/16.0, entropy 1.03454, value loss 0.02755, policy loss 0.06770\n",
      "Updates 11300, num timesteps 904080, FPS 1808, mean/median reward 5.8/4.0, min/max reward 1.0/17.0, entropy 1.05876, value loss 0.09016, policy loss -0.01450\n",
      "Updates 11400, num timesteps 912080, FPS 1810, mean/median reward 4.0/4.0, min/max reward 1.0/12.0, entropy 1.06543, value loss 0.01069, policy loss -0.00416\n",
      "Updates 11500, num timesteps 920080, FPS 1812, mean/median reward 6.8/4.0, min/max reward 1.0/18.0, entropy 0.99948, value loss 0.17145, policy loss -0.08131\n",
      "Updates 11600, num timesteps 928080, FPS 1813, mean/median reward 4.8/5.0, min/max reward 1.0/12.0, entropy 1.07815, value loss 0.03561, policy loss 0.06968\n",
      "Updates 11700, num timesteps 936080, FPS 1816, mean/median reward 5.5/4.0, min/max reward 1.0/16.0, entropy 1.08226, value loss 0.07400, policy loss 0.05697\n",
      "Updates 11800, num timesteps 944080, FPS 1818, mean/median reward 7.4/4.0, min/max reward 0.0/33.0, entropy 0.96876, value loss 0.10348, policy loss -0.07483\n",
      "Updates 11900, num timesteps 952080, FPS 1819, mean/median reward 4.6/3.0, min/max reward 1.0/12.0, entropy 0.96736, value loss 0.04781, policy loss 0.02406\n",
      "Updates 12000, num timesteps 960080, FPS 1821, mean/median reward 7.9/6.0, min/max reward 2.0/16.0, entropy 1.04777, value loss 0.10013, policy loss 0.03942\n",
      "Updates 12100, num timesteps 968080, FPS 1824, mean/median reward 9.6/8.0, min/max reward 1.0/21.0, entropy 0.99325, value loss 0.01175, policy loss 0.01731\n",
      "Updates 12200, num timesteps 976080, FPS 1825, mean/median reward 8.6/7.0, min/max reward 1.0/18.0, entropy 0.95946, value loss 0.01506, policy loss -0.00746\n",
      "Updates 12300, num timesteps 984080, FPS 1828, mean/median reward 4.1/1.0, min/max reward 0.0/15.0, entropy 0.40160, value loss 0.32191, policy loss 0.11181\n",
      "Updates 12400, num timesteps 992080, FPS 1825, mean/median reward 1.9/1.0, min/max reward 0.0/7.0, entropy 0.96168, value loss 0.04526, policy loss 0.01165\n",
      "Updates 12500, num timesteps 1000080, FPS 1828, mean/median reward 6.1/5.0, min/max reward 0.0/15.0, entropy 1.00723, value loss 0.01938, policy loss 0.00757\n",
      "Updates 12600, num timesteps 1008080, FPS 1829, mean/median reward 6.6/5.0, min/max reward 1.0/15.0, entropy 1.06821, value loss 0.12920, policy loss -0.00843\n",
      "Updates 12700, num timesteps 1016080, FPS 1831, mean/median reward 8.3/6.0, min/max reward 1.0/20.0, entropy 1.06882, value loss 0.01976, policy loss 0.00163\n",
      "Updates 12800, num timesteps 1024080, FPS 1832, mean/median reward 6.1/6.0, min/max reward 0.0/18.0, entropy 1.03374, value loss 0.04239, policy loss 0.07074\n",
      "Updates 12900, num timesteps 1032080, FPS 1833, mean/median reward 4.2/4.0, min/max reward 0.0/12.0, entropy 1.03381, value loss 0.18366, policy loss -0.12089\n",
      "Updates 13000, num timesteps 1040080, FPS 1835, mean/median reward 5.9/5.0, min/max reward 1.0/21.0, entropy 1.09071, value loss 0.02499, policy loss 0.00392\n",
      "Updates 13100, num timesteps 1048080, FPS 1831, mean/median reward 0.4/0.0, min/max reward 0.0/2.0, entropy 1.06403, value loss 0.04575, policy loss 0.00081\n",
      "Updates 13200, num timesteps 1056080, FPS 1833, mean/median reward 6.2/5.0, min/max reward 1.0/15.0, entropy 0.96829, value loss 0.04581, policy loss 0.01648\n",
      "Updates 13300, num timesteps 1064080, FPS 1835, mean/median reward 5.4/4.0, min/max reward 1.0/20.0, entropy 1.01589, value loss 0.02918, policy loss 0.01403\n",
      "Updates 13400, num timesteps 1072080, FPS 1836, mean/median reward 7.5/5.0, min/max reward 0.0/24.0, entropy 1.01841, value loss 0.02069, policy loss -0.00343\n",
      "Updates 13500, num timesteps 1080080, FPS 1838, mean/median reward 12.8/11.0, min/max reward 1.0/24.0, entropy 1.08807, value loss 0.04082, policy loss -0.03180\n",
      "Updates 13600, num timesteps 1088080, FPS 1840, mean/median reward 8.6/6.0, min/max reward 0.0/24.0, entropy 1.07192, value loss 0.13894, policy loss -0.06823\n",
      "Updates 13700, num timesteps 1096080, FPS 1842, mean/median reward 5.6/5.0, min/max reward 2.0/12.0, entropy 1.03312, value loss 0.03221, policy loss 0.00075\n",
      "Updates 13800, num timesteps 1104080, FPS 1844, mean/median reward 3.5/2.0, min/max reward 1.0/13.0, entropy 0.95933, value loss 0.01767, policy loss 0.01927\n",
      "Updates 13900, num timesteps 1112080, FPS 1845, mean/median reward 5.2/4.0, min/max reward 0.0/17.0, entropy 1.10746, value loss 0.20148, policy loss -0.02222\n",
      "Updates 14000, num timesteps 1120080, FPS 1847, mean/median reward 5.6/5.0, min/max reward 1.0/15.0, entropy 1.00884, value loss 0.04413, policy loss 0.01622\n",
      "Updates 14100, num timesteps 1128080, FPS 1848, mean/median reward 6.5/5.0, min/max reward 1.0/15.0, entropy 1.09339, value loss 0.01525, policy loss 0.00275\n",
      "Updates 14200, num timesteps 1136080, FPS 1849, mean/median reward 7.4/5.0, min/max reward 1.0/15.0, entropy 0.93698, value loss 0.02735, policy loss -0.06290\n",
      "Updates 14300, num timesteps 1144080, FPS 1851, mean/median reward 6.9/4.0, min/max reward 1.0/22.0, entropy 1.04181, value loss 0.03573, policy loss -0.06527\n",
      "Updates 14400, num timesteps 1152080, FPS 1854, mean/median reward 6.9/5.0, min/max reward 1.0/22.0, entropy 1.05812, value loss 0.00920, policy loss -0.03452\n",
      "Updates 14500, num timesteps 1160080, FPS 1855, mean/median reward 7.9/5.0, min/max reward 1.0/21.0, entropy 0.98510, value loss 0.02734, policy loss -0.01873\n",
      "Updates 14600, num timesteps 1168080, FPS 1858, mean/median reward 8.1/6.0, min/max reward 1.0/21.0, entropy 0.94144, value loss 0.02364, policy loss 0.02814\n",
      "Updates 14700, num timesteps 1176080, FPS 1861, mean/median reward 8.2/6.0, min/max reward 0.0/23.0, entropy 1.01867, value loss 0.10676, policy loss -0.03197\n",
      "Updates 14800, num timesteps 1184080, FPS 1862, mean/median reward 5.8/5.0, min/max reward 0.0/12.0, entropy 0.86287, value loss 0.20665, policy loss -0.02754\n",
      "Updates 14900, num timesteps 1192080, FPS 1864, mean/median reward 3.4/3.0, min/max reward 1.0/7.0, entropy 1.07575, value loss 0.02033, policy loss 0.01018\n",
      "Updates 15000, num timesteps 1200080, FPS 1866, mean/median reward 8.1/6.0, min/max reward 1.0/18.0, entropy 0.98691, value loss 0.04220, policy loss -0.04709\n",
      "Updates 15100, num timesteps 1208080, FPS 1868, mean/median reward 6.4/5.0, min/max reward 1.0/24.0, entropy 1.10868, value loss 0.02262, policy loss -0.00119\n",
      "Updates 15200, num timesteps 1216080, FPS 1869, mean/median reward 7.5/5.0, min/max reward 2.0/23.0, entropy 0.96737, value loss 0.01631, policy loss 0.02929\n",
      "Updates 15300, num timesteps 1224080, FPS 1871, mean/median reward 6.9/5.0, min/max reward 2.0/18.0, entropy 0.86981, value loss 0.13464, policy loss -0.09849\n",
      "Updates 15400, num timesteps 1232080, FPS 1873, mean/median reward 6.3/3.0, min/max reward 0.0/23.0, entropy 1.00462, value loss 0.04178, policy loss 0.06198\n",
      "Updates 15500, num timesteps 1240080, FPS 1874, mean/median reward 3.6/2.0, min/max reward 1.0/11.0, entropy 0.98752, value loss 0.18139, policy loss -0.16215\n",
      "Updates 15600, num timesteps 1248080, FPS 1876, mean/median reward 4.9/3.0, min/max reward 1.0/21.0, entropy 0.93441, value loss 0.01502, policy loss -0.01485\n",
      "Updates 15700, num timesteps 1256080, FPS 1877, mean/median reward 8.4/7.0, min/max reward 1.0/30.0, entropy 1.02932, value loss 0.01622, policy loss -0.04257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 15800, num timesteps 1264080, FPS 1879, mean/median reward 5.9/4.0, min/max reward 0.0/20.0, entropy 0.92801, value loss 0.04350, policy loss -0.01925\n",
      "Updates 15900, num timesteps 1272080, FPS 1880, mean/median reward 7.7/6.0, min/max reward 1.0/33.0, entropy 1.03699, value loss 0.03446, policy loss 0.02422\n",
      "Updates 16000, num timesteps 1280080, FPS 1882, mean/median reward 7.9/5.0, min/max reward 2.0/30.0, entropy 1.05538, value loss 0.03893, policy loss -0.09876\n",
      "Updates 16100, num timesteps 1288080, FPS 1884, mean/median reward 9.4/5.0, min/max reward 1.0/28.0, entropy 0.92339, value loss 0.04171, policy loss 0.00754\n",
      "Updates 16200, num timesteps 1296080, FPS 1887, mean/median reward 5.1/2.0, min/max reward 0.0/15.0, entropy 1.02342, value loss 0.21763, policy loss -0.07242\n",
      "Updates 16300, num timesteps 1304080, FPS 1888, mean/median reward 7.4/5.0, min/max reward 0.0/18.0, entropy 1.00406, value loss 0.01467, policy loss 0.03208\n",
      "Updates 16400, num timesteps 1312080, FPS 1890, mean/median reward 9.1/5.0, min/max reward 0.0/35.0, entropy 1.03355, value loss 0.06607, policy loss -0.03104\n",
      "Updates 16500, num timesteps 1320080, FPS 1892, mean/median reward 9.2/6.0, min/max reward 1.0/29.0, entropy 1.02165, value loss 0.14192, policy loss -0.16584\n",
      "Updates 16600, num timesteps 1328080, FPS 1893, mean/median reward 7.0/6.0, min/max reward 1.0/20.0, entropy 1.01742, value loss 0.60195, policy loss -0.26687\n",
      "Updates 16700, num timesteps 1336080, FPS 1895, mean/median reward 9.0/6.0, min/max reward 1.0/37.0, entropy 1.07377, value loss 0.03662, policy loss 0.04210\n",
      "Updates 16800, num timesteps 1344080, FPS 1896, mean/median reward 7.8/6.0, min/max reward 1.0/17.0, entropy 1.09249, value loss 0.02286, policy loss -0.00484\n",
      "Updates 16900, num timesteps 1352080, FPS 1897, mean/median reward 11.4/9.0, min/max reward 2.0/31.0, entropy 1.04879, value loss 0.04210, policy loss 0.08510\n",
      "Updates 17000, num timesteps 1360080, FPS 1899, mean/median reward 7.5/5.0, min/max reward 1.0/31.0, entropy 0.93879, value loss 0.02004, policy loss 0.00064\n",
      "Updates 17100, num timesteps 1368080, FPS 1900, mean/median reward 9.6/5.0, min/max reward 0.0/39.0, entropy 1.02323, value loss 0.00693, policy loss -0.00492\n",
      "Updates 17200, num timesteps 1376080, FPS 1902, mean/median reward 8.0/4.0, min/max reward 1.0/22.0, entropy 0.93308, value loss 0.02072, policy loss 0.01949\n",
      "Updates 17300, num timesteps 1384080, FPS 1903, mean/median reward 7.9/5.0, min/max reward 1.0/26.0, entropy 1.06616, value loss 0.35874, policy loss -0.20283\n",
      "Updates 17400, num timesteps 1392080, FPS 1904, mean/median reward 5.2/3.0, min/max reward 0.0/15.0, entropy 0.93135, value loss 0.03473, policy loss 0.04826\n",
      "Updates 17500, num timesteps 1400080, FPS 1904, mean/median reward 7.6/9.0, min/max reward 1.0/17.0, entropy 1.08527, value loss 0.03754, policy loss -0.07228\n",
      "Updates 17600, num timesteps 1408080, FPS 1906, mean/median reward 10.0/7.0, min/max reward 2.0/25.0, entropy 1.02150, value loss 0.04065, policy loss -0.02978\n",
      "Updates 17700, num timesteps 1416080, FPS 1908, mean/median reward 13.8/9.0, min/max reward 1.0/44.0, entropy 0.88956, value loss 0.06172, policy loss 0.09049\n",
      "Updates 17800, num timesteps 1424080, FPS 1908, mean/median reward 6.6/4.0, min/max reward 1.0/23.0, entropy 1.11677, value loss 0.01470, policy loss 0.00730\n",
      "Updates 17900, num timesteps 1432080, FPS 1909, mean/median reward 8.9/5.0, min/max reward 2.0/21.0, entropy 0.90836, value loss 0.09421, policy loss -0.08444\n",
      "Updates 18000, num timesteps 1440080, FPS 1911, mean/median reward 8.3/6.0, min/max reward 1.0/18.0, entropy 0.78507, value loss 0.13532, policy loss -0.00945\n",
      "Updates 18100, num timesteps 1448080, FPS 1911, mean/median reward 13.8/9.0, min/max reward 3.0/39.0, entropy 0.99588, value loss 0.03000, policy loss 0.04227\n",
      "Updates 18200, num timesteps 1456080, FPS 1913, mean/median reward 8.3/6.0, min/max reward 0.0/39.0, entropy 1.11025, value loss 0.02254, policy loss 0.03982\n",
      "Updates 18300, num timesteps 1464080, FPS 1914, mean/median reward 10.5/8.0, min/max reward 1.0/42.0, entropy 1.02325, value loss 0.02135, policy loss 0.03336\n",
      "Updates 18400, num timesteps 1472080, FPS 1914, mean/median reward 10.0/7.0, min/max reward 2.0/34.0, entropy 0.90209, value loss 0.37427, policy loss -0.05311\n",
      "Updates 18500, num timesteps 1480080, FPS 1915, mean/median reward 9.6/7.0, min/max reward 0.0/37.0, entropy 0.93275, value loss 0.02124, policy loss 0.03469\n",
      "Updates 18600, num timesteps 1488080, FPS 1916, mean/median reward 8.2/4.0, min/max reward 0.0/43.0, entropy 0.95564, value loss 0.57404, policy loss -0.12094\n",
      "Updates 18700, num timesteps 1496080, FPS 1917, mean/median reward 4.8/4.0, min/max reward 0.0/14.0, entropy 0.94042, value loss 0.19749, policy loss -0.08669\n",
      "Updates 18800, num timesteps 1504080, FPS 1919, mean/median reward 11.1/7.0, min/max reward 0.0/33.0, entropy 0.88228, value loss 0.16332, policy loss -0.11169\n",
      "Updates 18900, num timesteps 1512080, FPS 1919, mean/median reward 5.9/6.0, min/max reward 0.0/11.0, entropy 0.83434, value loss 0.00906, policy loss 0.02791\n",
      "Updates 19000, num timesteps 1520080, FPS 1920, mean/median reward 7.6/6.0, min/max reward 0.0/18.0, entropy 0.94693, value loss 0.01906, policy loss 0.00383\n",
      "Updates 19100, num timesteps 1528080, FPS 1921, mean/median reward 6.4/4.0, min/max reward 1.0/24.0, entropy 1.04096, value loss 0.02419, policy loss 0.08101\n",
      "Updates 19200, num timesteps 1536080, FPS 1922, mean/median reward 11.4/9.0, min/max reward 2.0/24.0, entropy 0.88306, value loss 0.02954, policy loss -0.01966\n",
      "Updates 19300, num timesteps 1544080, FPS 1922, mean/median reward 1.2/0.0, min/max reward 0.0/6.0, entropy 1.03978, value loss 0.01412, policy loss 0.01638\n",
      "Updates 19400, num timesteps 1552080, FPS 1923, mean/median reward 6.8/2.0, min/max reward 0.0/19.0, entropy 1.02326, value loss 0.03363, policy loss 0.00765\n",
      "Updates 19500, num timesteps 1560080, FPS 1925, mean/median reward 10.6/6.0, min/max reward 0.0/37.0, entropy 1.02712, value loss 0.08150, policy loss -0.06406\n",
      "Updates 19600, num timesteps 1568080, FPS 1925, mean/median reward 6.9/7.0, min/max reward 0.0/19.0, entropy 0.97420, value loss 0.02843, policy loss -0.00588\n",
      "Updates 19700, num timesteps 1576080, FPS 1927, mean/median reward 9.5/7.0, min/max reward 2.0/35.0, entropy 0.88439, value loss 0.18246, policy loss 0.02175\n",
      "Updates 19800, num timesteps 1584080, FPS 1927, mean/median reward 7.6/4.0, min/max reward 0.0/28.0, entropy 1.00931, value loss 0.04270, policy loss -0.00787\n",
      "Updates 19900, num timesteps 1592080, FPS 1928, mean/median reward 6.4/5.0, min/max reward 0.0/16.0, entropy 1.00007, value loss 0.02947, policy loss -0.00164\n",
      "Updates 20000, num timesteps 1600080, FPS 1929, mean/median reward 5.1/4.0, min/max reward 1.0/12.0, entropy 0.94313, value loss 0.10207, policy loss -0.04437\n",
      "Updates 20100, num timesteps 1608080, FPS 1931, mean/median reward 11.3/10.0, min/max reward 1.0/27.0, entropy 1.14616, value loss 0.02753, policy loss 0.02295\n",
      "Updates 20200, num timesteps 1616080, FPS 1931, mean/median reward 7.0/6.0, min/max reward 1.0/17.0, entropy 0.92285, value loss 0.04686, policy loss 0.09705\n",
      "Updates 20300, num timesteps 1624080, FPS 1932, mean/median reward 10.4/4.0, min/max reward 1.0/31.0, entropy 1.08940, value loss 0.02808, policy loss -0.02394\n",
      "Updates 20400, num timesteps 1632080, FPS 1933, mean/median reward 9.2/5.0, min/max reward 0.0/22.0, entropy 1.01151, value loss 0.01921, policy loss 0.00491\n",
      "Updates 20500, num timesteps 1640080, FPS 1934, mean/median reward 6.9/5.0, min/max reward 0.0/21.0, entropy 0.76401, value loss 0.01834, policy loss 0.00996\n",
      "Updates 20600, num timesteps 1648080, FPS 1935, mean/median reward 11.3/7.0, min/max reward 0.0/28.0, entropy 0.87472, value loss 0.03956, policy loss 0.03611\n",
      "Updates 20700, num timesteps 1656080, FPS 1936, mean/median reward 9.1/6.0, min/max reward 0.0/27.0, entropy 1.03718, value loss 0.01012, policy loss 0.00605\n",
      "Updates 20800, num timesteps 1664080, FPS 1936, mean/median reward 10.4/5.0, min/max reward 0.0/34.0, entropy 0.95722, value loss 0.59157, policy loss -0.41494\n",
      "Updates 20900, num timesteps 1672080, FPS 1938, mean/median reward 14.8/10.0, min/max reward 2.0/37.0, entropy 0.91134, value loss 0.22613, policy loss -0.06254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 21000, num timesteps 1680080, FPS 1938, mean/median reward 7.0/5.0, min/max reward 2.0/15.0, entropy 1.12355, value loss 0.01592, policy loss 0.02095\n",
      "Updates 21100, num timesteps 1688080, FPS 1939, mean/median reward 10.1/6.0, min/max reward 1.0/43.0, entropy 1.01558, value loss 0.02040, policy loss 0.02104\n",
      "Updates 21200, num timesteps 1696080, FPS 1939, mean/median reward 11.2/7.0, min/max reward 2.0/41.0, entropy 0.93226, value loss 0.52417, policy loss -0.10814\n",
      "Updates 21300, num timesteps 1704080, FPS 1941, mean/median reward 10.9/7.0, min/max reward 1.0/34.0, entropy 0.99533, value loss 0.01923, policy loss 0.01273\n",
      "Updates 21400, num timesteps 1712080, FPS 1941, mean/median reward 6.4/4.0, min/max reward 1.0/21.0, entropy 0.97012, value loss 0.04476, policy loss -0.06146\n",
      "Updates 21500, num timesteps 1720080, FPS 1943, mean/median reward 11.4/8.0, min/max reward 1.0/38.0, entropy 0.94025, value loss 0.32960, policy loss -0.10961\n",
      "Updates 21600, num timesteps 1728080, FPS 1943, mean/median reward 1.9/2.0, min/max reward 0.0/7.0, entropy 0.91343, value loss 0.02283, policy loss -0.06041\n",
      "Updates 21700, num timesteps 1736080, FPS 1944, mean/median reward 9.9/5.0, min/max reward 0.0/22.0, entropy 0.88999, value loss 0.01133, policy loss -0.02677\n",
      "Updates 21800, num timesteps 1744080, FPS 1945, mean/median reward 15.6/13.0, min/max reward 1.0/53.0, entropy 0.99227, value loss 0.32117, policy loss -0.15683\n",
      "Updates 21900, num timesteps 1752080, FPS 1945, mean/median reward 7.3/5.0, min/max reward 1.0/33.0, entropy 0.88772, value loss 0.04383, policy loss 0.02736\n",
      "Updates 22000, num timesteps 1760080, FPS 1947, mean/median reward 6.1/4.0, min/max reward 1.0/17.0, entropy 0.97497, value loss 0.01902, policy loss -0.04393\n",
      "Updates 22100, num timesteps 1768080, FPS 1947, mean/median reward 13.1/8.0, min/max reward 2.0/37.0, entropy 0.98809, value loss 0.22571, policy loss -0.16883\n",
      "Updates 22200, num timesteps 1776080, FPS 1947, mean/median reward 5.1/1.0, min/max reward 0.0/19.0, entropy 1.18643, value loss 0.10745, policy loss 0.05792\n",
      "Updates 22300, num timesteps 1784080, FPS 1947, mean/median reward 5.3/5.0, min/max reward 0.0/12.0, entropy 1.08982, value loss 0.35336, policy loss -0.25913\n",
      "Updates 22400, num timesteps 1792080, FPS 1948, mean/median reward 7.6/5.0, min/max reward 1.0/24.0, entropy 0.95177, value loss 0.22454, policy loss -0.07217\n",
      "Updates 22500, num timesteps 1800080, FPS 1948, mean/median reward 10.6/10.0, min/max reward 1.0/30.0, entropy 1.03656, value loss 0.12146, policy loss -0.09500\n",
      "Updates 22600, num timesteps 1808080, FPS 1949, mean/median reward 10.8/8.0, min/max reward 1.0/50.0, entropy 1.12039, value loss 0.04640, policy loss -0.14439\n",
      "Updates 22700, num timesteps 1816080, FPS 1950, mean/median reward 9.8/9.0, min/max reward 1.0/25.0, entropy 0.99865, value loss 0.20300, policy loss 0.02391\n",
      "Updates 22800, num timesteps 1824080, FPS 1951, mean/median reward 9.0/5.0, min/max reward 2.0/21.0, entropy 1.04314, value loss 0.02239, policy loss 0.01450\n",
      "Updates 22900, num timesteps 1832080, FPS 1951, mean/median reward 6.9/4.0, min/max reward 1.0/25.0, entropy 1.08948, value loss 0.02451, policy loss -0.01664\n",
      "Updates 23000, num timesteps 1840080, FPS 1953, mean/median reward 10.8/8.0, min/max reward 1.0/28.0, entropy 1.01354, value loss 0.29486, policy loss -0.19964\n",
      "Updates 23100, num timesteps 1848080, FPS 1954, mean/median reward 7.5/3.0, min/max reward 1.0/28.0, entropy 0.92907, value loss 0.03850, policy loss 0.01339\n",
      "Updates 23200, num timesteps 1856080, FPS 1955, mean/median reward 7.3/6.0, min/max reward 1.0/23.0, entropy 0.96404, value loss 0.05776, policy loss -0.00416\n",
      "Updates 23300, num timesteps 1864080, FPS 1955, mean/median reward 12.7/6.0, min/max reward 1.0/58.0, entropy 1.07942, value loss 0.02781, policy loss 0.03668\n",
      "Updates 23400, num timesteps 1872080, FPS 1957, mean/median reward 11.7/9.0, min/max reward 1.0/39.0, entropy 1.05805, value loss 0.04788, policy loss -0.03924\n",
      "Updates 23500, num timesteps 1880080, FPS 1957, mean/median reward 14.4/7.0, min/max reward 1.0/38.0, entropy 1.10142, value loss 0.04979, policy loss -0.02713\n",
      "Updates 23600, num timesteps 1888080, FPS 1958, mean/median reward 6.9/3.0, min/max reward 0.0/24.0, entropy 1.03548, value loss 0.02016, policy loss -0.00201\n",
      "Updates 23700, num timesteps 1896080, FPS 1959, mean/median reward 11.6/8.0, min/max reward 1.0/37.0, entropy 1.12472, value loss 0.31404, policy loss 0.23500\n",
      "Updates 23800, num timesteps 1904080, FPS 1959, mean/median reward 10.9/8.0, min/max reward 0.0/44.0, entropy 0.95091, value loss 0.04682, policy loss -0.02751\n",
      "Updates 23900, num timesteps 1912080, FPS 1961, mean/median reward 17.4/9.0, min/max reward 0.0/63.0, entropy 0.91752, value loss 0.03902, policy loss -0.02365\n",
      "Updates 24000, num timesteps 1920080, FPS 1961, mean/median reward 11.1/8.0, min/max reward 3.0/35.0, entropy 0.92686, value loss 0.11457, policy loss 0.07302\n",
      "Updates 24100, num timesteps 1928080, FPS 1963, mean/median reward 12.4/10.0, min/max reward 0.0/47.0, entropy 0.88490, value loss 0.03875, policy loss 0.01932\n",
      "Updates 24200, num timesteps 1936080, FPS 1963, mean/median reward 8.9/5.0, min/max reward 0.0/25.0, entropy 1.06820, value loss 0.00846, policy loss 0.00152\n",
      "Updates 24300, num timesteps 1944080, FPS 1964, mean/median reward 11.2/9.0, min/max reward 0.0/32.0, entropy 0.97381, value loss 1.39673, policy loss -0.27825\n",
      "Updates 24400, num timesteps 1952080, FPS 1964, mean/median reward 12.9/10.0, min/max reward 0.0/34.0, entropy 0.94467, value loss 0.01880, policy loss -0.02139\n",
      "Updates 24500, num timesteps 1960080, FPS 1965, mean/median reward 4.1/2.0, min/max reward 0.0/15.0, entropy 0.92860, value loss 0.01537, policy loss -0.00707\n",
      "Updates 24600, num timesteps 1968080, FPS 1966, mean/median reward 14.2/13.0, min/max reward 0.0/34.0, entropy 0.98098, value loss 0.01405, policy loss 0.03138\n",
      "Updates 24700, num timesteps 1976080, FPS 1966, mean/median reward 11.6/9.0, min/max reward 0.0/24.0, entropy 0.95143, value loss 0.01637, policy loss 0.02370\n",
      "Updates 24800, num timesteps 1984080, FPS 1967, mean/median reward 10.9/6.0, min/max reward 2.0/47.0, entropy 1.09930, value loss 0.08062, policy loss 0.05723\n",
      "Updates 24900, num timesteps 1992080, FPS 1968, mean/median reward 13.1/8.0, min/max reward 1.0/41.0, entropy 1.02518, value loss 0.08005, policy loss 0.01588\n",
      "Updates 25000, num timesteps 2000080, FPS 1968, mean/median reward 5.8/1.0, min/max reward 0.0/23.0, entropy 0.95248, value loss 0.92700, policy loss -0.23988\n",
      "Updates 25100, num timesteps 2008080, FPS 1968, mean/median reward 6.9/3.0, min/max reward 0.0/25.0, entropy 1.00347, value loss 0.01740, policy loss -0.02553\n",
      "Updates 25200, num timesteps 2016080, FPS 1969, mean/median reward 8.5/5.0, min/max reward 2.0/19.0, entropy 0.88509, value loss 0.06455, policy loss 0.02260\n",
      "Updates 25300, num timesteps 2024080, FPS 1970, mean/median reward 8.5/6.0, min/max reward 1.0/20.0, entropy 0.91513, value loss 1.13113, policy loss -0.32994\n",
      "Updates 25400, num timesteps 2032080, FPS 1971, mean/median reward 9.3/9.0, min/max reward 0.0/25.0, entropy 0.90626, value loss 0.39891, policy loss -0.05555\n",
      "Updates 25500, num timesteps 2040080, FPS 1972, mean/median reward 14.8/12.0, min/max reward 1.0/41.0, entropy 0.81923, value loss 0.10510, policy loss -0.02306\n",
      "Updates 25600, num timesteps 2048080, FPS 1973, mean/median reward 9.9/7.0, min/max reward 0.0/25.0, entropy 1.03751, value loss 0.04917, policy loss 0.09734\n",
      "Updates 25700, num timesteps 2056080, FPS 1974, mean/median reward 9.8/6.0, min/max reward 0.0/34.0, entropy 1.08312, value loss 0.03465, policy loss -0.01915\n",
      "Updates 25800, num timesteps 2064080, FPS 1975, mean/median reward 12.2/9.0, min/max reward 0.0/28.0, entropy 1.01114, value loss 0.06049, policy loss 0.02400\n",
      "Updates 25900, num timesteps 2072080, FPS 1975, mean/median reward 20.4/18.0, min/max reward 7.0/74.0, entropy 0.79458, value loss 0.45395, policy loss -0.10940\n",
      "Updates 26000, num timesteps 2080080, FPS 1976, mean/median reward 12.0/6.0, min/max reward 0.0/39.0, entropy 0.90404, value loss 0.05732, policy loss -0.02459\n",
      "Updates 26100, num timesteps 2088080, FPS 1976, mean/median reward 9.6/8.0, min/max reward 0.0/26.0, entropy 0.94339, value loss 0.10170, policy loss -0.03838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 26200, num timesteps 2096080, FPS 1976, mean/median reward 8.5/6.0, min/max reward 0.0/31.0, entropy 1.14230, value loss 0.13724, policy loss -0.14946\n",
      "Updates 26300, num timesteps 2104080, FPS 1977, mean/median reward 3.8/4.0, min/max reward 0.0/10.0, entropy 1.02393, value loss 0.02663, policy loss -0.00220\n",
      "Updates 26400, num timesteps 2112080, FPS 1977, mean/median reward 12.1/6.0, min/max reward 0.0/52.0, entropy 0.98174, value loss 0.08838, policy loss -0.06165\n",
      "Updates 26500, num timesteps 2120080, FPS 1978, mean/median reward 12.8/7.0, min/max reward 0.0/44.0, entropy 0.98822, value loss 0.01652, policy loss -0.03101\n",
      "Updates 26600, num timesteps 2128080, FPS 1978, mean/median reward 8.6/6.0, min/max reward 1.0/19.0, entropy 1.06595, value loss 1.49266, policy loss 0.32022\n",
      "Updates 26700, num timesteps 2136080, FPS 1979, mean/median reward 6.6/4.0, min/max reward 0.0/19.0, entropy 0.92445, value loss 0.09814, policy loss 0.00376\n",
      "Updates 26800, num timesteps 2144080, FPS 1979, mean/median reward 7.9/5.0, min/max reward 0.0/28.0, entropy 0.92097, value loss 0.02698, policy loss -0.00890\n",
      "Updates 26900, num timesteps 2152080, FPS 1979, mean/median reward 7.0/3.0, min/max reward 0.0/38.0, entropy 0.95459, value loss 0.02501, policy loss 0.09246\n",
      "Updates 27000, num timesteps 2160080, FPS 1979, mean/median reward 8.1/7.0, min/max reward 0.0/22.0, entropy 0.92792, value loss 0.09772, policy loss 0.10377\n",
      "Updates 27100, num timesteps 2168080, FPS 1980, mean/median reward 10.3/6.0, min/max reward 1.0/33.0, entropy 0.88702, value loss 0.01477, policy loss -0.01336\n",
      "Updates 27200, num timesteps 2176080, FPS 1981, mean/median reward 13.0/10.0, min/max reward 2.0/33.0, entropy 0.89321, value loss 0.10636, policy loss -0.05218\n",
      "Updates 27300, num timesteps 2184080, FPS 1981, mean/median reward 8.1/6.0, min/max reward 0.0/26.0, entropy 0.98348, value loss 0.01818, policy loss 0.01524\n",
      "Updates 27400, num timesteps 2192080, FPS 1982, mean/median reward 12.1/7.0, min/max reward 1.0/45.0, entropy 0.89358, value loss 0.09890, policy loss -0.00244\n",
      "Updates 27500, num timesteps 2200080, FPS 1983, mean/median reward 10.5/7.0, min/max reward 0.0/32.0, entropy 0.80653, value loss 0.02426, policy loss 0.00919\n",
      "Updates 27600, num timesteps 2208080, FPS 1983, mean/median reward 10.0/7.0, min/max reward 0.0/45.0, entropy 1.08518, value loss 0.04247, policy loss -0.08606\n",
      "Updates 27700, num timesteps 2216080, FPS 1983, mean/median reward 8.9/7.0, min/max reward 0.0/26.0, entropy 0.87981, value loss 0.06868, policy loss 0.04379\n",
      "Updates 27800, num timesteps 2224080, FPS 1984, mean/median reward 12.9/8.0, min/max reward 1.0/35.0, entropy 1.00440, value loss 0.06695, policy loss -0.00506\n",
      "Updates 27900, num timesteps 2232080, FPS 1984, mean/median reward 15.5/6.0, min/max reward 0.0/46.0, entropy 1.09934, value loss 0.02525, policy loss -0.01973\n",
      "Updates 28000, num timesteps 2240080, FPS 1985, mean/median reward 8.9/9.0, min/max reward 0.0/24.0, entropy 0.92768, value loss 1.27404, policy loss -0.59144\n",
      "Updates 28100, num timesteps 2248080, FPS 1986, mean/median reward 8.5/4.0, min/max reward 0.0/42.0, entropy 1.08921, value loss 0.62970, policy loss -0.11526\n",
      "Updates 28200, num timesteps 2256080, FPS 1986, mean/median reward 8.4/6.0, min/max reward 1.0/19.0, entropy 0.97428, value loss 0.03028, policy loss -0.00264\n",
      "Updates 28300, num timesteps 2264080, FPS 1987, mean/median reward 10.0/5.0, min/max reward 0.0/48.0, entropy 0.96353, value loss 0.01647, policy loss -0.03263\n",
      "Updates 28400, num timesteps 2272080, FPS 1987, mean/median reward 10.4/5.0, min/max reward 0.0/38.0, entropy 0.98458, value loss 0.06493, policy loss 0.08085\n",
      "Updates 28500, num timesteps 2280080, FPS 1988, mean/median reward 11.3/5.0, min/max reward 0.0/44.0, entropy 1.02904, value loss 0.01445, policy loss -0.00951\n",
      "Updates 28600, num timesteps 2288080, FPS 1988, mean/median reward 9.3/8.0, min/max reward 0.0/23.0, entropy 0.86718, value loss 0.03268, policy loss 0.04789\n",
      "Updates 28700, num timesteps 2296080, FPS 1989, mean/median reward 10.8/8.0, min/max reward 0.0/33.0, entropy 0.89330, value loss 0.02479, policy loss 0.00553\n",
      "Updates 28800, num timesteps 2304080, FPS 1989, mean/median reward 10.1/6.0, min/max reward 0.0/30.0, entropy 0.19140, value loss 0.29798, policy loss -0.08996\n",
      "Updates 28900, num timesteps 2312080, FPS 1989, mean/median reward 2.8/0.0, min/max reward 0.0/15.0, entropy 0.93060, value loss 0.04870, policy loss -0.00768\n",
      "Updates 29000, num timesteps 2320080, FPS 1990, mean/median reward 15.6/9.0, min/max reward 0.0/58.0, entropy 0.93051, value loss 0.02954, policy loss 0.00974\n",
      "Updates 29100, num timesteps 2328080, FPS 1991, mean/median reward 10.1/7.0, min/max reward 0.0/36.0, entropy 1.01761, value loss 1.56058, policy loss -0.06813\n",
      "Updates 29200, num timesteps 2336080, FPS 1991, mean/median reward 9.0/7.0, min/max reward 0.0/25.0, entropy 1.09068, value loss 0.01761, policy loss -0.04044\n",
      "Updates 29300, num timesteps 2344080, FPS 1991, mean/median reward 10.8/5.0, min/max reward 0.0/43.0, entropy 1.12783, value loss 0.05466, policy loss -0.06189\n",
      "Updates 29400, num timesteps 2352080, FPS 1992, mean/median reward 10.0/6.0, min/max reward 0.0/26.0, entropy 0.97223, value loss 0.08626, policy loss -0.02610\n",
      "Updates 29500, num timesteps 2360080, FPS 1992, mean/median reward 9.2/6.0, min/max reward 0.0/44.0, entropy 1.02380, value loss 0.03032, policy loss -0.00914\n",
      "Updates 29600, num timesteps 2368080, FPS 1993, mean/median reward 11.2/7.0, min/max reward 1.0/50.0, entropy 1.07896, value loss 0.04065, policy loss -0.03072\n",
      "Updates 29700, num timesteps 2376080, FPS 1993, mean/median reward 11.9/5.0, min/max reward 1.0/42.0, entropy 1.06256, value loss 0.07306, policy loss 0.04523\n",
      "Updates 29800, num timesteps 2384080, FPS 1994, mean/median reward 5.3/4.0, min/max reward 0.0/15.0, entropy 1.00972, value loss 0.01065, policy loss 0.02498\n",
      "Updates 29900, num timesteps 2392080, FPS 1995, mean/median reward 8.4/5.0, min/max reward 0.0/28.0, entropy 0.87507, value loss 0.04483, policy loss 0.00570\n",
      "Updates 30000, num timesteps 2400080, FPS 1996, mean/median reward 12.6/5.0, min/max reward 1.0/44.0, entropy 0.97753, value loss 0.12501, policy loss 0.03829\n",
      "Updates 30100, num timesteps 2408080, FPS 1996, mean/median reward 9.4/6.0, min/max reward 0.0/44.0, entropy 0.88269, value loss 0.28416, policy loss 0.04589\n",
      "Updates 30200, num timesteps 2416080, FPS 1997, mean/median reward 7.9/6.0, min/max reward 0.0/29.0, entropy 1.03720, value loss 0.02054, policy loss -0.03123\n",
      "Updates 30300, num timesteps 2424080, FPS 1998, mean/median reward 12.8/5.0, min/max reward 0.0/36.0, entropy 1.00972, value loss 0.02518, policy loss -0.04059\n",
      "Updates 30400, num timesteps 2432080, FPS 1998, mean/median reward 10.9/4.0, min/max reward 0.0/33.0, entropy 0.94452, value loss 0.47332, policy loss 0.08763\n",
      "Updates 30500, num timesteps 2440080, FPS 1999, mean/median reward 6.8/3.0, min/max reward 0.0/28.0, entropy 0.90817, value loss 0.79308, policy loss -0.19033\n",
      "Updates 30600, num timesteps 2448080, FPS 2000, mean/median reward 8.2/5.0, min/max reward 0.0/33.0, entropy 0.95269, value loss 0.09684, policy loss 0.01519\n",
      "Updates 30700, num timesteps 2456080, FPS 2000, mean/median reward 15.6/17.0, min/max reward 0.0/45.0, entropy 1.05302, value loss 0.02336, policy loss 0.02346\n",
      "Updates 30800, num timesteps 2464080, FPS 2001, mean/median reward 8.3/6.0, min/max reward 0.0/25.0, entropy 0.87237, value loss 0.17768, policy loss 0.04340\n",
      "Updates 30900, num timesteps 2472080, FPS 2001, mean/median reward 12.1/5.0, min/max reward 0.0/48.0, entropy 0.92241, value loss 0.23026, policy loss -0.07819\n",
      "Updates 31000, num timesteps 2480080, FPS 2002, mean/median reward 13.7/12.0, min/max reward 1.0/34.0, entropy 0.93362, value loss 0.69007, policy loss -0.00575\n",
      "Updates 31100, num timesteps 2488080, FPS 2003, mean/median reward 8.8/5.0, min/max reward 0.0/42.0, entropy 0.97348, value loss 0.71865, policy loss -0.26378\n",
      "Updates 31200, num timesteps 2496080, FPS 2003, mean/median reward 10.1/5.0, min/max reward 0.0/24.0, entropy 1.11059, value loss 0.07264, policy loss 0.00972\n",
      "Updates 31300, num timesteps 2504080, FPS 2004, mean/median reward 8.6/5.0, min/max reward 0.0/38.0, entropy 0.89624, value loss 0.02714, policy loss 0.03221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 31400, num timesteps 2512080, FPS 2004, mean/median reward 8.1/5.0, min/max reward 0.0/25.0, entropy 1.03937, value loss 0.02516, policy loss -0.00121\n",
      "Updates 31500, num timesteps 2520080, FPS 2004, mean/median reward 12.3/8.0, min/max reward 0.0/49.0, entropy 1.07477, value loss 1.39824, policy loss -0.00872\n",
      "Updates 31600, num timesteps 2528080, FPS 2005, mean/median reward 9.4/6.0, min/max reward 0.0/25.0, entropy 1.02698, value loss 0.08703, policy loss 0.01544\n",
      "Updates 31700, num timesteps 2536080, FPS 2005, mean/median reward 18.1/7.0, min/max reward 0.0/80.0, entropy 0.98136, value loss 0.03342, policy loss -0.00653\n",
      "Updates 31800, num timesteps 2544080, FPS 2005, mean/median reward 12.9/10.0, min/max reward 1.0/35.0, entropy 1.01920, value loss 0.01965, policy loss -0.00538\n",
      "Updates 31900, num timesteps 2552080, FPS 2006, mean/median reward 9.9/10.0, min/max reward 0.0/29.0, entropy 0.96252, value loss 0.32404, policy loss -0.22770\n",
      "Updates 32000, num timesteps 2560080, FPS 2006, mean/median reward 12.2/10.0, min/max reward 3.0/23.0, entropy 1.12159, value loss 0.08586, policy loss 0.05354\n",
      "Updates 32100, num timesteps 2568080, FPS 2007, mean/median reward 17.3/10.0, min/max reward 1.0/58.0, entropy 0.80124, value loss 0.09567, policy loss -0.02183\n",
      "Updates 32200, num timesteps 2576080, FPS 2007, mean/median reward 7.9/6.0, min/max reward 0.0/28.0, entropy 0.99410, value loss 0.18937, policy loss 0.03386\n",
      "Updates 32300, num timesteps 2584080, FPS 2008, mean/median reward 9.1/9.0, min/max reward 1.0/23.0, entropy 0.91738, value loss 0.04909, policy loss 0.01009\n",
      "Updates 32400, num timesteps 2592080, FPS 2008, mean/median reward 15.4/10.0, min/max reward 4.0/48.0, entropy 1.02616, value loss 0.03975, policy loss -0.07576\n",
      "Updates 32500, num timesteps 2600080, FPS 2009, mean/median reward 12.9/8.0, min/max reward 1.0/32.0, entropy 0.98485, value loss 0.02769, policy loss -0.02861\n",
      "Updates 32600, num timesteps 2608080, FPS 2009, mean/median reward 9.0/6.0, min/max reward 1.0/22.0, entropy 0.91307, value loss 1.22378, policy loss -0.18303\n",
      "Updates 32700, num timesteps 2616080, FPS 2010, mean/median reward 10.3/3.0, min/max reward 0.0/36.0, entropy 0.92477, value loss 0.16332, policy loss -0.02657\n",
      "Updates 32800, num timesteps 2624080, FPS 2009, mean/median reward 0.7/0.0, min/max reward 0.0/4.0, entropy 1.20603, value loss 0.01087, policy loss 0.01494\n",
      "Updates 32900, num timesteps 2632080, FPS 2009, mean/median reward 6.1/6.0, min/max reward 0.0/16.0, entropy 0.99041, value loss 0.01136, policy loss -0.01893\n",
      "Updates 33000, num timesteps 2640080, FPS 2010, mean/median reward 15.6/12.0, min/max reward 0.0/35.0, entropy 0.83442, value loss 0.40313, policy loss -0.17832\n",
      "Updates 33100, num timesteps 2648080, FPS 2011, mean/median reward 12.6/8.0, min/max reward 0.0/34.0, entropy 0.98990, value loss 0.03270, policy loss -0.00545\n",
      "Updates 33200, num timesteps 2656080, FPS 2011, mean/median reward 12.7/8.0, min/max reward 0.0/36.0, entropy 1.11204, value loss 0.01969, policy loss -0.02582\n",
      "Updates 33300, num timesteps 2664080, FPS 2012, mean/median reward 15.1/12.0, min/max reward 0.0/51.0, entropy 1.01211, value loss 1.20357, policy loss -0.00720\n",
      "Updates 33400, num timesteps 2672080, FPS 2012, mean/median reward 15.2/8.0, min/max reward 0.0/46.0, entropy 1.04254, value loss 0.04445, policy loss -0.02469\n",
      "Updates 33500, num timesteps 2680080, FPS 2012, mean/median reward 11.9/7.0, min/max reward 0.0/35.0, entropy 1.01785, value loss 0.09944, policy loss 0.06448\n",
      "Updates 33600, num timesteps 2688080, FPS 2013, mean/median reward 12.1/9.0, min/max reward 0.0/31.0, entropy 0.89210, value loss 0.01458, policy loss -0.02653\n",
      "Updates 33700, num timesteps 2696080, FPS 2014, mean/median reward 11.4/6.0, min/max reward 0.0/30.0, entropy 0.93906, value loss 0.13808, policy loss 0.09015\n",
      "Updates 33800, num timesteps 2704080, FPS 2014, mean/median reward 9.2/8.0, min/max reward 1.0/21.0, entropy 1.11998, value loss 0.03197, policy loss 0.08774\n",
      "Updates 33900, num timesteps 2712080, FPS 2015, mean/median reward 10.1/3.0, min/max reward 0.0/44.0, entropy 1.07572, value loss 0.02859, policy loss -0.03216\n",
      "Updates 34000, num timesteps 2720080, FPS 2015, mean/median reward 17.0/15.0, min/max reward 2.0/37.0, entropy 1.08051, value loss 0.07893, policy loss 0.12203\n",
      "Updates 34100, num timesteps 2728080, FPS 2016, mean/median reward 13.2/10.0, min/max reward 3.0/28.0, entropy 0.84468, value loss 0.06650, policy loss -0.08506\n",
      "Updates 34200, num timesteps 2736080, FPS 2016, mean/median reward 11.2/6.0, min/max reward 1.0/29.0, entropy 0.99176, value loss 0.12437, policy loss -0.11632\n",
      "Updates 34300, num timesteps 2744080, FPS 2018, mean/median reward 12.4/8.0, min/max reward 1.0/29.0, entropy 0.86607, value loss 0.10408, policy loss 0.01302\n",
      "Updates 34400, num timesteps 2752080, FPS 2018, mean/median reward 18.2/11.0, min/max reward 2.0/49.0, entropy 0.90454, value loss 0.08273, policy loss 0.04027\n",
      "Updates 34500, num timesteps 2760080, FPS 2019, mean/median reward 12.8/8.0, min/max reward 0.0/34.0, entropy 0.96270, value loss 0.04410, policy loss 0.02372\n",
      "Updates 34600, num timesteps 2768080, FPS 2020, mean/median reward 9.6/4.0, min/max reward 1.0/39.0, entropy 1.04399, value loss 0.06220, policy loss -0.08596\n",
      "Updates 34700, num timesteps 2776080, FPS 2021, mean/median reward 14.7/9.0, min/max reward 0.0/53.0, entropy 0.93923, value loss 0.03615, policy loss 0.00274\n",
      "Updates 34800, num timesteps 2784080, FPS 2021, mean/median reward 13.6/10.0, min/max reward 1.0/31.0, entropy 0.99351, value loss 0.02438, policy loss -0.02973\n",
      "Updates 34900, num timesteps 2792080, FPS 2022, mean/median reward 11.7/5.0, min/max reward 0.0/46.0, entropy 0.95748, value loss 0.02781, policy loss -0.02450\n",
      "Updates 35000, num timesteps 2800080, FPS 2023, mean/median reward 11.6/8.0, min/max reward 0.0/38.0, entropy 0.90256, value loss 0.07412, policy loss -0.04355\n",
      "Updates 35100, num timesteps 2808080, FPS 2023, mean/median reward 15.1/6.0, min/max reward 1.0/54.0, entropy 0.94544, value loss 0.02363, policy loss -0.02529\n",
      "Updates 35200, num timesteps 2816080, FPS 2023, mean/median reward 11.5/6.0, min/max reward 0.0/52.0, entropy 0.92036, value loss 0.01831, policy loss 0.01761\n",
      "Updates 35300, num timesteps 2824080, FPS 2023, mean/median reward 11.2/12.0, min/max reward 0.0/34.0, entropy 0.98454, value loss 0.05162, policy loss 0.05478\n",
      "Updates 35400, num timesteps 2832080, FPS 2024, mean/median reward 9.6/6.0, min/max reward 0.0/52.0, entropy 0.98675, value loss 0.04076, policy loss -0.02789\n",
      "Updates 35500, num timesteps 2840080, FPS 2025, mean/median reward 14.8/7.0, min/max reward 1.0/52.0, entropy 0.99689, value loss 0.07902, policy loss 0.04276\n",
      "Updates 35600, num timesteps 2848080, FPS 2026, mean/median reward 17.4/8.0, min/max reward 2.0/55.0, entropy 0.93707, value loss 0.25308, policy loss -0.02461\n",
      "Updates 35700, num timesteps 2856080, FPS 2026, mean/median reward 18.6/13.0, min/max reward 1.0/57.0, entropy 0.92471, value loss 0.06233, policy loss -0.04198\n",
      "Updates 35800, num timesteps 2864080, FPS 2027, mean/median reward 11.6/7.0, min/max reward 1.0/46.0, entropy 1.06876, value loss 0.02960, policy loss 0.00808\n",
      "Updates 35900, num timesteps 2872080, FPS 2027, mean/median reward 11.8/5.0, min/max reward 0.0/42.0, entropy 0.83344, value loss 0.29454, policy loss 0.09245\n",
      "Updates 36000, num timesteps 2880080, FPS 2027, mean/median reward 6.4/4.0, min/max reward 0.0/26.0, entropy 0.89356, value loss 0.03929, policy loss 0.03739\n",
      "Updates 36100, num timesteps 2888080, FPS 2028, mean/median reward 14.2/9.0, min/max reward 1.0/51.0, entropy 0.99020, value loss 0.07659, policy loss 0.05320\n",
      "Updates 36200, num timesteps 2896080, FPS 2029, mean/median reward 17.7/8.0, min/max reward 1.0/66.0, entropy 0.91022, value loss 0.12166, policy loss 0.10730\n",
      "Updates 36300, num timesteps 2904080, FPS 2029, mean/median reward 15.1/7.0, min/max reward 1.0/44.0, entropy 0.89892, value loss 0.05249, policy loss -0.03754\n",
      "Updates 36400, num timesteps 2912080, FPS 2030, mean/median reward 14.9/7.0, min/max reward 0.0/44.0, entropy 0.92558, value loss 0.20722, policy loss 0.14134\n",
      "Updates 36500, num timesteps 2920080, FPS 2030, mean/median reward 22.8/21.0, min/max reward 0.0/54.0, entropy 1.00292, value loss 0.01820, policy loss 0.04417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 36600, num timesteps 2928080, FPS 2030, mean/median reward 19.3/13.0, min/max reward 0.0/57.0, entropy 0.77298, value loss 0.44713, policy loss -0.16852\n",
      "Updates 36700, num timesteps 2936080, FPS 2030, mean/median reward 13.1/7.0, min/max reward 1.0/83.0, entropy 0.84360, value loss 0.08801, policy loss -0.02699\n",
      "Updates 36800, num timesteps 2944080, FPS 2031, mean/median reward 7.8/3.0, min/max reward 0.0/34.0, entropy 1.14514, value loss 0.02668, policy loss 0.02796\n",
      "Updates 36900, num timesteps 2952080, FPS 2031, mean/median reward 9.6/6.0, min/max reward 0.0/32.0, entropy 0.96848, value loss 0.11516, policy loss 0.07589\n",
      "Updates 37000, num timesteps 2960080, FPS 2032, mean/median reward 16.7/9.0, min/max reward 0.0/56.0, entropy 1.00625, value loss 0.05729, policy loss 0.05806\n",
      "Updates 37100, num timesteps 2968080, FPS 2033, mean/median reward 7.9/4.0, min/max reward 0.0/22.0, entropy 0.92757, value loss 0.13532, policy loss -0.06789\n",
      "Updates 37200, num timesteps 2976080, FPS 2033, mean/median reward 9.5/5.0, min/max reward 0.0/34.0, entropy 0.85780, value loss 0.06245, policy loss 0.04914\n",
      "Updates 37300, num timesteps 2984080, FPS 2033, mean/median reward 4.9/4.0, min/max reward 0.0/12.0, entropy 0.92654, value loss 0.02842, policy loss -0.02501\n",
      "Updates 37400, num timesteps 2992080, FPS 2034, mean/median reward 6.1/5.0, min/max reward 0.0/21.0, entropy 1.04798, value loss 0.01812, policy loss -0.02475\n",
      "Updates 37500, num timesteps 3000080, FPS 2035, mean/median reward 9.0/8.0, min/max reward 0.0/26.0, entropy 0.84478, value loss 0.62265, policy loss -0.06735\n",
      "Updates 37600, num timesteps 3008080, FPS 2036, mean/median reward 17.1/10.0, min/max reward 1.0/65.0, entropy 1.00317, value loss 1.61480, policy loss -0.17538\n",
      "Updates 37700, num timesteps 3016080, FPS 2036, mean/median reward 10.0/7.0, min/max reward 0.0/32.0, entropy 0.93074, value loss 0.02608, policy loss 0.04071\n",
      "Updates 37800, num timesteps 3024080, FPS 2036, mean/median reward 13.1/14.0, min/max reward 0.0/28.0, entropy 1.05920, value loss 0.58521, policy loss 0.24574\n",
      "Updates 37900, num timesteps 3032080, FPS 2037, mean/median reward 12.4/7.0, min/max reward 0.0/40.0, entropy 0.89924, value loss 0.02442, policy loss -0.02412\n",
      "Updates 38000, num timesteps 3040080, FPS 2038, mean/median reward 15.6/8.0, min/max reward 0.0/49.0, entropy 0.74221, value loss 2.51312, policy loss -0.20034\n",
      "Updates 38100, num timesteps 3048080, FPS 2038, mean/median reward 7.0/3.0, min/max reward 0.0/34.0, entropy 1.03764, value loss 0.30703, policy loss -0.11002\n",
      "Updates 38200, num timesteps 3056080, FPS 2038, mean/median reward 8.4/6.0, min/max reward 1.0/25.0, entropy 1.07098, value loss 0.09811, policy loss 0.01286\n",
      "Updates 38300, num timesteps 3064080, FPS 2039, mean/median reward 10.8/6.0, min/max reward 0.0/58.0, entropy 0.96325, value loss 0.03516, policy loss 0.06699\n",
      "Updates 38400, num timesteps 3072080, FPS 2039, mean/median reward 8.8/6.0, min/max reward 0.0/32.0, entropy 0.90144, value loss 0.02791, policy loss -0.00224\n",
      "Updates 38500, num timesteps 3080080, FPS 2039, mean/median reward 18.8/8.0, min/max reward 0.0/82.0, entropy 0.78255, value loss 0.12328, policy loss -0.02774\n",
      "Updates 38600, num timesteps 3088080, FPS 2040, mean/median reward 8.1/5.0, min/max reward 1.0/30.0, entropy 0.93821, value loss 0.03039, policy loss -0.06705\n",
      "Updates 38700, num timesteps 3096080, FPS 2040, mean/median reward 8.7/8.0, min/max reward 0.0/21.0, entropy 0.88337, value loss 0.06928, policy loss 0.03542\n",
      "Updates 38800, num timesteps 3104080, FPS 2041, mean/median reward 11.2/8.0, min/max reward 1.0/31.0, entropy 1.00052, value loss 0.02510, policy loss -0.03437\n",
      "Updates 38900, num timesteps 3112080, FPS 2042, mean/median reward 16.0/12.0, min/max reward 0.0/39.0, entropy 0.84733, value loss 0.23231, policy loss -0.01644\n",
      "Updates 39000, num timesteps 3120080, FPS 2042, mean/median reward 9.3/6.0, min/max reward 0.0/27.0, entropy 0.88258, value loss 0.10077, policy loss 0.06410\n",
      "Updates 39100, num timesteps 3128080, FPS 2043, mean/median reward 13.8/10.0, min/max reward 0.0/35.0, entropy 0.85669, value loss 0.04010, policy loss -0.01373\n",
      "Updates 39200, num timesteps 3136080, FPS 2043, mean/median reward 9.2/8.0, min/max reward 0.0/27.0, entropy 0.93614, value loss 0.04014, policy loss -0.06557\n",
      "Updates 39300, num timesteps 3144080, FPS 2043, mean/median reward 1.9/1.0, min/max reward 0.0/6.0, entropy 1.15318, value loss 0.01562, policy loss 0.03072\n",
      "Updates 39400, num timesteps 3152080, FPS 2044, mean/median reward 6.7/4.0, min/max reward 0.0/18.0, entropy 0.81018, value loss 0.04509, policy loss -0.08765\n",
      "Updates 39500, num timesteps 3160080, FPS 2045, mean/median reward 18.0/15.0, min/max reward 1.0/45.0, entropy 0.85040, value loss 0.02206, policy loss -0.00307\n",
      "Updates 39600, num timesteps 3168080, FPS 2045, mean/median reward 14.4/6.0, min/max reward 1.0/52.0, entropy 0.91011, value loss 0.06681, policy loss -0.05050\n",
      "Updates 39700, num timesteps 3176080, FPS 2045, mean/median reward 6.2/4.0, min/max reward 0.0/35.0, entropy 0.99809, value loss 0.04268, policy loss -0.02982\n",
      "Updates 39800, num timesteps 3184080, FPS 2046, mean/median reward 12.6/8.0, min/max reward 0.0/42.0, entropy 1.01700, value loss 0.06132, policy loss 0.00581\n",
      "Updates 39900, num timesteps 3192080, FPS 2046, mean/median reward 11.7/4.0, min/max reward 0.0/51.0, entropy 0.93606, value loss 0.28452, policy loss -0.00617\n",
      "Updates 40000, num timesteps 3200080, FPS 2047, mean/median reward 7.3/5.0, min/max reward 0.0/16.0, entropy 1.10063, value loss 0.02642, policy loss 0.03225\n",
      "Updates 40100, num timesteps 3208080, FPS 2047, mean/median reward 10.8/10.0, min/max reward 0.0/36.0, entropy 0.98415, value loss 0.03550, policy loss 0.03405\n",
      "Updates 40200, num timesteps 3216080, FPS 2048, mean/median reward 13.6/7.0, min/max reward 0.0/72.0, entropy 0.87557, value loss 0.07328, policy loss 0.00166\n",
      "Updates 40300, num timesteps 3224080, FPS 2048, mean/median reward 13.1/9.0, min/max reward 1.0/55.0, entropy 0.99397, value loss 0.09783, policy loss -0.06324\n",
      "Updates 40400, num timesteps 3232080, FPS 2049, mean/median reward 7.8/4.0, min/max reward 0.0/24.0, entropy 0.85955, value loss 0.76268, policy loss -0.24347\n",
      "Updates 40500, num timesteps 3240080, FPS 2049, mean/median reward 14.9/8.0, min/max reward 0.0/58.0, entropy 1.00879, value loss 0.04288, policy loss 0.02196\n",
      "Updates 40600, num timesteps 3248080, FPS 2049, mean/median reward 8.1/5.0, min/max reward 1.0/21.0, entropy 0.88362, value loss 0.06442, policy loss -0.02418\n",
      "Updates 40700, num timesteps 3256080, FPS 2049, mean/median reward 7.6/5.0, min/max reward 0.0/28.0, entropy 0.88698, value loss 0.22158, policy loss -0.06249\n",
      "Updates 40800, num timesteps 3264080, FPS 2050, mean/median reward 16.7/12.0, min/max reward 1.0/41.0, entropy 0.99852, value loss 0.05391, policy loss 0.00713\n",
      "Updates 40900, num timesteps 3272080, FPS 2050, mean/median reward 15.8/12.0, min/max reward 2.0/43.0, entropy 0.78915, value loss 0.11199, policy loss -0.00265\n",
      "Updates 41000, num timesteps 3280080, FPS 2051, mean/median reward 12.8/7.0, min/max reward 0.0/64.0, entropy 0.92722, value loss 0.03677, policy loss 0.05741\n",
      "Updates 41100, num timesteps 3288080, FPS 2051, mean/median reward 17.7/8.0, min/max reward 0.0/63.0, entropy 0.82694, value loss 0.66615, policy loss -0.16959\n",
      "Updates 41200, num timesteps 3296080, FPS 2051, mean/median reward 8.3/5.0, min/max reward 0.0/30.0, entropy 0.89710, value loss 0.38692, policy loss -0.06371\n",
      "Updates 41300, num timesteps 3304080, FPS 2051, mean/median reward 11.2/6.0, min/max reward 0.0/57.0, entropy 0.81522, value loss 0.02251, policy loss 0.05081\n",
      "Updates 41400, num timesteps 3312080, FPS 2052, mean/median reward 19.8/7.0, min/max reward 0.0/90.0, entropy 0.90457, value loss 0.04608, policy loss -0.03372\n",
      "Updates 41500, num timesteps 3320080, FPS 2052, mean/median reward 16.7/4.0, min/max reward 0.0/52.0, entropy 0.90959, value loss 0.05579, policy loss -0.04834\n",
      "Updates 41600, num timesteps 3328080, FPS 2052, mean/median reward 10.5/7.0, min/max reward 0.0/37.0, entropy 0.90413, value loss 0.13023, policy loss -0.14070\n",
      "Updates 41700, num timesteps 3336080, FPS 2052, mean/median reward 7.1/4.0, min/max reward 0.0/34.0, entropy 0.87094, value loss 0.05190, policy loss -0.01160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 41800, num timesteps 3344080, FPS 2053, mean/median reward 12.2/4.0, min/max reward 0.0/49.0, entropy 0.91097, value loss 0.02017, policy loss 0.01262\n",
      "Updates 41900, num timesteps 3352080, FPS 2053, mean/median reward 18.4/17.0, min/max reward 1.0/46.0, entropy 0.80788, value loss 0.84357, policy loss 0.00334\n",
      "Updates 42000, num timesteps 3360080, FPS 2054, mean/median reward 14.3/16.0, min/max reward 0.0/28.0, entropy 0.95520, value loss 0.58406, policy loss -0.18529\n",
      "Updates 42100, num timesteps 3368080, FPS 2054, mean/median reward 14.4/8.0, min/max reward 0.0/52.0, entropy 1.03195, value loss 0.06186, policy loss 0.00824\n",
      "Updates 42200, num timesteps 3376080, FPS 2054, mean/median reward 11.4/6.0, min/max reward 2.0/52.0, entropy 0.94373, value loss 0.37547, policy loss -0.06156\n",
      "Updates 42300, num timesteps 3384080, FPS 2055, mean/median reward 15.5/8.0, min/max reward 4.0/46.0, entropy 0.86267, value loss 0.11401, policy loss 0.09627\n",
      "Updates 42400, num timesteps 3392080, FPS 2055, mean/median reward 8.9/4.0, min/max reward 0.0/40.0, entropy 0.89623, value loss 0.04323, policy loss 0.04218\n",
      "Updates 42500, num timesteps 3400080, FPS 2055, mean/median reward 4.3/3.0, min/max reward 0.0/16.0, entropy 0.94910, value loss 0.03146, policy loss -0.09314\n",
      "Updates 42600, num timesteps 3408080, FPS 2056, mean/median reward 15.5/7.0, min/max reward 0.0/80.0, entropy 0.82535, value loss 0.04638, policy loss -0.03932\n",
      "Updates 42700, num timesteps 3416080, FPS 2057, mean/median reward 14.2/6.0, min/max reward 0.0/79.0, entropy 0.92886, value loss 0.06212, policy loss 0.00547\n",
      "Updates 42800, num timesteps 3424080, FPS 2057, mean/median reward 13.9/5.0, min/max reward 0.0/81.0, entropy 0.92204, value loss 0.17338, policy loss -0.04080\n",
      "Updates 42900, num timesteps 3432080, FPS 2057, mean/median reward 9.9/5.0, min/max reward 0.0/46.0, entropy 0.89112, value loss 0.09449, policy loss 0.01473\n",
      "Updates 43000, num timesteps 3440080, FPS 2058, mean/median reward 9.6/5.0, min/max reward 0.0/37.0, entropy 0.96265, value loss 0.32480, policy loss 0.09042\n",
      "Updates 43100, num timesteps 3448080, FPS 2058, mean/median reward 9.1/7.0, min/max reward 0.0/27.0, entropy 1.01429, value loss 0.08901, policy loss -0.04224\n",
      "Updates 43200, num timesteps 3456080, FPS 2059, mean/median reward 18.9/8.0, min/max reward 0.0/64.0, entropy 0.81310, value loss 0.22637, policy loss -0.06105\n",
      "Updates 43300, num timesteps 3464080, FPS 2059, mean/median reward 19.9/8.0, min/max reward 0.0/64.0, entropy 0.88856, value loss 0.13214, policy loss 0.01180\n",
      "Updates 43400, num timesteps 3472080, FPS 2060, mean/median reward 20.2/14.0, min/max reward 0.0/64.0, entropy 0.97819, value loss 0.27668, policy loss 0.12313\n",
      "Updates 43500, num timesteps 3480080, FPS 2060, mean/median reward 21.1/12.0, min/max reward 4.0/67.0, entropy 0.82728, value loss 0.02914, policy loss 0.01248\n",
      "Updates 43600, num timesteps 3488080, FPS 2060, mean/median reward 11.7/8.0, min/max reward 2.0/38.0, entropy 0.97052, value loss 0.09004, policy loss -0.05104\n",
      "Updates 43700, num timesteps 3496080, FPS 2061, mean/median reward 16.9/12.0, min/max reward 1.0/47.0, entropy 0.66341, value loss 0.12497, policy loss 0.01097\n",
      "Updates 43800, num timesteps 3504080, FPS 2061, mean/median reward 12.4/8.0, min/max reward 1.0/44.0, entropy 0.94613, value loss 0.01942, policy loss -0.02793\n",
      "Updates 43900, num timesteps 3512080, FPS 2062, mean/median reward 17.6/12.0, min/max reward 0.0/67.0, entropy 0.81881, value loss 0.89336, policy loss 0.04257\n",
      "Updates 44000, num timesteps 3520080, FPS 2062, mean/median reward 10.8/7.0, min/max reward 0.0/32.0, entropy 0.80420, value loss 0.06614, policy loss -0.04278\n",
      "Updates 44100, num timesteps 3528080, FPS 2063, mean/median reward 23.1/8.0, min/max reward 2.0/84.0, entropy 0.92821, value loss 0.07311, policy loss -0.04728\n",
      "Updates 44200, num timesteps 3536080, FPS 2063, mean/median reward 14.6/7.0, min/max reward 0.0/70.0, entropy 1.03367, value loss 0.09474, policy loss 0.06225\n",
      "Updates 44300, num timesteps 3544080, FPS 2064, mean/median reward 7.6/4.0, min/max reward 0.0/40.0, entropy 0.91481, value loss 0.08053, policy loss 0.03998\n",
      "Updates 44400, num timesteps 3552080, FPS 2064, mean/median reward 8.9/5.0, min/max reward 0.0/29.0, entropy 0.89351, value loss 0.02113, policy loss 0.01027\n",
      "Updates 44500, num timesteps 3560080, FPS 2064, mean/median reward 19.7/17.0, min/max reward 0.0/46.0, entropy 0.87840, value loss 0.09342, policy loss 0.09236\n",
      "Updates 44600, num timesteps 3568080, FPS 2064, mean/median reward 14.6/9.0, min/max reward 1.0/58.0, entropy 1.04844, value loss 0.04582, policy loss 0.04630\n",
      "Updates 44700, num timesteps 3576080, FPS 2064, mean/median reward 17.4/9.0, min/max reward 0.0/63.0, entropy 0.92766, value loss 0.02020, policy loss -0.00442\n",
      "Updates 44800, num timesteps 3584080, FPS 2065, mean/median reward 13.6/12.0, min/max reward 0.0/50.0, entropy 1.00250, value loss 0.05925, policy loss 0.02150\n",
      "Updates 44900, num timesteps 3592080, FPS 2065, mean/median reward 13.2/8.0, min/max reward 2.0/42.0, entropy 0.78542, value loss 0.08160, policy loss 0.01072\n",
      "Updates 45000, num timesteps 3600080, FPS 2066, mean/median reward 18.1/13.0, min/max reward 2.0/49.0, entropy 0.91538, value loss 0.04550, policy loss 0.01768\n",
      "Updates 45100, num timesteps 3608080, FPS 2066, mean/median reward 10.8/11.0, min/max reward 0.0/25.0, entropy 0.90466, value loss 0.04619, policy loss 0.09629\n",
      "Updates 45200, num timesteps 3616080, FPS 2066, mean/median reward 18.2/13.0, min/max reward 0.0/61.0, entropy 0.88383, value loss 0.08783, policy loss 0.00492\n",
      "Updates 45300, num timesteps 3624080, FPS 2066, mean/median reward 13.5/11.0, min/max reward 0.0/36.0, entropy 0.90026, value loss 0.04772, policy loss -0.03669\n",
      "Updates 45400, num timesteps 3632080, FPS 2067, mean/median reward 15.2/7.0, min/max reward 0.0/89.0, entropy 0.94289, value loss 0.11174, policy loss 0.03126\n",
      "Updates 45500, num timesteps 3640080, FPS 2068, mean/median reward 13.1/2.0, min/max reward 0.0/55.0, entropy 1.08821, value loss 0.08081, policy loss 0.09163\n",
      "Updates 45600, num timesteps 3648080, FPS 2068, mean/median reward 11.9/6.0, min/max reward 0.0/42.0, entropy 0.93668, value loss 0.08639, policy loss 0.07691\n",
      "Updates 45700, num timesteps 3656080, FPS 2068, mean/median reward 16.4/9.0, min/max reward 0.0/97.0, entropy 0.88152, value loss 0.04821, policy loss -0.04875\n",
      "Updates 45800, num timesteps 3664080, FPS 2069, mean/median reward 15.1/9.0, min/max reward 0.0/52.0, entropy 1.00365, value loss 0.08705, policy loss -0.03180\n",
      "Updates 45900, num timesteps 3672080, FPS 2069, mean/median reward 11.4/6.0, min/max reward 0.0/39.0, entropy 0.83235, value loss 0.17632, policy loss 0.00536\n",
      "Updates 57300, num timesteps 4584080, FPS 2101, mean/median reward 7.3/6.0, min/max reward 0.0/38.0, entropy 1.08029, value loss 0.03419, policy loss 0.02204\n",
      "Updates 57400, num timesteps 4592080, FPS 2101, mean/median reward 14.3/11.0, min/max reward 0.0/48.0, entropy 1.01113, value loss 0.06439, policy loss 0.01128\n",
      "Updates 57500, num timesteps 4600080, FPS 2101, mean/median reward 11.9/5.0, min/max reward 0.0/62.0, entropy 1.05419, value loss 0.02982, policy loss -0.02281\n",
      "Updates 57600, num timesteps 4608080, FPS 2101, mean/median reward 12.2/6.0, min/max reward 0.0/70.0, entropy 0.95292, value loss 0.06113, policy loss 0.04463\n",
      "Updates 57700, num timesteps 4616080, FPS 2102, mean/median reward 14.1/8.0, min/max reward 0.0/71.0, entropy 1.01391, value loss 0.70930, policy loss -0.18187\n",
      "Updates 57800, num timesteps 4624080, FPS 2102, mean/median reward 16.4/8.0, min/max reward 1.0/81.0, entropy 0.94286, value loss 0.05254, policy loss -0.00794\n",
      "Updates 57900, num timesteps 4632080, FPS 2103, mean/median reward 18.2/8.0, min/max reward 1.0/81.0, entropy 0.73558, value loss 0.30336, policy loss 0.06837\n",
      "Updates 58000, num timesteps 4640080, FPS 2103, mean/median reward 23.7/12.0, min/max reward 1.0/59.0, entropy 0.92220, value loss 0.21401, policy loss 0.02910\n",
      "Updates 58100, num timesteps 4648080, FPS 2103, mean/median reward 14.1/7.0, min/max reward 0.0/48.0, entropy 0.97890, value loss 0.07283, policy loss 0.03714\n",
      "Updates 58200, num timesteps 4656080, FPS 2103, mean/median reward 8.2/5.0, min/max reward 0.0/37.0, entropy 1.05304, value loss 0.05601, policy loss -0.00802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 58300, num timesteps 4664080, FPS 2103, mean/median reward 19.6/16.0, min/max reward 0.0/63.0, entropy 0.62027, value loss 0.12501, policy loss -0.00450\n",
      "Updates 58400, num timesteps 4672080, FPS 2103, mean/median reward 16.9/5.0, min/max reward 0.0/85.0, entropy 1.05140, value loss 0.04899, policy loss 0.03593\n",
      "Updates 58500, num timesteps 4680080, FPS 2103, mean/median reward 9.7/5.0, min/max reward 0.0/33.0, entropy 1.09451, value loss 0.03309, policy loss -0.05202\n",
      "Updates 58600, num timesteps 4688080, FPS 2103, mean/median reward 17.6/12.0, min/max reward 0.0/73.0, entropy 0.94924, value loss 0.54471, policy loss -0.18611\n",
      "Updates 58700, num timesteps 4696080, FPS 2103, mean/median reward 14.9/12.0, min/max reward 2.0/49.0, entropy 0.95293, value loss 0.05017, policy loss -0.01130\n",
      "Updates 58800, num timesteps 4704080, FPS 2103, mean/median reward 10.6/6.0, min/max reward 1.0/32.0, entropy 1.01862, value loss 0.07465, policy loss -0.01769\n",
      "Updates 58900, num timesteps 4712080, FPS 2104, mean/median reward 11.4/4.0, min/max reward 1.0/62.0, entropy 1.03197, value loss 0.14880, policy loss 0.06571\n",
      "Updates 59000, num timesteps 4720080, FPS 2104, mean/median reward 23.0/14.0, min/max reward 0.0/87.0, entropy 0.80814, value loss 1.22889, policy loss -0.05391\n",
      "Updates 59100, num timesteps 4728080, FPS 2104, mean/median reward 22.2/11.0, min/max reward 1.0/87.0, entropy 1.09131, value loss 0.03975, policy loss 0.00218\n",
      "Updates 59200, num timesteps 4736080, FPS 2104, mean/median reward 9.5/4.0, min/max reward 0.0/60.0, entropy 1.13832, value loss 0.02384, policy loss -0.00433\n",
      "Updates 59300, num timesteps 4744080, FPS 2104, mean/median reward 4.6/4.0, min/max reward 0.0/18.0, entropy 0.95628, value loss 0.05543, policy loss 0.07003\n",
      "Updates 59400, num timesteps 4752080, FPS 2105, mean/median reward 15.7/4.0, min/max reward 0.0/71.0, entropy 0.90456, value loss 0.04528, policy loss 0.05294\n",
      "Updates 59500, num timesteps 4760080, FPS 2105, mean/median reward 22.1/5.0, min/max reward 0.0/84.0, entropy 1.01103, value loss 0.03389, policy loss -0.03795\n",
      "Updates 59600, num timesteps 4768080, FPS 2105, mean/median reward 17.7/6.0, min/max reward 0.0/89.0, entropy 1.05250, value loss 0.02583, policy loss 0.04415\n",
      "Updates 59700, num timesteps 4776080, FPS 2105, mean/median reward 5.9/3.0, min/max reward 0.0/19.0, entropy 1.10594, value loss 0.01506, policy loss 0.00231\n",
      "Updates 59800, num timesteps 4784080, FPS 2105, mean/median reward 8.6/3.0, min/max reward 0.0/28.0, entropy 0.97846, value loss 0.27961, policy loss -0.07304\n",
      "Updates 59900, num timesteps 4792080, FPS 2106, mean/median reward 9.4/7.0, min/max reward 0.0/37.0, entropy 1.01504, value loss 0.13028, policy loss 0.05000\n",
      "Updates 60000, num timesteps 4800080, FPS 2106, mean/median reward 14.2/9.0, min/max reward 0.0/46.0, entropy 1.09581, value loss 0.24681, policy loss 0.17357\n",
      "Updates 60100, num timesteps 4808080, FPS 2106, mean/median reward 20.4/9.0, min/max reward 0.0/75.0, entropy 1.07980, value loss 0.05521, policy loss -0.02900\n",
      "Updates 60200, num timesteps 4816080, FPS 2106, mean/median reward 12.2/9.0, min/max reward 0.0/49.0, entropy 1.06960, value loss 0.04232, policy loss 0.00081\n",
      "Updates 60300, num timesteps 4824080, FPS 2106, mean/median reward 12.6/9.0, min/max reward 0.0/56.0, entropy 0.91867, value loss 0.07581, policy loss -0.01078\n",
      "Updates 60400, num timesteps 4832080, FPS 2107, mean/median reward 20.2/14.0, min/max reward 0.0/69.0, entropy 0.87027, value loss 0.12901, policy loss -0.02640\n",
      "Updates 60500, num timesteps 4840080, FPS 2107, mean/median reward 17.9/10.0, min/max reward 0.0/56.0, entropy 0.84432, value loss 0.03828, policy loss -0.03927\n",
      "Updates 60600, num timesteps 4848080, FPS 2107, mean/median reward 9.6/6.0, min/max reward 0.0/36.0, entropy 1.08921, value loss 0.18954, policy loss -0.09303\n",
      "Updates 60700, num timesteps 4856080, FPS 2107, mean/median reward 6.7/4.0, min/max reward 0.0/17.0, entropy 1.02962, value loss 0.16993, policy loss -0.14689\n",
      "Updates 60800, num timesteps 4864080, FPS 2107, mean/median reward 13.1/5.0, min/max reward 1.0/52.0, entropy 0.92252, value loss 0.04393, policy loss -0.00260\n",
      "Updates 60900, num timesteps 4872080, FPS 2108, mean/median reward 27.6/20.0, min/max reward 3.0/104.0, entropy 0.83506, value loss 0.12291, policy loss -0.00260\n",
      "Updates 61000, num timesteps 4880080, FPS 2108, mean/median reward 18.8/7.0, min/max reward 3.0/96.0, entropy 1.01844, value loss 0.06498, policy loss 0.01631\n",
      "Updates 61100, num timesteps 4888080, FPS 2108, mean/median reward 15.7/7.0, min/max reward 1.0/61.0, entropy 0.93518, value loss 0.28007, policy loss 0.00661\n",
      "Updates 61200, num timesteps 4896080, FPS 2108, mean/median reward 17.2/8.0, min/max reward 0.0/90.0, entropy 0.97083, value loss 0.02890, policy loss 0.04732\n",
      "Updates 61300, num timesteps 4904080, FPS 2108, mean/median reward 16.5/9.0, min/max reward 0.0/57.0, entropy 0.89625, value loss 0.13396, policy loss 0.02283\n",
      "Updates 61400, num timesteps 4912080, FPS 2108, mean/median reward 18.0/8.0, min/max reward 0.0/78.0, entropy 1.03045, value loss 0.07371, policy loss 0.05365\n",
      "Updates 61500, num timesteps 4920080, FPS 2108, mean/median reward 13.3/4.0, min/max reward 0.0/78.0, entropy 0.96711, value loss 0.17357, policy loss -0.01240\n",
      "Updates 61600, num timesteps 4928080, FPS 2109, mean/median reward 13.8/6.0, min/max reward 0.0/73.0, entropy 0.88931, value loss 0.02825, policy loss -0.00165\n",
      "Updates 61700, num timesteps 4936080, FPS 2109, mean/median reward 27.1/18.0, min/max reward 1.0/85.0, entropy 0.93570, value loss 0.18868, policy loss 0.09345\n",
      "Updates 61800, num timesteps 4944080, FPS 2109, mean/median reward 15.9/10.0, min/max reward 2.0/44.0, entropy 0.82285, value loss 0.26034, policy loss -0.09881\n",
      "Updates 61900, num timesteps 4952080, FPS 2109, mean/median reward 9.7/7.0, min/max reward 0.0/55.0, entropy 1.01259, value loss 0.03752, policy loss -0.02762\n",
      "Updates 62000, num timesteps 4960080, FPS 2109, mean/median reward 9.2/7.0, min/max reward 0.0/32.0, entropy 0.99043, value loss 0.59518, policy loss 0.00545\n",
      "Updates 62100, num timesteps 4968080, FPS 2110, mean/median reward 13.9/10.0, min/max reward 0.0/43.0, entropy 0.88429, value loss 0.30171, policy loss 0.06804\n",
      "Updates 62200, num timesteps 4976080, FPS 2110, mean/median reward 17.2/11.0, min/max reward 0.0/66.0, entropy 0.84531, value loss 0.36565, policy loss -0.04852\n",
      "Updates 62300, num timesteps 4984080, FPS 2110, mean/median reward 4.6/0.0, min/max reward 0.0/34.0, entropy 0.16565, value loss 0.04439, policy loss -0.01768\n",
      "Updates 62400, num timesteps 4992080, FPS 2107, mean/median reward 0.4/0.0, min/max reward 0.0/1.0, entropy 0.17685, value loss 0.03295, policy loss 0.02463\n",
      "Updates 62500, num timesteps 5000080, FPS 2106, mean/median reward 1.4/1.0, min/max reward 0.0/5.0, entropy 1.21137, value loss 0.03114, policy loss 0.01812\n",
      "Updates 62600, num timesteps 5008080, FPS 2106, mean/median reward 4.7/3.0, min/max reward 0.0/16.0, entropy 1.09640, value loss 0.01062, policy loss 0.01876\n",
      "Updates 62700, num timesteps 5016080, FPS 2106, mean/median reward 6.2/5.0, min/max reward 0.0/16.0, entropy 0.98857, value loss 0.02393, policy loss -0.02738\n",
      "Updates 62800, num timesteps 5024080, FPS 2106, mean/median reward 16.9/8.0, min/max reward 0.0/56.0, entropy 0.85375, value loss 0.07056, policy loss -0.03315\n",
      "Updates 62900, num timesteps 5032080, FPS 2107, mean/median reward 23.1/15.0, min/max reward 3.0/86.0, entropy 0.85270, value loss 0.08470, policy loss -0.02994\n",
      "Updates 63000, num timesteps 5040080, FPS 2107, mean/median reward 8.6/2.0, min/max reward 0.0/38.0, entropy 0.95023, value loss 0.07937, policy loss 0.01264\n",
      "Updates 63100, num timesteps 5048080, FPS 2107, mean/median reward 12.0/6.0, min/max reward 0.0/38.0, entropy 0.82573, value loss 0.04531, policy loss 0.01007\n",
      "Updates 63200, num timesteps 5056080, FPS 2107, mean/median reward 17.6/12.0, min/max reward 1.0/47.0, entropy 0.84040, value loss 0.16071, policy loss -0.18328\n",
      "Updates 63300, num timesteps 5064080, FPS 2107, mean/median reward 29.3/14.0, min/max reward 2.0/89.0, entropy 0.76147, value loss 0.11693, policy loss 0.06433\n",
      "Updates 63400, num timesteps 5072080, FPS 2108, mean/median reward 16.1/8.0, min/max reward 3.0/51.0, entropy 0.88860, value loss 0.03378, policy loss -0.05508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 63500, num timesteps 5080080, FPS 2108, mean/median reward 11.4/7.0, min/max reward 0.0/50.0, entropy 0.93191, value loss 0.11934, policy loss 0.15172\n",
      "Updates 63600, num timesteps 5088080, FPS 2108, mean/median reward 17.3/8.0, min/max reward 0.0/86.0, entropy 0.98782, value loss 0.24210, policy loss 0.06463\n",
      "Updates 63700, num timesteps 5096080, FPS 2108, mean/median reward 9.9/6.0, min/max reward 0.0/37.0, entropy 0.94741, value loss 0.36724, policy loss -0.09425\n",
      "Updates 63800, num timesteps 5104080, FPS 2108, mean/median reward 8.0/4.0, min/max reward 0.0/37.0, entropy 0.96391, value loss 0.03663, policy loss -0.03640\n",
      "Updates 63900, num timesteps 5112080, FPS 2109, mean/median reward 17.3/8.0, min/max reward 0.0/67.0, entropy 0.83075, value loss 0.08821, policy loss -0.11966\n",
      "Updates 64000, num timesteps 5120080, FPS 2109, mean/median reward 14.3/7.0, min/max reward 0.0/36.0, entropy 0.95977, value loss 0.24921, policy loss 0.04732\n",
      "Updates 64100, num timesteps 5128080, FPS 2109, mean/median reward 19.8/13.0, min/max reward 2.0/78.0, entropy 1.00480, value loss 0.03266, policy loss 0.00204\n",
      "Updates 64200, num timesteps 5136080, FPS 2109, mean/median reward 14.6/9.0, min/max reward 2.0/67.0, entropy 0.96013, value loss 0.02622, policy loss -0.00244\n",
      "Updates 64300, num timesteps 5144080, FPS 2109, mean/median reward 12.8/8.0, min/max reward 0.0/43.0, entropy 0.96613, value loss 0.04262, policy loss -0.06678\n",
      "Updates 64400, num timesteps 5152080, FPS 2109, mean/median reward 17.2/9.0, min/max reward 0.0/67.0, entropy 1.01335, value loss 0.06765, policy loss 0.01365\n",
      "Updates 64500, num timesteps 5160080, FPS 2110, mean/median reward 16.0/6.0, min/max reward 0.0/67.0, entropy 0.76007, value loss 0.13446, policy loss 0.03209\n",
      "Updates 64600, num timesteps 5168080, FPS 2110, mean/median reward 17.8/7.0, min/max reward 0.0/67.0, entropy 0.73989, value loss 0.34349, policy loss 0.05572\n",
      "Updates 64700, num timesteps 5176080, FPS 2110, mean/median reward 14.9/6.0, min/max reward 0.0/55.0, entropy 0.89549, value loss 0.02947, policy loss 0.01286\n",
      "Updates 64800, num timesteps 5184080, FPS 2110, mean/median reward 11.0/2.0, min/max reward 0.0/60.0, entropy 0.98515, value loss 0.10109, policy loss 0.06975\n",
      "Updates 64900, num timesteps 5192080, FPS 2110, mean/median reward 9.8/2.0, min/max reward 0.0/39.0, entropy 0.80341, value loss 0.03268, policy loss 0.01125\n",
      "Updates 65000, num timesteps 5200080, FPS 2110, mean/median reward 20.4/6.0, min/max reward 0.0/89.0, entropy 1.11212, value loss 0.07410, policy loss 0.02049\n",
      "Updates 65100, num timesteps 5208080, FPS 2111, mean/median reward 9.8/6.0, min/max reward 0.0/37.0, entropy 0.88369, value loss 0.09057, policy loss -0.07633\n",
      "Updates 65200, num timesteps 5216080, FPS 2111, mean/median reward 11.4/1.0, min/max reward 0.0/75.0, entropy 0.53044, value loss 0.30526, policy loss -0.01147\n",
      "Updates 65300, num timesteps 5224080, FPS 2110, mean/median reward 8.1/2.0, min/max reward 0.0/46.0, entropy 1.08718, value loss 0.00628, policy loss 0.01125\n",
      "Updates 65400, num timesteps 5232080, FPS 2110, mean/median reward 10.7/4.0, min/max reward 0.0/46.0, entropy 0.80764, value loss 0.03366, policy loss 0.02523\n",
      "Updates 65500, num timesteps 5240080, FPS 2111, mean/median reward 18.3/13.0, min/max reward 2.0/49.0, entropy 0.90521, value loss 1.06886, policy loss -0.00716\n",
      "Updates 65600, num timesteps 5248080, FPS 2111, mean/median reward 25.0/12.0, min/max reward 0.0/97.0, entropy 0.77698, value loss 0.08051, policy loss -0.00909\n",
      "Updates 65700, num timesteps 5256080, FPS 2111, mean/median reward 18.5/7.0, min/max reward 0.0/77.0, entropy 1.03467, value loss 0.21273, policy loss -0.06190\n",
      "Updates 65800, num timesteps 5264080, FPS 2111, mean/median reward 11.6/6.0, min/max reward 0.0/92.0, entropy 1.05091, value loss 0.03176, policy loss 0.02280\n",
      "Updates 65900, num timesteps 5272080, FPS 2111, mean/median reward 5.2/2.0, min/max reward 0.0/17.0, entropy 0.89285, value loss 0.06249, policy loss -0.04327\n",
      "Updates 66000, num timesteps 5280080, FPS 2111, mean/median reward 7.6/5.0, min/max reward 0.0/23.0, entropy 0.99607, value loss 0.10967, policy loss 0.03536\n",
      "Updates 66100, num timesteps 5288080, FPS 2112, mean/median reward 7.1/4.0, min/max reward 0.0/34.0, entropy 0.92660, value loss 0.03352, policy loss 0.00424\n",
      "Updates 66200, num timesteps 5296080, FPS 2112, mean/median reward 11.4/5.0, min/max reward 0.0/47.0, entropy 1.02891, value loss 0.01797, policy loss -0.01370\n",
      "Updates 66300, num timesteps 5304080, FPS 2112, mean/median reward 17.9/14.0, min/max reward 1.0/45.0, entropy 0.91611, value loss 0.21086, policy loss 0.08840\n",
      "Updates 66400, num timesteps 5312080, FPS 2112, mean/median reward 11.4/6.0, min/max reward 1.0/39.0, entropy 1.07494, value loss 0.06590, policy loss -0.02882\n",
      "Updates 66500, num timesteps 5320080, FPS 2112, mean/median reward 13.4/8.0, min/max reward 2.0/44.0, entropy 1.02860, value loss 0.04508, policy loss -0.00103\n",
      "Updates 66600, num timesteps 5328080, FPS 2112, mean/median reward 20.0/17.0, min/max reward 2.0/59.0, entropy 0.92608, value loss 0.03556, policy loss 0.06917\n",
      "Updates 66700, num timesteps 5336080, FPS 2112, mean/median reward 16.1/11.0, min/max reward 1.0/42.0, entropy 0.96217, value loss 0.03834, policy loss 0.06015\n",
      "Updates 66800, num timesteps 5344080, FPS 2113, mean/median reward 20.9/11.0, min/max reward 1.0/69.0, entropy 1.06827, value loss 0.03746, policy loss 0.08194\n",
      "Updates 66900, num timesteps 5352080, FPS 2113, mean/median reward 13.9/11.0, min/max reward 1.0/41.0, entropy 1.11416, value loss 0.12408, policy loss 0.12403\n",
      "Updates 67000, num timesteps 5360080, FPS 2113, mean/median reward 19.6/10.0, min/max reward 1.0/91.0, entropy 1.08115, value loss 0.02271, policy loss 0.02825\n",
      "Updates 67100, num timesteps 5368080, FPS 2113, mean/median reward 9.0/6.0, min/max reward 0.0/26.0, entropy 0.84000, value loss 0.02330, policy loss -0.02553\n",
      "Updates 67200, num timesteps 5376080, FPS 2113, mean/median reward 10.1/3.0, min/max reward 0.0/36.0, entropy 0.97923, value loss 0.18599, policy loss -0.03162\n",
      "Updates 67300, num timesteps 5384080, FPS 2114, mean/median reward 17.8/7.0, min/max reward 0.0/71.0, entropy 0.95378, value loss 0.11591, policy loss -0.02061\n",
      "Updates 67400, num timesteps 5392080, FPS 2114, mean/median reward 14.8/7.0, min/max reward 0.0/71.0, entropy 0.91328, value loss 0.08298, policy loss 0.02738\n",
      "Updates 67500, num timesteps 5400080, FPS 2114, mean/median reward 17.9/5.0, min/max reward 0.0/81.0, entropy 0.96166, value loss 0.05682, policy loss 0.01418\n",
      "Updates 67600, num timesteps 5408080, FPS 2114, mean/median reward 11.4/4.0, min/max reward 0.0/42.0, entropy 1.03676, value loss 0.04662, policy loss -0.07072\n",
      "Updates 67700, num timesteps 5416080, FPS 2115, mean/median reward 15.8/10.0, min/max reward 0.0/55.0, entropy 0.85785, value loss 0.41467, policy loss -0.07767\n",
      "Updates 67800, num timesteps 5424080, FPS 2115, mean/median reward 16.8/11.0, min/max reward 1.0/73.0, entropy 0.87062, value loss 0.03169, policy loss 0.05494\n",
      "Updates 67900, num timesteps 5432080, FPS 2115, mean/median reward 25.1/12.0, min/max reward 1.0/73.0, entropy 0.91781, value loss 0.08877, policy loss 0.02739\n",
      "Updates 68000, num timesteps 5440080, FPS 2115, mean/median reward 17.1/5.0, min/max reward 0.0/83.0, entropy 0.88784, value loss 0.07290, policy loss -0.03555\n",
      "Updates 68100, num timesteps 5448080, FPS 2115, mean/median reward 14.6/8.0, min/max reward 0.0/62.0, entropy 0.90799, value loss 0.05262, policy loss -0.10221\n",
      "Updates 68200, num timesteps 5456080, FPS 2115, mean/median reward 23.2/20.0, min/max reward 0.0/71.0, entropy 0.87260, value loss 0.15658, policy loss 0.03517\n",
      "Updates 68300, num timesteps 5464080, FPS 2115, mean/median reward 13.2/5.0, min/max reward 1.0/69.0, entropy 1.04777, value loss 0.04178, policy loss -0.00740\n",
      "Updates 68400, num timesteps 5472080, FPS 2115, mean/median reward 11.5/5.0, min/max reward 0.0/48.0, entropy 0.97260, value loss 0.02215, policy loss 0.04363\n",
      "Updates 68500, num timesteps 5480080, FPS 2115, mean/median reward 11.6/5.0, min/max reward 0.0/53.0, entropy 0.97378, value loss 0.07598, policy loss 0.10643\n",
      "Updates 68600, num timesteps 5488080, FPS 2116, mean/median reward 15.2/9.0, min/max reward 0.0/87.0, entropy 0.95848, value loss 0.09089, policy loss -0.03318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 68700, num timesteps 5496080, FPS 2116, mean/median reward 20.6/9.0, min/max reward 0.0/104.0, entropy 0.94558, value loss 0.07054, policy loss -0.01645\n",
      "Updates 68800, num timesteps 5504080, FPS 2116, mean/median reward 21.2/9.0, min/max reward 0.0/104.0, entropy 0.99871, value loss 0.11643, policy loss 0.09169\n",
      "Updates 68900, num timesteps 5512080, FPS 2117, mean/median reward 17.6/7.0, min/max reward 1.0/96.0, entropy 0.92998, value loss 0.01638, policy loss -0.02717\n",
      "Updates 69000, num timesteps 5520080, FPS 2117, mean/median reward 18.9/7.0, min/max reward 0.0/82.0, entropy 0.90898, value loss 0.04563, policy loss 0.02175\n",
      "Updates 69100, num timesteps 5528080, FPS 2117, mean/median reward 6.6/3.0, min/max reward 0.0/31.0, entropy 1.00769, value loss 0.02002, policy loss -0.01730\n",
      "Updates 69200, num timesteps 5536080, FPS 2118, mean/median reward 9.5/4.0, min/max reward 0.0/34.0, entropy 1.01901, value loss 0.01332, policy loss -0.00461\n",
      "Updates 69300, num timesteps 5544080, FPS 2118, mean/median reward 16.9/8.0, min/max reward 0.0/46.0, entropy 0.93662, value loss 0.07853, policy loss -0.08427\n",
      "Updates 69400, num timesteps 5552080, FPS 2118, mean/median reward 19.6/11.0, min/max reward 0.0/51.0, entropy 0.80169, value loss 0.08613, policy loss 0.06438\n",
      "Updates 69500, num timesteps 5560080, FPS 2118, mean/median reward 16.7/11.0, min/max reward 0.0/66.0, entropy 0.89304, value loss 0.12592, policy loss -0.02072\n",
      "Updates 69600, num timesteps 5568080, FPS 2119, mean/median reward 10.2/4.0, min/max reward 0.0/66.0, entropy 1.04555, value loss 0.06105, policy loss -0.02855\n",
      "Updates 69700, num timesteps 5576080, FPS 2119, mean/median reward 16.7/10.0, min/max reward 0.0/79.0, entropy 1.13373, value loss 0.02144, policy loss -0.03306\n",
      "Updates 69800, num timesteps 5584080, FPS 2119, mean/median reward 12.4/11.0, min/max reward 0.0/33.0, entropy 0.93384, value loss 0.05747, policy loss 0.02330\n",
      "Updates 69900, num timesteps 5592080, FPS 2119, mean/median reward 21.3/14.0, min/max reward 0.0/70.0, entropy 0.93818, value loss 0.05767, policy loss 0.00487\n",
      "Updates 70000, num timesteps 5600080, FPS 2119, mean/median reward 21.0/15.0, min/max reward 1.0/70.0, entropy 0.97797, value loss 0.06472, policy loss 0.02032\n",
      "Updates 70100, num timesteps 5608080, FPS 2119, mean/median reward 13.1/7.0, min/max reward 0.0/38.0, entropy 1.01341, value loss 0.04199, policy loss -0.00996\n",
      "Updates 70200, num timesteps 5616080, FPS 2119, mean/median reward 19.7/7.0, min/max reward 0.0/93.0, entropy 0.93575, value loss 0.06430, policy loss -0.00170\n",
      "Updates 70300, num timesteps 5624080, FPS 2119, mean/median reward 17.4/5.0, min/max reward 0.0/84.0, entropy 0.83639, value loss 0.04309, policy loss 0.01322\n",
      "Updates 70400, num timesteps 5632080, FPS 2120, mean/median reward 17.6/10.0, min/max reward 0.0/84.0, entropy 0.77459, value loss 1.06627, policy loss -0.06576\n",
      "Updates 70500, num timesteps 5640080, FPS 2120, mean/median reward 28.6/12.0, min/max reward 1.0/105.0, entropy 0.87883, value loss 1.06158, policy loss -0.24670\n",
      "Updates 70600, num timesteps 5648080, FPS 2120, mean/median reward 6.2/2.0, min/max reward 0.0/37.0, entropy 0.93666, value loss 0.08335, policy loss 0.03224\n",
      "Updates 70700, num timesteps 5656080, FPS 2120, mean/median reward 12.4/6.0, min/max reward 0.0/49.0, entropy 0.84573, value loss 0.04677, policy loss 0.04477\n",
      "Updates 70800, num timesteps 5664080, FPS 2120, mean/median reward 15.9/7.0, min/max reward 0.0/71.0, entropy 0.88821, value loss 0.12364, policy loss -0.02348\n",
      "Updates 70900, num timesteps 5672080, FPS 2121, mean/median reward 16.1/9.0, min/max reward 0.0/53.0, entropy 0.95672, value loss 0.14788, policy loss -0.03865\n",
      "Updates 71000, num timesteps 5680080, FPS 2121, mean/median reward 12.6/8.0, min/max reward 0.0/51.0, entropy 1.09470, value loss 0.04177, policy loss 0.00858\n",
      "Updates 71100, num timesteps 5688080, FPS 2121, mean/median reward 10.7/3.0, min/max reward 0.0/73.0, entropy 0.98877, value loss 0.03636, policy loss -0.05749\n",
      "Updates 71200, num timesteps 5696080, FPS 2121, mean/median reward 17.3/15.0, min/max reward 0.0/60.0, entropy 0.91085, value loss 0.14190, policy loss 0.12928\n",
      "Updates 71300, num timesteps 5704080, FPS 2122, mean/median reward 14.4/13.0, min/max reward 0.0/48.0, entropy 0.93985, value loss 0.30700, policy loss -0.06798\n",
      "Updates 71400, num timesteps 5712080, FPS 2122, mean/median reward 20.2/10.0, min/max reward 4.0/69.0, entropy 0.97162, value loss 0.08044, policy loss -0.02248\n",
      "Updates 71500, num timesteps 5720080, FPS 2122, mean/median reward 14.2/8.0, min/max reward 1.0/63.0, entropy 0.99973, value loss 0.02466, policy loss 0.01460\n",
      "Updates 71600, num timesteps 5728080, FPS 2122, mean/median reward 16.6/8.0, min/max reward 1.0/55.0, entropy 0.87825, value loss 0.31879, policy loss 0.12607\n",
      "Updates 71700, num timesteps 5736080, FPS 2122, mean/median reward 23.0/9.0, min/max reward 0.0/85.0, entropy 0.90713, value loss 0.06091, policy loss -0.04216\n",
      "Updates 71800, num timesteps 5744080, FPS 2122, mean/median reward 10.0/9.0, min/max reward 0.0/34.0, entropy 0.95233, value loss 0.06184, policy loss -0.01817\n",
      "Updates 71900, num timesteps 5752080, FPS 2122, mean/median reward 10.1/7.0, min/max reward 0.0/25.0, entropy 0.88743, value loss 0.04511, policy loss -0.04397\n",
      "Updates 72000, num timesteps 5760080, FPS 2123, mean/median reward 16.9/15.0, min/max reward 0.0/43.0, entropy 0.89773, value loss 0.04263, policy loss 0.03308\n",
      "Updates 72100, num timesteps 5768080, FPS 2123, mean/median reward 30.3/25.0, min/max reward 1.0/72.0, entropy 1.00839, value loss 0.04704, policy loss 0.01605\n",
      "Updates 72200, num timesteps 5776080, FPS 2123, mean/median reward 14.4/5.0, min/max reward 0.0/62.0, entropy 0.88989, value loss 0.04131, policy loss 0.05644\n",
      "Updates 72300, num timesteps 5784080, FPS 2123, mean/median reward 20.5/11.0, min/max reward 0.0/62.0, entropy 0.93388, value loss 0.06323, policy loss -0.05147\n",
      "Updates 72400, num timesteps 5792080, FPS 2123, mean/median reward 24.6/11.0, min/max reward 0.0/92.0, entropy 0.85703, value loss 0.03487, policy loss 0.02521\n",
      "Updates 72500, num timesteps 5800080, FPS 2124, mean/median reward 18.2/7.0, min/max reward 1.0/60.0, entropy 0.85134, value loss 0.08290, policy loss 0.03483\n",
      "Updates 72600, num timesteps 5808080, FPS 2124, mean/median reward 12.5/2.0, min/max reward 0.0/47.0, entropy 0.94677, value loss 0.05733, policy loss -0.04365\n",
      "Updates 72700, num timesteps 5816080, FPS 2124, mean/median reward 10.4/8.0, min/max reward 0.0/43.0, entropy 0.73091, value loss 0.43705, policy loss -0.00795\n",
      "Updates 72800, num timesteps 5824080, FPS 2124, mean/median reward 9.8/7.0, min/max reward 0.0/43.0, entropy 0.93015, value loss 0.04684, policy loss -0.00246\n",
      "Updates 72900, num timesteps 5832080, FPS 2124, mean/median reward 15.9/7.0, min/max reward 1.0/90.0, entropy 0.93726, value loss 0.09322, policy loss 0.05815\n",
      "Updates 73000, num timesteps 5840080, FPS 2125, mean/median reward 21.6/6.0, min/max reward 0.0/70.0, entropy 0.97677, value loss 0.10724, policy loss 0.02843\n",
      "Updates 73100, num timesteps 5848080, FPS 2125, mean/median reward 17.2/3.0, min/max reward 0.0/70.0, entropy 0.85706, value loss 0.01394, policy loss 0.01975\n",
      "Updates 73200, num timesteps 5856080, FPS 2125, mean/median reward 17.1/8.0, min/max reward 0.0/101.0, entropy 1.00640, value loss 0.19603, policy loss 0.00815\n",
      "Updates 73300, num timesteps 5864080, FPS 2125, mean/median reward 14.4/4.0, min/max reward 0.0/65.0, entropy 0.98143, value loss 0.12468, policy loss 0.08920\n",
      "Updates 73400, num timesteps 5872080, FPS 2126, mean/median reward 16.8/5.0, min/max reward 0.0/70.0, entropy 0.92813, value loss 0.04630, policy loss 0.03705\n",
      "Updates 73500, num timesteps 5880080, FPS 2126, mean/median reward 14.7/4.0, min/max reward 0.0/63.0, entropy 1.05333, value loss 0.08219, policy loss 0.02396\n",
      "Updates 73600, num timesteps 5888080, FPS 2126, mean/median reward 13.1/4.0, min/max reward 0.0/58.0, entropy 0.92314, value loss 0.23831, policy loss -0.05493\n",
      "Updates 73700, num timesteps 5896080, FPS 2126, mean/median reward 5.0/3.0, min/max reward 0.0/22.0, entropy 1.06732, value loss 0.05108, policy loss -0.03882\n",
      "Updates 73800, num timesteps 5904080, FPS 2126, mean/median reward 15.1/4.0, min/max reward 0.0/98.0, entropy 0.93683, value loss 0.12509, policy loss -0.03779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 73900, num timesteps 5912080, FPS 2126, mean/median reward 10.1/0.0, min/max reward 0.0/70.0, entropy 0.85985, value loss 0.32245, policy loss -0.11392\n",
      "Updates 74000, num timesteps 5920080, FPS 2127, mean/median reward 9.4/2.0, min/max reward 0.0/39.0, entropy 0.86024, value loss 0.07816, policy loss -0.02951\n",
      "Updates 74100, num timesteps 5928080, FPS 2127, mean/median reward 25.4/11.0, min/max reward 2.0/59.0, entropy 0.75124, value loss 0.06785, policy loss 0.04836\n",
      "Updates 74200, num timesteps 5936080, FPS 2127, mean/median reward 21.3/10.0, min/max reward 0.0/86.0, entropy 0.91529, value loss 0.10247, policy loss -0.10198\n",
      "Updates 74300, num timesteps 5944080, FPS 2127, mean/median reward 9.4/5.0, min/max reward 0.0/50.0, entropy 0.95950, value loss 0.02334, policy loss 0.04261\n",
      "Updates 74400, num timesteps 5952080, FPS 2127, mean/median reward 13.1/6.0, min/max reward 1.0/51.0, entropy 1.06353, value loss 0.38487, policy loss -0.02611\n",
      "Updates 74500, num timesteps 5960080, FPS 2128, mean/median reward 13.6/6.0, min/max reward 0.0/51.0, entropy 0.99343, value loss 0.18312, policy loss -0.07362\n",
      "Updates 74600, num timesteps 5968080, FPS 2127, mean/median reward 13.9/13.0, min/max reward 0.0/38.0, entropy 0.90444, value loss 0.11357, policy loss -0.03389\n",
      "Updates 74700, num timesteps 5976080, FPS 2127, mean/median reward 14.1/13.0, min/max reward 0.0/35.0, entropy 1.09744, value loss 0.03860, policy loss 0.01363\n",
      "Updates 74800, num timesteps 5984080, FPS 2128, mean/median reward 9.3/6.0, min/max reward 0.0/27.0, entropy 0.85472, value loss 0.07197, policy loss 0.01903\n",
      "Updates 74900, num timesteps 5992080, FPS 2128, mean/median reward 11.3/2.0, min/max reward 0.0/73.0, entropy 1.00075, value loss 0.05977, policy loss -0.05100\n",
      "Updates 75000, num timesteps 6000080, FPS 2128, mean/median reward 21.4/14.0, min/max reward 0.0/73.0, entropy 0.89119, value loss 0.04623, policy loss -0.00241\n",
      "Updates 75100, num timesteps 6008080, FPS 2128, mean/median reward 11.8/5.0, min/max reward 0.0/49.0, entropy 0.84946, value loss 1.27391, policy loss -0.59330\n",
      "Updates 75200, num timesteps 6016080, FPS 2128, mean/median reward 16.2/7.0, min/max reward 0.0/60.0, entropy 0.99533, value loss 0.12746, policy loss -0.07777\n",
      "Updates 75300, num timesteps 6024080, FPS 2129, mean/median reward 13.1/7.0, min/max reward 0.0/62.0, entropy 0.93640, value loss 0.01010, policy loss 0.01484\n",
      "Updates 75400, num timesteps 6032080, FPS 2129, mean/median reward 16.8/9.0, min/max reward 0.0/46.0, entropy 0.82567, value loss 0.03771, policy loss -0.00187\n",
      "Updates 75500, num timesteps 6040080, FPS 2129, mean/median reward 15.6/6.0, min/max reward 0.0/55.0, entropy 0.87952, value loss 0.14964, policy loss -0.10150\n",
      "Updates 75600, num timesteps 6048080, FPS 2129, mean/median reward 19.6/10.0, min/max reward 0.0/93.0, entropy 0.86753, value loss 0.08515, policy loss -0.02030\n",
      "Updates 75700, num timesteps 6056080, FPS 2129, mean/median reward 12.3/3.0, min/max reward 0.0/55.0, entropy 0.97636, value loss 0.08864, policy loss 0.06445\n",
      "Updates 75800, num timesteps 6064080, FPS 2129, mean/median reward 19.5/5.0, min/max reward 0.0/74.0, entropy 0.96746, value loss 0.11365, policy loss 0.00994\n",
      "Updates 75900, num timesteps 6072080, FPS 2130, mean/median reward 11.2/3.0, min/max reward 0.0/55.0, entropy 1.07450, value loss 0.03531, policy loss -0.01741\n",
      "Updates 76000, num timesteps 6080080, FPS 2130, mean/median reward 16.5/5.0, min/max reward 0.0/90.0, entropy 0.95306, value loss 0.20221, policy loss 0.09850\n",
      "Updates 76100, num timesteps 6088080, FPS 2130, mean/median reward 17.2/9.0, min/max reward 0.0/67.0, entropy 1.01470, value loss 3.55310, policy loss -0.37747\n",
      "Updates 76200, num timesteps 6096080, FPS 2130, mean/median reward 14.2/9.0, min/max reward 1.0/48.0, entropy 1.01512, value loss 0.01751, policy loss 0.01781\n",
      "Updates 76300, num timesteps 6104080, FPS 2130, mean/median reward 13.7/11.0, min/max reward 2.0/31.0, entropy 1.12855, value loss 0.04762, policy loss -0.03540\n",
      "Updates 76400, num timesteps 6112080, FPS 2131, mean/median reward 21.3/12.0, min/max reward 2.0/86.0, entropy 0.98151, value loss 0.09253, policy loss -0.03484\n",
      "Updates 76500, num timesteps 6120080, FPS 2131, mean/median reward 20.4/11.0, min/max reward 0.0/71.0, entropy 0.94096, value loss 0.09282, policy loss 0.02201\n",
      "Updates 76600, num timesteps 6128080, FPS 2131, mean/median reward 12.9/6.0, min/max reward 0.0/45.0, entropy 0.91256, value loss 0.23503, policy loss -0.05813\n",
      "Updates 76700, num timesteps 6136080, FPS 2131, mean/median reward 13.7/7.0, min/max reward 0.0/60.0, entropy 0.90650, value loss 0.22168, policy loss 0.04841\n",
      "Updates 76800, num timesteps 6144080, FPS 2132, mean/median reward 13.2/6.0, min/max reward 0.0/42.0, entropy 0.87144, value loss 0.07176, policy loss 0.04568\n",
      "Updates 76900, num timesteps 6152080, FPS 2132, mean/median reward 17.0/12.0, min/max reward 0.0/53.0, entropy 0.96266, value loss 0.06594, policy loss 0.01706\n",
      "Updates 77000, num timesteps 6160080, FPS 2132, mean/median reward 25.1/10.0, min/max reward 0.0/87.0, entropy 1.00061, value loss 0.05114, policy loss 0.04932\n",
      "Updates 77100, num timesteps 6168080, FPS 2132, mean/median reward 13.9/7.0, min/max reward 1.0/53.0, entropy 1.04414, value loss 0.02902, policy loss -0.07723\n",
      "Updates 77200, num timesteps 6176080, FPS 2132, mean/median reward 18.9/18.0, min/max reward 1.0/53.0, entropy 0.86717, value loss 0.03876, policy loss -0.04194\n",
      "Updates 77300, num timesteps 6184080, FPS 2132, mean/median reward 19.2/12.0, min/max reward 0.0/55.0, entropy 0.74710, value loss 0.03663, policy loss 0.03094\n",
      "Updates 77400, num timesteps 6192080, FPS 2133, mean/median reward 25.7/16.0, min/max reward 5.0/70.0, entropy 0.96586, value loss 0.05509, policy loss 0.02312\n",
      "Updates 77500, num timesteps 6200080, FPS 2133, mean/median reward 21.9/8.0, min/max reward 0.0/93.0, entropy 1.02162, value loss 0.02189, policy loss -0.00168\n",
      "Updates 77600, num timesteps 6208080, FPS 2133, mean/median reward 14.4/6.0, min/max reward 0.0/46.0, entropy 0.91183, value loss 0.25896, policy loss -0.09298\n",
      "Updates 77700, num timesteps 6216080, FPS 2133, mean/median reward 9.6/3.0, min/max reward 0.0/75.0, entropy 0.87131, value loss 0.08053, policy loss -0.08186\n",
      "Updates 77800, num timesteps 6224080, FPS 2133, mean/median reward 7.8/5.0, min/max reward 0.0/39.0, entropy 1.11229, value loss 0.02623, policy loss 0.07788\n",
      "Updates 77900, num timesteps 6232080, FPS 2133, mean/median reward 9.2/7.0, min/max reward 0.0/29.0, entropy 0.86489, value loss 0.03639, policy loss 0.03413\n",
      "Updates 78000, num timesteps 6240080, FPS 2133, mean/median reward 14.4/11.0, min/max reward 0.0/40.0, entropy 0.83493, value loss 0.05044, policy loss 0.03297\n",
      "Updates 78100, num timesteps 6248080, FPS 2133, mean/median reward 20.9/14.0, min/max reward 0.0/64.0, entropy 0.97481, value loss 0.10742, policy loss -0.00381\n",
      "Updates 78200, num timesteps 6256080, FPS 2133, mean/median reward 15.2/9.0, min/max reward 0.0/57.0, entropy 0.91715, value loss 0.53289, policy loss -0.04567\n",
      "Updates 78300, num timesteps 6264080, FPS 2134, mean/median reward 7.2/3.0, min/max reward 0.0/30.0, entropy 1.03692, value loss 0.05228, policy loss 0.04234\n",
      "Updates 78400, num timesteps 6272080, FPS 2134, mean/median reward 20.7/9.0, min/max reward 0.0/63.0, entropy 1.02243, value loss 0.05754, policy loss 0.02825\n",
      "Updates 78500, num timesteps 6280080, FPS 2134, mean/median reward 22.7/16.0, min/max reward 1.0/50.0, entropy 0.89122, value loss 0.05381, policy loss -0.04346\n",
      "Updates 78600, num timesteps 6288080, FPS 2134, mean/median reward 19.3/11.0, min/max reward 0.0/50.0, entropy 0.92614, value loss 0.36104, policy loss -0.09581\n",
      "Updates 78700, num timesteps 6296080, FPS 2134, mean/median reward 13.7/6.0, min/max reward 0.0/62.0, entropy 1.07952, value loss 0.07805, policy loss -0.05351\n",
      "Updates 78800, num timesteps 6304080, FPS 2134, mean/median reward 17.8/5.0, min/max reward 0.0/62.0, entropy 0.98533, value loss 0.10537, policy loss -0.08237\n",
      "Updates 78900, num timesteps 6312080, FPS 2134, mean/median reward 17.4/12.0, min/max reward 1.0/66.0, entropy 0.93785, value loss 0.12059, policy loss -0.02565\n",
      "Updates 79000, num timesteps 6320080, FPS 2135, mean/median reward 21.7/15.0, min/max reward 0.0/53.0, entropy 0.97698, value loss 0.05214, policy loss -0.00602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79100, num timesteps 6328080, FPS 2135, mean/median reward 9.1/5.0, min/max reward 0.0/36.0, entropy 0.97228, value loss 0.10460, policy loss -0.02618\n",
      "Updates 79200, num timesteps 6336080, FPS 2135, mean/median reward 7.5/4.0, min/max reward 0.0/49.0, entropy 0.92150, value loss 0.04655, policy loss -0.01472\n",
      "Updates 79300, num timesteps 6344080, FPS 2135, mean/median reward 13.0/8.0, min/max reward 0.0/48.0, entropy 1.00232, value loss 0.04810, policy loss 0.04901\n",
      "Updates 79400, num timesteps 6352080, FPS 2136, mean/median reward 15.3/6.0, min/max reward 0.0/80.0, entropy 0.77696, value loss 0.14421, policy loss 0.04545\n",
      "Updates 79500, num timesteps 6360080, FPS 2136, mean/median reward 12.2/5.0, min/max reward 0.0/48.0, entropy 0.96945, value loss 0.37632, policy loss -0.14876\n",
      "Updates 79600, num timesteps 6368080, FPS 2137, mean/median reward 22.1/11.0, min/max reward 0.0/84.0, entropy 0.95012, value loss 0.05050, policy loss -0.00828\n",
      "Updates 79700, num timesteps 6376080, FPS 2137, mean/median reward 16.9/6.0, min/max reward 0.0/64.0, entropy 0.98555, value loss 0.08154, policy loss 0.05546\n",
      "Updates 79800, num timesteps 6384080, FPS 2137, mean/median reward 12.1/8.0, min/max reward 1.0/33.0, entropy 1.02333, value loss 0.01480, policy loss -0.03056\n",
      "Updates 79900, num timesteps 6392080, FPS 2137, mean/median reward 17.0/13.0, min/max reward 2.0/58.0, entropy 0.91590, value loss 0.04229, policy loss -0.07743\n",
      "Updates 80000, num timesteps 6400080, FPS 2137, mean/median reward 20.6/13.0, min/max reward 0.0/48.0, entropy 0.79116, value loss 0.10510, policy loss 0.09077\n",
      "Updates 80100, num timesteps 6408080, FPS 2138, mean/median reward 20.6/6.0, min/max reward 0.0/87.0, entropy 0.84006, value loss 0.19280, policy loss 0.01980\n",
      "Updates 80200, num timesteps 6416080, FPS 2138, mean/median reward 16.5/6.0, min/max reward 1.0/75.0, entropy 0.98105, value loss 0.03854, policy loss 0.03703\n",
      "Updates 80300, num timesteps 6424080, FPS 2138, mean/median reward 19.6/6.0, min/max reward 1.0/89.0, entropy 0.83749, value loss 1.17222, policy loss -0.04996\n",
      "Updates 80400, num timesteps 6432080, FPS 2138, mean/median reward 36.2/40.0, min/max reward 0.0/88.0, entropy 0.84979, value loss 0.05065, policy loss 0.02520\n",
      "Updates 80500, num timesteps 6440080, FPS 2139, mean/median reward 18.4/4.0, min/max reward 0.0/71.0, entropy 0.98638, value loss 0.06680, policy loss -0.05174\n",
      "Updates 80600, num timesteps 6448080, FPS 2139, mean/median reward 8.2/2.0, min/max reward 0.0/36.0, entropy 0.91823, value loss 0.02975, policy loss 0.00016\n",
      "Updates 80700, num timesteps 6456080, FPS 2139, mean/median reward 11.5/4.0, min/max reward 0.0/49.0, entropy 0.93813, value loss 0.11365, policy loss 0.07012\n",
      "Updates 80800, num timesteps 6464080, FPS 2139, mean/median reward 14.1/11.0, min/max reward 0.0/54.0, entropy 0.82359, value loss 0.17753, policy loss -0.09763\n",
      "Updates 80900, num timesteps 6472080, FPS 2139, mean/median reward 21.4/9.0, min/max reward 1.0/80.0, entropy 0.96250, value loss 0.05857, policy loss -0.04990\n",
      "Updates 81000, num timesteps 6480080, FPS 2140, mean/median reward 21.5/17.0, min/max reward 2.0/62.0, entropy 0.91402, value loss 0.20207, policy loss 0.02160\n",
      "Updates 81100, num timesteps 6488080, FPS 2140, mean/median reward 18.2/9.0, min/max reward 1.0/65.0, entropy 0.72222, value loss 0.06583, policy loss 0.02856\n",
      "Updates 81200, num timesteps 6496080, FPS 2140, mean/median reward 30.1/26.0, min/max reward 1.0/92.0, entropy 1.04702, value loss 0.07164, policy loss -0.00418\n",
      "Updates 81300, num timesteps 6504080, FPS 2140, mean/median reward 16.4/6.0, min/max reward 1.0/66.0, entropy 0.80696, value loss 0.06171, policy loss 0.01560\n",
      "Updates 81400, num timesteps 6512080, FPS 2140, mean/median reward 14.4/5.0, min/max reward 1.0/68.0, entropy 0.76123, value loss 0.08306, policy loss -0.03047\n",
      "Updates 81500, num timesteps 6520080, FPS 2140, mean/median reward 25.4/6.0, min/max reward 0.0/91.0, entropy 0.81047, value loss 0.10056, policy loss -0.09917\n",
      "Updates 81600, num timesteps 6528080, FPS 2140, mean/median reward 11.4/1.0, min/max reward 0.0/91.0, entropy 0.95764, value loss 0.15414, policy loss -0.07513\n",
      "Updates 81700, num timesteps 6536080, FPS 2140, mean/median reward 15.5/1.0, min/max reward 0.0/97.0, entropy 0.87524, value loss 0.03330, policy loss -0.01556\n",
      "Updates 81800, num timesteps 6544080, FPS 2141, mean/median reward 15.6/1.0, min/max reward 0.0/75.0, entropy 0.78622, value loss 0.06393, policy loss -0.01687\n",
      "Updates 81900, num timesteps 6552080, FPS 2141, mean/median reward 14.6/5.0, min/max reward 0.0/44.0, entropy 0.91393, value loss 0.06203, policy loss -0.02858\n",
      "Updates 82000, num timesteps 6560080, FPS 2141, mean/median reward 20.1/21.0, min/max reward 0.0/66.0, entropy 1.02542, value loss 0.20489, policy loss -0.03453\n",
      "Updates 82100, num timesteps 6568080, FPS 2141, mean/median reward 22.2/15.0, min/max reward 0.0/67.0, entropy 1.08835, value loss 0.07477, policy loss 0.03959\n",
      "Updates 82200, num timesteps 6576080, FPS 2141, mean/median reward 17.9/14.0, min/max reward 0.0/62.0, entropy 0.88377, value loss 0.03433, policy loss -0.00963\n",
      "Updates 82300, num timesteps 6584080, FPS 2141, mean/median reward 14.6/12.0, min/max reward 0.0/62.0, entropy 0.91604, value loss 0.05667, policy loss 0.00951\n",
      "Updates 82400, num timesteps 6592080, FPS 2142, mean/median reward 23.7/14.0, min/max reward 0.0/79.0, entropy 0.98349, value loss 0.07303, policy loss -0.00146\n",
      "Updates 82500, num timesteps 6600080, FPS 2142, mean/median reward 15.2/14.0, min/max reward 1.0/41.0, entropy 0.96875, value loss 0.14234, policy loss 0.04043\n",
      "Updates 82600, num timesteps 6608080, FPS 2142, mean/median reward 13.1/10.0, min/max reward 1.0/29.0, entropy 0.88984, value loss 0.25657, policy loss 0.01857\n",
      "Updates 82700, num timesteps 6616080, FPS 2142, mean/median reward 18.4/5.0, min/max reward 0.0/87.0, entropy 0.73939, value loss 1.10780, policy loss -0.03176\n",
      "Updates 82800, num timesteps 6624080, FPS 2142, mean/median reward 15.1/7.0, min/max reward 0.0/61.0, entropy 0.84825, value loss 0.02094, policy loss 0.02632\n",
      "Updates 82900, num timesteps 6632080, FPS 2142, mean/median reward 10.4/5.0, min/max reward 0.0/30.0, entropy 1.00761, value loss 0.03520, policy loss 0.05509\n",
      "Updates 83000, num timesteps 6640080, FPS 2143, mean/median reward 20.8/5.0, min/max reward 0.0/93.0, entropy 0.94435, value loss 0.07925, policy loss 0.02999\n",
      "Updates 83100, num timesteps 6648080, FPS 2143, mean/median reward 25.9/20.0, min/max reward 0.0/93.0, entropy 0.95034, value loss 0.02207, policy loss -0.02542\n",
      "Updates 83200, num timesteps 6656080, FPS 2143, mean/median reward 17.5/8.0, min/max reward 0.0/57.0, entropy 0.83334, value loss 0.34254, policy loss -0.02854\n",
      "Updates 83300, num timesteps 6664080, FPS 2143, mean/median reward 17.4/8.0, min/max reward 0.0/74.0, entropy 0.90770, value loss 0.20606, policy loss 0.00582\n",
      "Updates 83400, num timesteps 6672080, FPS 2143, mean/median reward 11.4/5.0, min/max reward 0.0/52.0, entropy 0.94823, value loss 0.16471, policy loss -0.00228\n",
      "Updates 83500, num timesteps 6680080, FPS 2143, mean/median reward 14.1/9.0, min/max reward 1.0/46.0, entropy 0.96147, value loss 0.19302, policy loss 0.01362\n",
      "Updates 83600, num timesteps 6688080, FPS 2144, mean/median reward 21.9/14.0, min/max reward 1.0/62.0, entropy 0.85988, value loss 0.04873, policy loss -0.01859\n",
      "Updates 83700, num timesteps 6696080, FPS 2144, mean/median reward 28.9/14.0, min/max reward 2.0/95.0, entropy 0.84640, value loss 0.14223, policy loss 0.13932\n",
      "Updates 83800, num timesteps 6704080, FPS 2144, mean/median reward 19.1/9.0, min/max reward 1.0/74.0, entropy 1.04400, value loss 0.02588, policy loss -0.02730\n",
      "Updates 83900, num timesteps 6712080, FPS 2144, mean/median reward 15.3/9.0, min/max reward 0.0/74.0, entropy 0.94564, value loss 0.02874, policy loss -0.04247\n",
      "Updates 84000, num timesteps 6720080, FPS 2144, mean/median reward 19.5/11.0, min/max reward 0.0/74.0, entropy 0.97707, value loss 0.02877, policy loss -0.06158\n",
      "Updates 84100, num timesteps 6728080, FPS 2145, mean/median reward 16.6/6.0, min/max reward 0.0/56.0, entropy 0.90797, value loss 0.04918, policy loss -0.02772\n",
      "Updates 84200, num timesteps 6736080, FPS 2145, mean/median reward 9.9/4.0, min/max reward 0.0/31.0, entropy 1.00422, value loss 0.24530, policy loss -0.07097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 84300, num timesteps 6744080, FPS 2145, mean/median reward 10.8/5.0, min/max reward 1.0/31.0, entropy 0.99767, value loss 0.02110, policy loss -0.00078\n",
      "Updates 84400, num timesteps 6752080, FPS 2145, mean/median reward 16.1/10.0, min/max reward 0.0/88.0, entropy 1.13357, value loss 0.68163, policy loss 0.10927\n",
      "Updates 84500, num timesteps 6760080, FPS 2145, mean/median reward 13.8/8.0, min/max reward 1.0/41.0, entropy 0.91458, value loss 0.63538, policy loss -0.13415\n",
      "Updates 84600, num timesteps 6768080, FPS 2145, mean/median reward 10.8/5.0, min/max reward 0.0/57.0, entropy 0.97327, value loss 0.06199, policy loss -0.02055\n",
      "Updates 84700, num timesteps 6776080, FPS 2145, mean/median reward 20.0/10.0, min/max reward 0.0/73.0, entropy 0.99847, value loss 0.57645, policy loss -0.13259\n",
      "Updates 84800, num timesteps 6784080, FPS 2145, mean/median reward 14.0/5.0, min/max reward 0.0/61.0, entropy 0.98606, value loss 0.01910, policy loss 0.01117\n",
      "Updates 84900, num timesteps 6792080, FPS 2146, mean/median reward 9.7/4.0, min/max reward 0.0/39.0, entropy 1.06081, value loss 0.19054, policy loss 0.02503\n",
      "Updates 85000, num timesteps 6800080, FPS 2146, mean/median reward 13.8/5.0, min/max reward 1.0/86.0, entropy 1.04007, value loss 1.35597, policy loss 0.39428\n",
      "Updates 85100, num timesteps 6808080, FPS 2146, mean/median reward 17.6/6.0, min/max reward 0.0/60.0, entropy 1.05477, value loss 0.06107, policy loss 0.12282\n",
      "Updates 85200, num timesteps 6816080, FPS 2146, mean/median reward 12.5/3.0, min/max reward 0.0/49.0, entropy 0.90881, value loss 0.18800, policy loss 0.07287\n",
      "Updates 85300, num timesteps 6824080, FPS 2146, mean/median reward 13.4/7.0, min/max reward 2.0/48.0, entropy 0.98046, value loss 0.11456, policy loss 0.04722\n",
      "Updates 85400, num timesteps 6832080, FPS 2146, mean/median reward 17.2/8.0, min/max reward 2.0/63.0, entropy 0.97577, value loss 0.05944, policy loss 0.05661\n",
      "Updates 85500, num timesteps 6840080, FPS 2147, mean/median reward 12.3/7.0, min/max reward 0.0/59.0, entropy 1.07562, value loss 0.10492, policy loss -0.05126\n",
      "Updates 85600, num timesteps 6848080, FPS 2147, mean/median reward 19.4/10.0, min/max reward 0.0/63.0, entropy 0.92117, value loss 0.17376, policy loss 0.08535\n",
      "Updates 85700, num timesteps 6856080, FPS 2147, mean/median reward 12.8/4.0, min/max reward 0.0/58.0, entropy 0.93389, value loss 0.02935, policy loss 0.04153\n",
      "Updates 85800, num timesteps 6864080, FPS 2147, mean/median reward 11.2/6.0, min/max reward 1.0/42.0, entropy 0.94895, value loss 0.11022, policy loss 0.00812\n",
      "Updates 85900, num timesteps 6872080, FPS 2147, mean/median reward 19.0/11.0, min/max reward 0.0/45.0, entropy 0.93924, value loss 0.10929, policy loss -0.01710\n",
      "Updates 86000, num timesteps 6880080, FPS 2147, mean/median reward 17.9/7.0, min/max reward 0.0/59.0, entropy 0.94825, value loss 0.71281, policy loss -0.11440\n",
      "Updates 86100, num timesteps 6888080, FPS 2147, mean/median reward 9.4/5.0, min/max reward 0.0/33.0, entropy 1.09603, value loss 0.06718, policy loss -0.00444\n",
      "Updates 86200, num timesteps 6896080, FPS 2147, mean/median reward 19.3/13.0, min/max reward 0.0/58.0, entropy 1.05451, value loss 0.09552, policy loss -0.00945\n",
      "Updates 86300, num timesteps 6904080, FPS 2148, mean/median reward 17.6/15.0, min/max reward 0.0/38.0, entropy 1.00528, value loss 0.03700, policy loss -0.01472\n",
      "Updates 86400, num timesteps 6912080, FPS 2148, mean/median reward 16.8/13.0, min/max reward 0.0/53.0, entropy 0.96259, value loss 0.17614, policy loss 0.00009\n",
      "Updates 86500, num timesteps 6920080, FPS 2148, mean/median reward 14.4/12.0, min/max reward 0.0/45.0, entropy 1.18139, value loss 0.33163, policy loss 0.03128\n",
      "Updates 86600, num timesteps 6928080, FPS 2148, mean/median reward 13.9/9.0, min/max reward 0.0/52.0, entropy 0.99029, value loss 0.41566, policy loss -0.00735\n",
      "Updates 86700, num timesteps 6936080, FPS 2148, mean/median reward 11.4/5.0, min/max reward 0.0/52.0, entropy 0.91256, value loss 1.10779, policy loss -0.00191\n",
      "Updates 86800, num timesteps 6944080, FPS 2149, mean/median reward 14.3/8.0, min/max reward 0.0/62.0, entropy 1.05820, value loss 0.39659, policy loss 0.22439\n",
      "Updates 86900, num timesteps 6952080, FPS 2149, mean/median reward 21.5/10.0, min/max reward 0.0/61.0, entropy 1.09365, value loss 0.05333, policy loss 0.01898\n",
      "Updates 87000, num timesteps 6960080, FPS 2149, mean/median reward 12.9/5.0, min/max reward 0.0/90.0, entropy 1.00824, value loss 0.04966, policy loss 0.00678\n",
      "Updates 87100, num timesteps 6968080, FPS 2149, mean/median reward 13.6/7.0, min/max reward 0.0/90.0, entropy 1.11349, value loss 0.03922, policy loss -0.04009\n",
      "Updates 87200, num timesteps 6976080, FPS 2149, mean/median reward 11.8/4.0, min/max reward 0.0/53.0, entropy 1.08397, value loss 0.02681, policy loss -0.00809\n",
      "Updates 87300, num timesteps 6984080, FPS 2149, mean/median reward 17.4/7.0, min/max reward 0.0/83.0, entropy 1.01366, value loss 0.15946, policy loss -0.00001\n",
      "Updates 87400, num timesteps 6992080, FPS 2150, mean/median reward 23.2/20.0, min/max reward 0.0/74.0, entropy 1.12239, value loss 0.07207, policy loss 0.11580\n",
      "Updates 87500, num timesteps 7000080, FPS 2150, mean/median reward 15.2/6.0, min/max reward 0.0/86.0, entropy 0.96153, value loss 0.03661, policy loss -0.00126\n",
      "Updates 87600, num timesteps 7008080, FPS 2150, mean/median reward 14.9/6.0, min/max reward 0.0/86.0, entropy 0.97687, value loss 0.11529, policy loss -0.07233\n",
      "Updates 87700, num timesteps 7016080, FPS 2150, mean/median reward 21.5/11.0, min/max reward 0.0/86.0, entropy 0.93120, value loss 0.04785, policy loss 0.01226\n",
      "Updates 87800, num timesteps 7024080, FPS 2150, mean/median reward 20.6/9.0, min/max reward 0.0/59.0, entropy 1.02027, value loss 0.08190, policy loss 0.06286\n",
      "Updates 87900, num timesteps 7032080, FPS 2150, mean/median reward 11.9/5.0, min/max reward 0.0/39.0, entropy 1.06113, value loss 0.04503, policy loss 0.02821\n",
      "Updates 88000, num timesteps 7040080, FPS 2151, mean/median reward 10.9/5.0, min/max reward 0.0/47.0, entropy 0.99241, value loss 0.07249, policy loss 0.12358\n",
      "Updates 88100, num timesteps 7048080, FPS 2151, mean/median reward 35.7/34.0, min/max reward 0.0/90.0, entropy 0.97892, value loss 0.06160, policy loss -0.03631\n",
      "Updates 88200, num timesteps 7056080, FPS 2151, mean/median reward 14.6/4.0, min/max reward 0.0/47.0, entropy 0.96301, value loss 0.21826, policy loss 0.01281\n",
      "Updates 88300, num timesteps 7064080, FPS 2151, mean/median reward 19.6/5.0, min/max reward 0.0/95.0, entropy 0.93685, value loss 0.04362, policy loss 0.01299\n",
      "Updates 88400, num timesteps 7072080, FPS 2151, mean/median reward 5.1/2.0, min/max reward 0.0/28.0, entropy 0.91003, value loss 0.01765, policy loss 0.01844\n",
      "Updates 88500, num timesteps 7080080, FPS 2151, mean/median reward 12.2/3.0, min/max reward 0.0/50.0, entropy 0.94325, value loss 0.08765, policy loss -0.03339\n",
      "Updates 88600, num timesteps 7088080, FPS 2152, mean/median reward 9.1/5.0, min/max reward 0.0/28.0, entropy 1.04188, value loss 0.04462, policy loss 0.06305\n",
      "Updates 88700, num timesteps 7096080, FPS 2152, mean/median reward 18.5/6.0, min/max reward 0.0/87.0, entropy 0.90929, value loss 0.02503, policy loss -0.00201\n",
      "Updates 88800, num timesteps 7104080, FPS 2152, mean/median reward 16.4/3.0, min/max reward 0.0/72.0, entropy 0.97500, value loss 0.03347, policy loss 0.00027\n",
      "Updates 88900, num timesteps 7112080, FPS 2152, mean/median reward 24.1/8.0, min/max reward 1.0/74.0, entropy 1.01989, value loss 0.06010, policy loss -0.01900\n",
      "Updates 89000, num timesteps 7120080, FPS 2152, mean/median reward 12.7/7.0, min/max reward 0.0/45.0, entropy 1.04271, value loss 0.06168, policy loss 0.02298\n",
      "Updates 89100, num timesteps 7128080, FPS 2152, mean/median reward 16.5/12.0, min/max reward 1.0/41.0, entropy 1.07993, value loss 0.03658, policy loss -0.00292\n",
      "Updates 89200, num timesteps 7136080, FPS 2152, mean/median reward 11.6/9.0, min/max reward 1.0/45.0, entropy 1.04185, value loss 0.31169, policy loss 0.12655\n",
      "Updates 89300, num timesteps 7144080, FPS 2153, mean/median reward 14.0/6.0, min/max reward 1.0/70.0, entropy 1.13568, value loss 0.10063, policy loss -0.00649\n",
      "Updates 89400, num timesteps 7152080, FPS 2153, mean/median reward 20.6/9.0, min/max reward 1.0/78.0, entropy 1.02061, value loss 0.13212, policy loss 0.07334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89500, num timesteps 7160080, FPS 2153, mean/median reward 17.7/9.0, min/max reward 1.0/84.0, entropy 0.93885, value loss 0.01937, policy loss -0.00597\n",
      "Updates 89600, num timesteps 7168080, FPS 2153, mean/median reward 12.8/4.0, min/max reward 1.0/50.0, entropy 1.01167, value loss 0.14731, policy loss -0.09707\n",
      "Updates 89700, num timesteps 7176080, FPS 2153, mean/median reward 9.6/6.0, min/max reward 1.0/27.0, entropy 0.99237, value loss 0.39454, policy loss -0.09870\n",
      "Updates 89800, num timesteps 7184080, FPS 2154, mean/median reward 9.4/4.0, min/max reward 0.0/31.0, entropy 0.88887, value loss 0.05484, policy loss -0.00820\n",
      "Updates 89900, num timesteps 7192080, FPS 2154, mean/median reward 9.3/2.0, min/max reward 0.0/39.0, entropy 1.07734, value loss 0.02553, policy loss 0.06026\n",
      "Updates 90000, num timesteps 7200080, FPS 2154, mean/median reward 8.2/2.0, min/max reward 0.0/38.0, entropy 1.12722, value loss 0.04584, policy loss -0.05607\n",
      "Updates 90100, num timesteps 7208080, FPS 2154, mean/median reward 14.8/7.0, min/max reward 0.0/45.0, entropy 0.88689, value loss 0.16746, policy loss 0.07623\n",
      "Updates 90200, num timesteps 7216080, FPS 2154, mean/median reward 18.6/7.0, min/max reward 0.0/92.0, entropy 0.76632, value loss 0.16678, policy loss -0.09655\n",
      "Updates 90300, num timesteps 7224080, FPS 2154, mean/median reward 21.1/8.0, min/max reward 0.0/81.0, entropy 0.90978, value loss 0.02689, policy loss 0.03235\n",
      "Updates 90400, num timesteps 7232080, FPS 2154, mean/median reward 7.7/7.0, min/max reward 0.0/28.0, entropy 1.06727, value loss 0.05910, policy loss 0.00875\n",
      "Updates 90500, num timesteps 7240080, FPS 2154, mean/median reward 6.2/4.0, min/max reward 0.0/21.0, entropy 1.02613, value loss 0.64994, policy loss -0.20593\n",
      "Updates 90600, num timesteps 7248080, FPS 2155, mean/median reward 13.9/4.0, min/max reward 0.0/99.0, entropy 1.06003, value loss 0.01690, policy loss 0.03383\n",
      "Updates 90700, num timesteps 7256080, FPS 2155, mean/median reward 10.4/7.0, min/max reward 0.0/34.0, entropy 0.86039, value loss 0.03993, policy loss -0.04851\n",
      "Updates 90800, num timesteps 7264080, FPS 2155, mean/median reward 17.1/7.0, min/max reward 0.0/65.0, entropy 1.03602, value loss 0.06562, policy loss -0.08345\n",
      "Updates 90900, num timesteps 7272080, FPS 2155, mean/median reward 8.8/2.0, min/max reward 0.0/38.0, entropy 1.06719, value loss 0.02518, policy loss -0.03610\n",
      "Updates 91000, num timesteps 7280080, FPS 2155, mean/median reward 25.5/21.0, min/max reward 2.0/70.0, entropy 1.03106, value loss 0.68495, policy loss -0.20709\n",
      "Updates 91100, num timesteps 7288080, FPS 2155, mean/median reward 10.1/7.0, min/max reward 0.0/31.0, entropy 0.91991, value loss 0.02414, policy loss 0.00578\n",
      "Updates 91200, num timesteps 7296080, FPS 2155, mean/median reward 11.2/4.0, min/max reward 0.0/49.0, entropy 0.88763, value loss 0.06780, policy loss -0.07770\n",
      "Updates 91300, num timesteps 7304080, FPS 2155, mean/median reward 18.5/6.0, min/max reward 0.0/83.0, entropy 0.96918, value loss 0.06639, policy loss -0.04176\n",
      "Updates 91400, num timesteps 7312080, FPS 2156, mean/median reward 14.2/8.0, min/max reward 0.0/75.0, entropy 1.00534, value loss 0.04016, policy loss 0.07372\n",
      "Updates 91500, num timesteps 7320080, FPS 2156, mean/median reward 8.5/3.0, min/max reward 0.0/28.0, entropy 0.98860, value loss 0.04469, policy loss -0.00646\n",
      "Updates 91600, num timesteps 7328080, FPS 2156, mean/median reward 12.7/3.0, min/max reward 0.0/71.0, entropy 1.14784, value loss 0.01949, policy loss 0.02758\n",
      "Updates 91700, num timesteps 7336080, FPS 2156, mean/median reward 13.4/5.0, min/max reward 0.0/71.0, entropy 0.85935, value loss 0.01873, policy loss 0.00136\n",
      "Updates 91800, num timesteps 7344080, FPS 2156, mean/median reward 12.9/5.0, min/max reward 0.0/44.0, entropy 0.93948, value loss 0.04330, policy loss -0.08102\n",
      "Updates 91900, num timesteps 7352080, FPS 2156, mean/median reward 9.7/3.0, min/max reward 0.0/44.0, entropy 1.02043, value loss 0.05807, policy loss -0.03431\n",
      "Updates 92000, num timesteps 7360080, FPS 2156, mean/median reward 11.3/0.0, min/max reward 0.0/86.0, entropy 1.02426, value loss 0.09013, policy loss -0.11365\n",
      "Updates 92100, num timesteps 7368080, FPS 2156, mean/median reward 10.2/4.0, min/max reward 0.0/76.0, entropy 0.83775, value loss 0.21003, policy loss -0.06326\n",
      "Updates 92200, num timesteps 7376080, FPS 2156, mean/median reward 8.2/3.0, min/max reward 0.0/46.0, entropy 0.90809, value loss 0.02958, policy loss 0.00867\n",
      "Updates 92300, num timesteps 7384080, FPS 2156, mean/median reward 16.1/8.0, min/max reward 0.0/57.0, entropy 0.97866, value loss 0.05747, policy loss 0.00092\n",
      "Updates 92400, num timesteps 7392080, FPS 2157, mean/median reward 10.4/4.0, min/max reward 0.0/46.0, entropy 1.01593, value loss 0.10360, policy loss -0.00119\n",
      "Updates 92500, num timesteps 7400080, FPS 2157, mean/median reward 16.3/4.0, min/max reward 0.0/91.0, entropy 0.89135, value loss 0.04036, policy loss 0.02800\n",
      "Updates 92600, num timesteps 7408080, FPS 2157, mean/median reward 16.0/5.0, min/max reward 0.0/79.0, entropy 1.02761, value loss 0.05440, policy loss 0.02942\n",
      "Updates 92700, num timesteps 7416080, FPS 2157, mean/median reward 13.5/4.0, min/max reward 0.0/95.0, entropy 0.89979, value loss 0.02393, policy loss -0.03480\n",
      "Updates 92800, num timesteps 7424080, FPS 2157, mean/median reward 23.6/9.0, min/max reward 0.0/106.0, entropy 0.88763, value loss 0.04425, policy loss 0.02507\n",
      "Updates 92900, num timesteps 7432080, FPS 2157, mean/median reward 26.9/7.0, min/max reward 0.0/106.0, entropy 0.95691, value loss 0.08008, policy loss 0.00437\n",
      "Updates 93000, num timesteps 7440080, FPS 2157, mean/median reward 14.8/7.0, min/max reward 0.0/106.0, entropy 0.89012, value loss 0.31845, policy loss -0.13633\n",
      "Updates 93100, num timesteps 7448080, FPS 2158, mean/median reward 27.1/11.0, min/max reward 3.0/106.0, entropy 1.07568, value loss 0.04515, policy loss 0.06316\n",
      "Updates 93200, num timesteps 7456080, FPS 2158, mean/median reward 15.1/5.0, min/max reward 0.0/58.0, entropy 0.98861, value loss 0.03636, policy loss 0.00448\n",
      "Updates 93300, num timesteps 7464080, FPS 2158, mean/median reward 23.4/15.0, min/max reward 0.0/76.0, entropy 0.85450, value loss 0.05533, policy loss -0.02718\n",
      "Updates 93400, num timesteps 7472080, FPS 2158, mean/median reward 19.9/6.0, min/max reward 0.0/62.0, entropy 0.82906, value loss 0.02254, policy loss 0.03354\n",
      "Updates 93500, num timesteps 7480080, FPS 2158, mean/median reward 12.2/6.0, min/max reward 0.0/35.0, entropy 0.86989, value loss 0.18806, policy loss -0.02867\n",
      "Updates 93600, num timesteps 7488080, FPS 2158, mean/median reward 10.2/6.0, min/max reward 0.0/33.0, entropy 0.91871, value loss 0.08315, policy loss -0.10295\n",
      "Updates 93700, num timesteps 7496080, FPS 2159, mean/median reward 17.2/11.0, min/max reward 0.0/78.0, entropy 0.84731, value loss 0.02089, policy loss 0.00126\n",
      "Updates 93800, num timesteps 7504080, FPS 2159, mean/median reward 16.5/7.0, min/max reward 1.0/86.0, entropy 0.89036, value loss 0.06788, policy loss 0.09181\n",
      "Updates 93900, num timesteps 7512080, FPS 2159, mean/median reward 15.5/5.0, min/max reward 0.0/86.0, entropy 1.04953, value loss 0.03923, policy loss 0.01670\n",
      "Updates 94000, num timesteps 7520080, FPS 2159, mean/median reward 23.1/9.0, min/max reward 1.0/86.0, entropy 1.02398, value loss 0.19416, policy loss 0.07509\n",
      "Updates 94100, num timesteps 7528080, FPS 2159, mean/median reward 28.0/14.0, min/max reward 1.0/86.0, entropy 0.78180, value loss 0.06485, policy loss 0.00267\n",
      "Updates 94200, num timesteps 7536080, FPS 2159, mean/median reward 9.9/6.0, min/max reward 0.0/67.0, entropy 0.94125, value loss 0.03328, policy loss -0.08306\n",
      "Updates 94300, num timesteps 7544080, FPS 2159, mean/median reward 29.9/13.0, min/max reward 0.0/86.0, entropy 1.12424, value loss 0.05521, policy loss 0.08044\n",
      "Updates 94400, num timesteps 7552080, FPS 2160, mean/median reward 13.8/8.0, min/max reward 0.0/62.0, entropy 1.00634, value loss 0.04470, policy loss 0.02311\n",
      "Updates 94500, num timesteps 7560080, FPS 2160, mean/median reward 16.4/13.0, min/max reward 0.0/60.0, entropy 0.98724, value loss 0.02469, policy loss 0.00708\n",
      "Updates 94600, num timesteps 7568080, FPS 2160, mean/median reward 17.0/9.0, min/max reward 1.0/60.0, entropy 0.85247, value loss 0.05389, policy loss 0.06395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 94700, num timesteps 7576080, FPS 2160, mean/median reward 23.4/15.0, min/max reward 0.0/60.0, entropy 1.02873, value loss 0.51163, policy loss -0.23993\n",
      "Updates 94800, num timesteps 7584080, FPS 2160, mean/median reward 12.1/6.0, min/max reward 0.0/60.0, entropy 0.82523, value loss 0.17770, policy loss -0.10018\n",
      "Updates 94900, num timesteps 7592080, FPS 2160, mean/median reward 21.2/9.0, min/max reward 0.0/73.0, entropy 0.93764, value loss 0.07765, policy loss -0.00257\n",
      "Updates 95000, num timesteps 7600080, FPS 2160, mean/median reward 17.2/15.0, min/max reward 1.0/64.0, entropy 1.06789, value loss 0.03623, policy loss 0.00706\n",
      "Updates 95100, num timesteps 7608080, FPS 2160, mean/median reward 3.7/0.0, min/max reward 0.0/24.0, entropy 1.18036, value loss 0.04843, policy loss -0.06202\n",
      "Updates 95200, num timesteps 7616080, FPS 2160, mean/median reward 13.9/4.0, min/max reward 0.0/95.0, entropy 1.00080, value loss 0.06347, policy loss 0.02365\n",
      "Updates 95300, num timesteps 7624080, FPS 2160, mean/median reward 11.8/6.0, min/max reward 1.0/52.0, entropy 1.06367, value loss 0.04106, policy loss 0.03962\n",
      "Updates 95400, num timesteps 7632080, FPS 2160, mean/median reward 29.6/30.0, min/max reward 1.0/82.0, entropy 1.14376, value loss 0.02377, policy loss 0.02622\n",
      "Updates 95500, num timesteps 7640080, FPS 2160, mean/median reward 32.9/30.0, min/max reward 2.0/82.0, entropy 0.82974, value loss 0.42611, policy loss -0.09352\n",
      "Updates 95600, num timesteps 7648080, FPS 2160, mean/median reward 22.9/17.0, min/max reward 0.0/63.0, entropy 0.90243, value loss 0.13482, policy loss 0.00954\n",
      "Updates 95700, num timesteps 7656080, FPS 2160, mean/median reward 21.1/9.0, min/max reward 0.0/91.0, entropy 0.85124, value loss 0.02808, policy loss -0.01651\n",
      "Updates 95800, num timesteps 7664080, FPS 2161, mean/median reward 11.3/8.0, min/max reward 0.0/39.0, entropy 0.96611, value loss 0.02554, policy loss 0.01178\n",
      "Updates 95900, num timesteps 7672080, FPS 2161, mean/median reward 8.3/5.0, min/max reward 0.0/39.0, entropy 1.02810, value loss 0.77582, policy loss -0.16853\n",
      "Updates 96000, num timesteps 7680080, FPS 2161, mean/median reward 20.8/8.0, min/max reward 0.0/91.0, entropy 0.93666, value loss 0.06217, policy loss 0.06278\n",
      "Updates 96100, num timesteps 7688080, FPS 2161, mean/median reward 13.6/5.0, min/max reward 0.0/88.0, entropy 1.06114, value loss 0.04434, policy loss 0.03996\n",
      "Updates 96200, num timesteps 7696080, FPS 2161, mean/median reward 10.6/5.0, min/max reward 0.0/40.0, entropy 0.94733, value loss 0.08764, policy loss -0.05900\n",
      "Updates 96300, num timesteps 7704080, FPS 2161, mean/median reward 17.8/7.0, min/max reward 0.0/85.0, entropy 0.95952, value loss 0.03220, policy loss -0.03879\n",
      "Updates 96400, num timesteps 7712080, FPS 2161, mean/median reward 30.1/14.0, min/max reward 0.0/85.0, entropy 0.93468, value loss 0.03186, policy loss -0.01880\n",
      "Updates 96500, num timesteps 7720080, FPS 2161, mean/median reward 19.8/5.0, min/max reward 0.0/85.0, entropy 0.98212, value loss 0.06078, policy loss -0.00032\n",
      "Updates 96600, num timesteps 7728080, FPS 2161, mean/median reward 20.9/5.0, min/max reward 0.0/72.0, entropy 1.03213, value loss 0.01448, policy loss 0.00843\n",
      "Updates 96700, num timesteps 7736080, FPS 2162, mean/median reward 16.4/13.0, min/max reward 0.0/42.0, entropy 0.96170, value loss 0.03053, policy loss 0.03262\n",
      "Updates 96800, num timesteps 7744080, FPS 2162, mean/median reward 16.6/13.0, min/max reward 0.0/44.0, entropy 0.89925, value loss 0.10302, policy loss -0.03121\n",
      "Updates 96900, num timesteps 7752080, FPS 2162, mean/median reward 14.2/11.0, min/max reward 0.0/42.0, entropy 1.07380, value loss 0.05309, policy loss -0.05431\n",
      "Updates 97000, num timesteps 7760080, FPS 2162, mean/median reward 9.8/5.0, min/max reward 0.0/31.0, entropy 0.99294, value loss 0.05957, policy loss -0.03487\n",
      "Updates 97100, num timesteps 7768080, FPS 2162, mean/median reward 25.6/8.0, min/max reward 0.0/79.0, entropy 0.96806, value loss 0.50144, policy loss -0.06280\n",
      "Updates 97200, num timesteps 7776080, FPS 2162, mean/median reward 17.4/9.0, min/max reward 0.0/76.0, entropy 0.88739, value loss 0.01430, policy loss 0.03215\n",
      "Updates 97300, num timesteps 7784080, FPS 2162, mean/median reward 28.6/28.0, min/max reward 0.0/83.0, entropy 0.93795, value loss 0.03187, policy loss -0.04702\n",
      "Updates 97400, num timesteps 7792080, FPS 2162, mean/median reward 23.5/9.0, min/max reward 0.0/83.0, entropy 0.99002, value loss 0.02932, policy loss 0.03279\n",
      "Updates 97500, num timesteps 7800080, FPS 2163, mean/median reward 14.5/10.0, min/max reward 0.0/43.0, entropy 1.07916, value loss 0.06684, policy loss 0.00039\n",
      "Updates 97600, num timesteps 7808080, FPS 2163, mean/median reward 13.0/4.0, min/max reward 0.0/59.0, entropy 1.00834, value loss 0.02840, policy loss -0.00429\n",
      "Updates 97700, num timesteps 7816080, FPS 2163, mean/median reward 13.7/5.0, min/max reward 0.0/57.0, entropy 1.02214, value loss 0.04331, policy loss 0.01471\n",
      "Updates 97800, num timesteps 7824080, FPS 2163, mean/median reward 21.2/18.0, min/max reward 0.0/57.0, entropy 0.98590, value loss 0.02573, policy loss 0.03075\n",
      "Updates 97900, num timesteps 7832080, FPS 2163, mean/median reward 11.9/5.0, min/max reward 0.0/34.0, entropy 0.93668, value loss 0.05607, policy loss 0.02857\n",
      "Updates 98000, num timesteps 7840080, FPS 2163, mean/median reward 18.4/6.0, min/max reward 0.0/72.0, entropy 0.96689, value loss 0.11351, policy loss 0.03721\n",
      "Updates 98100, num timesteps 7848080, FPS 2163, mean/median reward 7.7/4.0, min/max reward 0.0/30.0, entropy 1.07865, value loss 0.07468, policy loss -0.01410\n",
      "Updates 98200, num timesteps 7856080, FPS 2163, mean/median reward 15.6/7.0, min/max reward 0.0/84.0, entropy 0.87225, value loss 0.03741, policy loss -0.05174\n",
      "Updates 98300, num timesteps 7864080, FPS 2163, mean/median reward 24.8/14.0, min/max reward 0.0/84.0, entropy 0.82646, value loss 0.04281, policy loss 0.02466\n",
      "Updates 98400, num timesteps 7872080, FPS 2164, mean/median reward 34.9/20.0, min/max reward 0.0/98.0, entropy 0.82049, value loss 0.02593, policy loss -0.02097\n",
      "Updates 98500, num timesteps 7880080, FPS 2164, mean/median reward 16.5/8.0, min/max reward 0.0/81.0, entropy 0.89599, value loss 0.02679, policy loss -0.06538\n",
      "Updates 98600, num timesteps 7888080, FPS 2164, mean/median reward 17.6/4.0, min/max reward 0.0/81.0, entropy 0.98596, value loss 0.02681, policy loss -0.02080\n",
      "Updates 98700, num timesteps 7896080, FPS 2164, mean/median reward 21.6/12.0, min/max reward 0.0/76.0, entropy 0.80831, value loss 0.12415, policy loss 0.02675\n",
      "Updates 98800, num timesteps 7904080, FPS 2164, mean/median reward 17.3/13.0, min/max reward 0.0/54.0, entropy 0.83110, value loss 0.13402, policy loss 0.00135\n",
      "Updates 98900, num timesteps 7912080, FPS 2164, mean/median reward 21.9/8.0, min/max reward 0.0/107.0, entropy 0.98227, value loss 0.08307, policy loss 0.07382\n",
      "Updates 99000, num timesteps 7920080, FPS 2164, mean/median reward 10.7/6.0, min/max reward 0.0/59.0, entropy 0.94600, value loss 0.03396, policy loss -0.01406\n",
      "Updates 99100, num timesteps 7928080, FPS 2165, mean/median reward 12.1/8.0, min/max reward 0.0/37.0, entropy 1.05344, value loss 0.04841, policy loss 0.01100\n",
      "Updates 99200, num timesteps 7936080, FPS 2165, mean/median reward 22.7/8.0, min/max reward 0.0/86.0, entropy 0.78945, value loss 0.10274, policy loss 0.08822\n",
      "Updates 99300, num timesteps 7944080, FPS 2165, mean/median reward 21.5/8.0, min/max reward 1.0/86.0, entropy 0.90686, value loss 0.04826, policy loss -0.06619\n",
      "Updates 99400, num timesteps 7952080, FPS 2165, mean/median reward 23.6/8.0, min/max reward 2.0/62.0, entropy 1.03520, value loss 0.03884, policy loss -0.09049\n",
      "Updates 99500, num timesteps 7960080, FPS 2166, mean/median reward 14.8/7.0, min/max reward 0.0/56.0, entropy 0.98015, value loss 0.07070, policy loss 0.00321\n",
      "Updates 99600, num timesteps 7968080, FPS 2166, mean/median reward 18.8/6.0, min/max reward 0.0/65.0, entropy 0.87498, value loss 0.39880, policy loss 0.01894\n",
      "Updates 99700, num timesteps 7976080, FPS 2166, mean/median reward 21.4/7.0, min/max reward 0.0/63.0, entropy 0.83970, value loss 0.07166, policy loss 0.00541\n",
      "Updates 99800, num timesteps 7984080, FPS 2166, mean/median reward 14.8/5.0, min/max reward 1.0/63.0, entropy 0.83600, value loss 0.07853, policy loss 0.06792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99900, num timesteps 7992080, FPS 2166, mean/median reward 12.8/5.0, min/max reward 1.0/63.0, entropy 0.79771, value loss 0.11179, policy loss -0.07604\n",
      "Updates 100000, num timesteps 8000080, FPS 2166, mean/median reward 22.2/13.0, min/max reward 0.0/79.0, entropy 1.11580, value loss 0.03935, policy loss -0.01893\n",
      "Updates 100100, num timesteps 8008080, FPS 2166, mean/median reward 18.4/6.0, min/max reward 0.0/79.0, entropy 0.97517, value loss 0.05914, policy loss 0.00548\n",
      "Updates 100200, num timesteps 8016080, FPS 2167, mean/median reward 12.6/9.0, min/max reward 0.0/71.0, entropy 0.92168, value loss 0.09048, policy loss -0.01969\n",
      "Updates 100300, num timesteps 8024080, FPS 2167, mean/median reward 12.3/4.0, min/max reward 0.0/78.0, entropy 0.86774, value loss 0.02876, policy loss 0.03019\n",
      "Updates 100400, num timesteps 8032080, FPS 2167, mean/median reward 19.3/4.0, min/max reward 0.0/85.0, entropy 0.77724, value loss 0.04856, policy loss -0.08574\n",
      "Updates 100500, num timesteps 8040080, FPS 2167, mean/median reward 20.6/10.0, min/max reward 0.0/82.0, entropy 0.90613, value loss 0.10759, policy loss 0.06370\n",
      "Updates 100600, num timesteps 8048080, FPS 2167, mean/median reward 15.1/7.0, min/max reward 0.0/82.0, entropy 1.03742, value loss 0.02800, policy loss 0.02140\n",
      "Updates 100700, num timesteps 8056080, FPS 2167, mean/median reward 16.6/7.0, min/max reward 0.0/82.0, entropy 0.89022, value loss 0.04053, policy loss 0.00310\n",
      "Updates 100800, num timesteps 8064080, FPS 2167, mean/median reward 6.7/5.0, min/max reward 0.0/29.0, entropy 1.05612, value loss 0.01119, policy loss -0.01716\n",
      "Updates 100900, num timesteps 8072080, FPS 2167, mean/median reward 21.8/4.0, min/max reward 0.0/72.0, entropy 0.88199, value loss 0.40982, policy loss -0.00579\n",
      "Updates 101000, num timesteps 8080080, FPS 2167, mean/median reward 21.2/13.0, min/max reward 0.0/80.0, entropy 0.82026, value loss 0.02309, policy loss -0.01505\n",
      "Updates 101100, num timesteps 8088080, FPS 2167, mean/median reward 20.5/14.0, min/max reward 0.0/95.0, entropy 0.93099, value loss 0.05959, policy loss 0.00698\n",
      "Updates 101200, num timesteps 8096080, FPS 2168, mean/median reward 10.2/6.0, min/max reward 0.0/35.0, entropy 0.91027, value loss 0.01239, policy loss 0.00575\n",
      "Updates 101300, num timesteps 8104080, FPS 2167, mean/median reward 11.2/4.0, min/max reward 0.0/56.0, entropy 0.94946, value loss 0.02211, policy loss 0.02587\n",
      "Updates 101400, num timesteps 8112080, FPS 2167, mean/median reward 11.5/9.0, min/max reward 0.0/35.0, entropy 0.93210, value loss 0.08776, policy loss 0.02934\n",
      "Updates 101500, num timesteps 8120080, FPS 2168, mean/median reward 21.9/10.0, min/max reward 0.0/67.0, entropy 0.97195, value loss 0.07327, policy loss 0.00630\n",
      "Updates 101600, num timesteps 8128080, FPS 2168, mean/median reward 17.6/9.0, min/max reward 0.0/53.0, entropy 0.77192, value loss 0.04364, policy loss -0.00661\n",
      "Updates 101700, num timesteps 8136080, FPS 2168, mean/median reward 22.2/8.0, min/max reward 0.0/87.0, entropy 0.98835, value loss 0.02882, policy loss -0.00176\n",
      "Updates 101800, num timesteps 8144080, FPS 2168, mean/median reward 20.6/10.0, min/max reward 0.0/73.0, entropy 0.98520, value loss 0.02890, policy loss -0.02921\n",
      "Updates 101900, num timesteps 8152080, FPS 2168, mean/median reward 22.4/15.0, min/max reward 1.0/73.0, entropy 1.17307, value loss 0.05399, policy loss 0.04292\n",
      "Updates 102000, num timesteps 8160080, FPS 2168, mean/median reward 17.8/14.0, min/max reward 4.0/73.0, entropy 1.04777, value loss 0.03150, policy loss -0.00245\n",
      "Updates 102100, num timesteps 8168080, FPS 2169, mean/median reward 12.6/5.0, min/max reward 0.0/61.0, entropy 1.01178, value loss 0.07265, policy loss -0.05813\n",
      "Updates 102200, num timesteps 8176080, FPS 2169, mean/median reward 18.7/13.0, min/max reward 0.0/60.0, entropy 0.90451, value loss 0.04805, policy loss 0.03365\n",
      "Updates 102300, num timesteps 8184080, FPS 2169, mean/median reward 15.9/6.0, min/max reward 0.0/68.0, entropy 0.86957, value loss 0.10169, policy loss 0.09050\n",
      "Updates 102400, num timesteps 8192080, FPS 2169, mean/median reward 13.5/5.0, min/max reward 1.0/66.0, entropy 0.96066, value loss 0.01930, policy loss -0.05495\n",
      "Updates 102500, num timesteps 8200080, FPS 2169, mean/median reward 11.7/6.0, min/max reward 1.0/40.0, entropy 0.88466, value loss 0.17914, policy loss -0.01888\n",
      "Updates 102600, num timesteps 8208080, FPS 2169, mean/median reward 12.7/6.0, min/max reward 0.0/55.0, entropy 1.01931, value loss 0.12540, policy loss 0.15376\n",
      "Updates 102700, num timesteps 8216080, FPS 2169, mean/median reward 25.3/12.0, min/max reward 0.0/102.0, entropy 0.92164, value loss 0.63636, policy loss -0.00876\n",
      "Updates 102800, num timesteps 8224080, FPS 2170, mean/median reward 11.5/6.0, min/max reward 0.0/40.0, entropy 0.95549, value loss 0.06643, policy loss -0.00908\n",
      "Updates 102900, num timesteps 8232080, FPS 2170, mean/median reward 8.1/8.0, min/max reward 0.0/28.0, entropy 1.03360, value loss 1.12781, policy loss -0.01234\n",
      "Updates 103000, num timesteps 8240080, FPS 2170, mean/median reward 22.2/8.0, min/max reward 1.0/78.0, entropy 0.90029, value loss 0.06189, policy loss 0.03721\n",
      "Updates 103100, num timesteps 8248080, FPS 2170, mean/median reward 14.6/6.0, min/max reward 0.0/78.0, entropy 0.87868, value loss 0.01395, policy loss -0.00909\n",
      "Updates 103200, num timesteps 8256080, FPS 2170, mean/median reward 17.7/11.0, min/max reward 1.0/58.0, entropy 1.12068, value loss 0.12674, policy loss 0.01802\n",
      "Updates 103300, num timesteps 8264080, FPS 2170, mean/median reward 25.2/11.0, min/max reward 0.0/91.0, entropy 0.87017, value loss 1.53927, policy loss 0.39813\n",
      "Updates 103400, num timesteps 8272080, FPS 2170, mean/median reward 12.1/7.0, min/max reward 0.0/35.0, entropy 0.86116, value loss 0.42446, policy loss -0.03453\n",
      "Updates 103500, num timesteps 8280080, FPS 2170, mean/median reward 15.4/6.0, min/max reward 0.0/92.0, entropy 0.99099, value loss 0.47356, policy loss -0.05296\n",
      "Updates 103600, num timesteps 8288080, FPS 2170, mean/median reward 20.5/9.0, min/max reward 0.0/92.0, entropy 0.91862, value loss 0.82994, policy loss 0.03058\n",
      "Updates 103700, num timesteps 8296080, FPS 2170, mean/median reward 16.0/6.0, min/max reward 0.0/92.0, entropy 0.92798, value loss 0.34678, policy loss 0.07222\n",
      "Updates 103800, num timesteps 8304080, FPS 2171, mean/median reward 21.5/9.0, min/max reward 0.0/92.0, entropy 0.94738, value loss 0.01359, policy loss 0.01258\n",
      "Updates 103900, num timesteps 8312080, FPS 2171, mean/median reward 17.4/6.0, min/max reward 0.0/92.0, entropy 1.09022, value loss 0.04485, policy loss 0.04098\n",
      "Updates 104000, num timesteps 8320080, FPS 2171, mean/median reward 17.8/3.0, min/max reward 0.0/92.0, entropy 1.01409, value loss 0.02613, policy loss 0.04466\n",
      "Updates 104100, num timesteps 8328080, FPS 2171, mean/median reward 20.1/6.0, min/max reward 0.0/64.0, entropy 0.85122, value loss 0.08583, policy loss 0.00474\n",
      "Updates 104200, num timesteps 8336080, FPS 2171, mean/median reward 8.8/4.0, min/max reward 0.0/45.0, entropy 0.78365, value loss 0.03133, policy loss 0.00237\n",
      "Updates 104300, num timesteps 8344080, FPS 2171, mean/median reward 16.6/6.0, min/max reward 0.0/80.0, entropy 1.06115, value loss 0.11894, policy loss -0.06969\n",
      "Updates 104400, num timesteps 8352080, FPS 2171, mean/median reward 12.0/7.0, min/max reward 0.0/50.0, entropy 1.08951, value loss 0.01758, policy loss -0.03467\n",
      "Updates 104500, num timesteps 8360080, FPS 2171, mean/median reward 16.2/7.0, min/max reward 0.0/82.0, entropy 1.05547, value loss 0.10111, policy loss 0.10077\n",
      "Updates 104600, num timesteps 8368080, FPS 2171, mean/median reward 15.3/7.0, min/max reward 1.0/53.0, entropy 1.03445, value loss 0.59467, policy loss -0.28989\n",
      "Updates 104700, num timesteps 8376080, FPS 2171, mean/median reward 16.0/3.0, min/max reward 0.0/99.0, entropy 1.03603, value loss 0.04821, policy loss 0.09357\n",
      "Updates 104800, num timesteps 8384080, FPS 2171, mean/median reward 15.6/6.0, min/max reward 0.0/69.0, entropy 1.00951, value loss 0.08479, policy loss 0.04473\n",
      "Updates 104900, num timesteps 8392080, FPS 2171, mean/median reward 22.9/8.0, min/max reward 0.0/82.0, entropy 0.78187, value loss 0.07019, policy loss 0.01976\n",
      "Updates 105000, num timesteps 8400080, FPS 2172, mean/median reward 15.9/6.0, min/max reward 0.0/82.0, entropy 0.94775, value loss 0.34711, policy loss 0.04346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 105100, num timesteps 8408080, FPS 2172, mean/median reward 18.8/5.0, min/max reward 1.0/105.0, entropy 1.17295, value loss 0.01666, policy loss -0.01576\n",
      "Updates 105200, num timesteps 8416080, FPS 2172, mean/median reward 18.0/5.0, min/max reward 0.0/83.0, entropy 1.05225, value loss 0.01119, policy loss -0.01656\n",
      "Updates 105300, num timesteps 8424080, FPS 2172, mean/median reward 12.9/5.0, min/max reward 0.0/83.0, entropy 0.72540, value loss 0.06074, policy loss 0.00203\n",
      "Updates 105400, num timesteps 8432080, FPS 2172, mean/median reward 20.7/7.0, min/max reward 1.0/90.0, entropy 0.96005, value loss 0.13289, policy loss 0.03571\n",
      "Updates 105500, num timesteps 8440080, FPS 2172, mean/median reward 14.6/6.0, min/max reward 1.0/71.0, entropy 0.84598, value loss 0.05629, policy loss 0.00811\n",
      "Updates 105600, num timesteps 8448080, FPS 2172, mean/median reward 15.3/8.0, min/max reward 2.0/72.0, entropy 1.04811, value loss 0.30583, policy loss -0.27226\n",
      "Updates 105700, num timesteps 8456080, FPS 2172, mean/median reward 12.2/10.0, min/max reward 2.0/39.0, entropy 0.93704, value loss 0.06509, policy loss 0.00106\n",
      "Updates 105800, num timesteps 8464080, FPS 2173, mean/median reward 18.7/10.0, min/max reward 2.0/83.0, entropy 0.92263, value loss 0.12792, policy loss 0.08764\n",
      "Updates 105900, num timesteps 8472080, FPS 2173, mean/median reward 20.2/9.0, min/max reward 1.0/83.0, entropy 0.89696, value loss 0.13858, policy loss -0.06742\n",
      "Updates 106000, num timesteps 8480080, FPS 2173, mean/median reward 26.0/10.0, min/max reward 0.0/96.0, entropy 0.83760, value loss 0.16953, policy loss 0.10106\n",
      "Updates 106100, num timesteps 8488080, FPS 2173, mean/median reward 26.8/14.0, min/max reward 0.0/89.0, entropy 0.98196, value loss 0.08100, policy loss -0.10158\n",
      "Updates 106200, num timesteps 8496080, FPS 2173, mean/median reward 10.4/6.0, min/max reward 0.0/39.0, entropy 1.07418, value loss 0.02205, policy loss 0.01901\n",
      "Updates 106300, num timesteps 8504080, FPS 2173, mean/median reward 10.8/7.0, min/max reward 0.0/39.0, entropy 0.98172, value loss 0.09414, policy loss 0.11332\n",
      "Updates 106400, num timesteps 8512080, FPS 2173, mean/median reward 17.9/8.0, min/max reward 2.0/68.0, entropy 0.96118, value loss 0.06613, policy loss 0.07627\n",
      "Updates 106500, num timesteps 8520080, FPS 2173, mean/median reward 20.4/10.0, min/max reward 2.0/104.0, entropy 0.88940, value loss 0.19274, policy loss 0.00164\n",
      "Updates 106600, num timesteps 8528080, FPS 2173, mean/median reward 20.6/11.0, min/max reward 0.0/82.0, entropy 1.00645, value loss 0.12962, policy loss 0.05918\n",
      "Updates 106700, num timesteps 8536080, FPS 2173, mean/median reward 17.6/8.0, min/max reward 0.0/92.0, entropy 1.01093, value loss 0.04179, policy loss -0.01037\n",
      "Updates 106800, num timesteps 8544080, FPS 2173, mean/median reward 23.5/8.0, min/max reward 0.0/92.0, entropy 0.98735, value loss 0.03897, policy loss 0.00264\n",
      "Updates 106900, num timesteps 8552080, FPS 2173, mean/median reward 22.3/12.0, min/max reward 0.0/92.0, entropy 0.83189, value loss 0.03780, policy loss 0.00406\n",
      "Updates 107000, num timesteps 8560080, FPS 2174, mean/median reward 12.7/5.0, min/max reward 0.0/92.0, entropy 1.13603, value loss 0.08519, policy loss -0.05451\n",
      "Updates 107100, num timesteps 8568080, FPS 2174, mean/median reward 8.4/5.0, min/max reward 0.0/35.0, entropy 1.06301, value loss 0.05373, policy loss 0.01369\n",
      "Updates 107200, num timesteps 8576080, FPS 2174, mean/median reward 8.1/3.0, min/max reward 0.0/21.0, entropy 1.08873, value loss 0.04367, policy loss -0.01406\n",
      "Updates 107300, num timesteps 8584080, FPS 2174, mean/median reward 17.6/7.0, min/max reward 1.0/80.0, entropy 0.99160, value loss 0.12501, policy loss 0.01037\n",
      "Updates 107400, num timesteps 8592080, FPS 2174, mean/median reward 10.8/5.0, min/max reward 0.0/50.0, entropy 1.06271, value loss 0.04285, policy loss 0.00960\n",
      "Updates 107500, num timesteps 8600080, FPS 2174, mean/median reward 18.1/7.0, min/max reward 0.0/67.0, entropy 0.90705, value loss 0.01540, policy loss -0.01529\n",
      "Updates 107600, num timesteps 8608080, FPS 2174, mean/median reward 26.9/12.0, min/max reward 0.0/81.0, entropy 1.06563, value loss 0.11530, policy loss 0.04544\n",
      "Updates 107700, num timesteps 8616080, FPS 2174, mean/median reward 11.7/3.0, min/max reward 0.0/81.0, entropy 1.14425, value loss 0.13374, policy loss 0.00526\n",
      "Updates 107800, num timesteps 8624080, FPS 2175, mean/median reward 16.4/3.0, min/max reward 0.0/81.0, entropy 1.11462, value loss 0.01070, policy loss -0.01587\n",
      "Updates 107900, num timesteps 8632080, FPS 2175, mean/median reward 16.6/11.0, min/max reward 0.0/91.0, entropy 0.92949, value loss 0.10414, policy loss -0.01249\n",
      "Updates 108000, num timesteps 8640080, FPS 2175, mean/median reward 18.0/9.0, min/max reward 0.0/86.0, entropy 0.99885, value loss 0.11073, policy loss -0.09021\n",
      "Updates 108100, num timesteps 8648080, FPS 2175, mean/median reward 12.8/6.0, min/max reward 0.0/53.0, entropy 0.90735, value loss 0.12057, policy loss 0.04150\n",
      "Updates 108200, num timesteps 8656080, FPS 2175, mean/median reward 12.4/3.0, min/max reward 0.0/63.0, entropy 1.06203, value loss 0.13546, policy loss -0.08540\n",
      "Updates 108300, num timesteps 8664080, FPS 2175, mean/median reward 15.2/9.0, min/max reward 0.0/77.0, entropy 1.07552, value loss 0.01911, policy loss -0.03196\n",
      "Updates 108400, num timesteps 8672080, FPS 2175, mean/median reward 11.2/4.0, min/max reward 0.0/55.0, entropy 0.91311, value loss 0.05321, policy loss 0.01475\n",
      "Updates 108500, num timesteps 8680080, FPS 2175, mean/median reward 16.0/5.0, min/max reward 0.0/62.0, entropy 1.00836, value loss 0.13111, policy loss -0.03793\n",
      "Updates 108600, num timesteps 8688080, FPS 2175, mean/median reward 16.5/6.0, min/max reward 0.0/71.0, entropy 1.03822, value loss 0.06288, policy loss -0.01190\n",
      "Updates 108700, num timesteps 8696080, FPS 2175, mean/median reward 10.8/5.0, min/max reward 0.0/71.0, entropy 1.01453, value loss 0.01406, policy loss 0.02582\n",
      "Updates 108800, num timesteps 8704080, FPS 2175, mean/median reward 24.3/9.0, min/max reward 0.0/89.0, entropy 0.84120, value loss 0.12685, policy loss 0.02987\n",
      "Updates 108900, num timesteps 8712080, FPS 2175, mean/median reward 14.8/7.0, min/max reward 0.0/71.0, entropy 0.89572, value loss 0.33481, policy loss 0.06621\n",
      "Updates 109000, num timesteps 8720080, FPS 2175, mean/median reward 14.7/4.0, min/max reward 0.0/66.0, entropy 0.98343, value loss 1.21294, policy loss -0.02958\n",
      "Updates 109100, num timesteps 8728080, FPS 2175, mean/median reward 12.8/5.0, min/max reward 0.0/83.0, entropy 0.95479, value loss 0.14487, policy loss 0.01018\n",
      "Updates 109200, num timesteps 8736080, FPS 2176, mean/median reward 14.7/5.0, min/max reward 0.0/82.0, entropy 0.89313, value loss 0.09776, policy loss -0.08696\n",
      "Updates 109300, num timesteps 8744080, FPS 2176, mean/median reward 14.5/5.0, min/max reward 0.0/57.0, entropy 0.94455, value loss 0.03395, policy loss -0.04315\n",
      "Updates 109400, num timesteps 8752080, FPS 2176, mean/median reward 14.9/8.0, min/max reward 0.0/57.0, entropy 1.00824, value loss 0.13311, policy loss -0.02448\n",
      "Updates 109500, num timesteps 8760080, FPS 2176, mean/median reward 14.4/6.0, min/max reward 0.0/64.0, entropy 0.99138, value loss 0.10058, policy loss 0.14236\n",
      "Updates 109600, num timesteps 8768080, FPS 2176, mean/median reward 14.8/10.0, min/max reward 0.0/38.0, entropy 0.99865, value loss 0.06687, policy loss 0.04012\n",
      "Updates 109700, num timesteps 8776080, FPS 2176, mean/median reward 22.8/7.0, min/max reward 0.0/77.0, entropy 0.86132, value loss 0.01435, policy loss 0.02681\n",
      "Updates 109800, num timesteps 8784080, FPS 2176, mean/median reward 29.7/10.0, min/max reward 0.0/77.0, entropy 0.78125, value loss 0.29755, policy loss 0.10829\n",
      "Updates 109900, num timesteps 8792080, FPS 2176, mean/median reward 25.5/6.0, min/max reward 0.0/83.0, entropy 0.96403, value loss 0.03546, policy loss 0.00640\n",
      "Updates 110000, num timesteps 8800080, FPS 2176, mean/median reward 21.7/7.0, min/max reward 0.0/87.0, entropy 0.93743, value loss 1.19319, policy loss 0.36630\n",
      "Updates 110100, num timesteps 8808080, FPS 2176, mean/median reward 7.7/2.0, min/max reward 0.0/51.0, entropy 1.06414, value loss 0.04728, policy loss -0.05039\n",
      "Updates 110200, num timesteps 8816080, FPS 2176, mean/median reward 18.8/6.0, min/max reward 0.0/76.0, entropy 0.83430, value loss 0.17360, policy loss -0.05505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 110300, num timesteps 8824080, FPS 2177, mean/median reward 17.3/11.0, min/max reward 0.0/76.0, entropy 0.97768, value loss 0.03476, policy loss -0.10614\n",
      "Updates 110400, num timesteps 8832080, FPS 2176, mean/median reward 11.2/11.0, min/max reward 0.0/45.0, entropy 0.89012, value loss 0.24022, policy loss -0.00213\n",
      "Updates 110500, num timesteps 8840080, FPS 2176, mean/median reward 14.7/5.0, min/max reward 0.0/81.0, entropy 0.98793, value loss 0.04636, policy loss 0.05622\n",
      "Updates 110600, num timesteps 8848080, FPS 2177, mean/median reward 20.6/5.0, min/max reward 0.0/103.0, entropy 0.85732, value loss 0.05464, policy loss 0.02509\n",
      "Updates 110700, num timesteps 8856080, FPS 2177, mean/median reward 19.2/4.0, min/max reward 0.0/103.0, entropy 0.91394, value loss 0.14479, policy loss 0.08934\n",
      "Updates 110800, num timesteps 8864080, FPS 2177, mean/median reward 10.1/3.0, min/max reward 0.0/56.0, entropy 0.88932, value loss 0.01947, policy loss -0.03315\n",
      "Updates 110900, num timesteps 8872080, FPS 2177, mean/median reward 12.1/3.0, min/max reward 1.0/59.0, entropy 0.98546, value loss 0.11114, policy loss 0.01467\n",
      "Updates 111000, num timesteps 8880080, FPS 2177, mean/median reward 15.7/4.0, min/max reward 1.0/86.0, entropy 1.11856, value loss 0.03985, policy loss 0.04292\n",
      "Updates 111100, num timesteps 8888080, FPS 2177, mean/median reward 11.1/7.0, min/max reward 0.0/34.0, entropy 1.02863, value loss 0.04158, policy loss -0.03263\n",
      "Updates 111200, num timesteps 8896080, FPS 2177, mean/median reward 12.5/7.0, min/max reward 0.0/34.0, entropy 0.90213, value loss 0.01424, policy loss -0.00385\n",
      "Updates 111300, num timesteps 8904080, FPS 2177, mean/median reward 18.2/14.0, min/max reward 0.0/56.0, entropy 0.91473, value loss 0.02949, policy loss -0.00285\n",
      "Updates 111400, num timesteps 8912080, FPS 2177, mean/median reward 23.8/14.0, min/max reward 0.0/62.0, entropy 0.88473, value loss 0.03943, policy loss 0.05363\n",
      "Updates 111500, num timesteps 8920080, FPS 2177, mean/median reward 14.1/5.0, min/max reward 0.0/58.0, entropy 1.01023, value loss 0.04782, policy loss -0.03681\n",
      "Updates 111600, num timesteps 8928080, FPS 2177, mean/median reward 11.9/4.0, min/max reward 0.0/58.0, entropy 0.97191, value loss 0.02464, policy loss -0.02482\n",
      "Updates 111700, num timesteps 8936080, FPS 2177, mean/median reward 17.0/13.0, min/max reward 0.0/58.0, entropy 0.87585, value loss 0.05053, policy loss 0.02047\n",
      "Updates 111800, num timesteps 8944080, FPS 2178, mean/median reward 23.2/18.0, min/max reward 0.0/77.0, entropy 0.81449, value loss 0.11156, policy loss -0.02347\n",
      "Updates 111900, num timesteps 8952080, FPS 2178, mean/median reward 26.2/9.0, min/max reward 0.0/93.0, entropy 0.94442, value loss 0.16178, policy loss 0.01819\n",
      "Updates 112000, num timesteps 8960080, FPS 2178, mean/median reward 28.8/5.0, min/max reward 0.0/104.0, entropy 1.00938, value loss 0.07460, policy loss 0.06765\n",
      "Updates 112100, num timesteps 8968080, FPS 2178, mean/median reward 16.9/4.0, min/max reward 0.0/93.0, entropy 0.96191, value loss 0.04153, policy loss -0.04179\n",
      "Updates 112200, num timesteps 8976080, FPS 2178, mean/median reward 13.9/4.0, min/max reward 0.0/86.0, entropy 0.94338, value loss 0.15733, policy loss -0.13051\n",
      "Updates 112300, num timesteps 8984080, FPS 2178, mean/median reward 14.7/6.0, min/max reward 0.0/52.0, entropy 0.99519, value loss 0.07081, policy loss 0.07447\n",
      "Updates 112400, num timesteps 8992080, FPS 2178, mean/median reward 24.7/15.0, min/max reward 0.0/88.0, entropy 0.89426, value loss 0.01600, policy loss -0.02753\n",
      "Updates 112500, num timesteps 9000080, FPS 2178, mean/median reward 25.9/19.0, min/max reward 0.0/71.0, entropy 0.97533, value loss 0.07697, policy loss 0.00461\n",
      "Updates 112600, num timesteps 9008080, FPS 2178, mean/median reward 15.8/4.0, min/max reward 0.0/72.0, entropy 1.01226, value loss 0.05837, policy loss 0.03092\n",
      "Updates 112700, num timesteps 9016080, FPS 2178, mean/median reward 20.2/11.0, min/max reward 1.0/73.0, entropy 0.82044, value loss 0.04304, policy loss 0.03279\n",
      "Updates 112800, num timesteps 9024080, FPS 2178, mean/median reward 17.4/11.0, min/max reward 0.0/61.0, entropy 1.04110, value loss 0.05273, policy loss -0.01338\n",
      "Updates 112900, num timesteps 9032080, FPS 2178, mean/median reward 13.9/7.0, min/max reward 0.0/63.0, entropy 0.96472, value loss 0.04432, policy loss -0.04454\n",
      "Updates 113000, num timesteps 9040080, FPS 2179, mean/median reward 13.8/7.0, min/max reward 0.0/64.0, entropy 0.94361, value loss 0.08620, policy loss -0.04020\n",
      "Updates 113100, num timesteps 9048080, FPS 2178, mean/median reward 11.2/6.0, min/max reward 0.0/77.0, entropy 0.86380, value loss 0.07669, policy loss 0.02348\n",
      "Updates 113200, num timesteps 9056080, FPS 2178, mean/median reward 21.3/6.0, min/max reward 0.0/90.0, entropy 0.95199, value loss 0.04491, policy loss 0.04920\n",
      "Updates 113300, num timesteps 9064080, FPS 2179, mean/median reward 25.6/19.0, min/max reward 0.0/63.0, entropy 0.93538, value loss 0.02270, policy loss -0.01734\n",
      "Updates 113400, num timesteps 9072080, FPS 2179, mean/median reward 16.4/10.0, min/max reward 0.0/54.0, entropy 1.07021, value loss 0.04950, policy loss 0.05953\n",
      "Updates 113500, num timesteps 9080080, FPS 2179, mean/median reward 13.5/4.0, min/max reward 0.0/83.0, entropy 0.94492, value loss 0.12230, policy loss -0.11844\n",
      "Updates 113600, num timesteps 9088080, FPS 2179, mean/median reward 16.0/6.0, min/max reward 0.0/83.0, entropy 0.88175, value loss 0.11025, policy loss 0.00186\n",
      "Updates 113700, num timesteps 9096080, FPS 2179, mean/median reward 16.4/10.0, min/max reward 0.0/56.0, entropy 0.83719, value loss 0.03067, policy loss -0.01092\n",
      "Updates 113800, num timesteps 9104080, FPS 2179, mean/median reward 11.9/4.0, min/max reward 0.0/61.0, entropy 1.01438, value loss 0.01977, policy loss 0.03403\n",
      "Updates 113900, num timesteps 9112080, FPS 2179, mean/median reward 24.0/9.0, min/max reward 0.0/92.0, entropy 0.94576, value loss 0.10714, policy loss 0.04541\n",
      "Updates 114000, num timesteps 9120080, FPS 2179, mean/median reward 24.0/13.0, min/max reward 0.0/88.0, entropy 0.95687, value loss 0.01745, policy loss -0.00868\n",
      "Updates 114100, num timesteps 9128080, FPS 2179, mean/median reward 17.6/10.0, min/max reward 0.0/75.0, entropy 1.03448, value loss 0.04534, policy loss -0.01585\n",
      "Updates 114200, num timesteps 9136080, FPS 2179, mean/median reward 14.2/3.0, min/max reward 0.0/83.0, entropy 1.05376, value loss 0.09476, policy loss 0.01653\n",
      "Updates 114300, num timesteps 9144080, FPS 2179, mean/median reward 17.1/7.0, min/max reward 0.0/63.0, entropy 0.85857, value loss 1.06798, policy loss -0.03722\n",
      "Updates 114400, num timesteps 9152080, FPS 2179, mean/median reward 14.2/2.0, min/max reward 0.0/70.0, entropy 0.97420, value loss 0.08524, policy loss -0.06279\n",
      "Updates 114500, num timesteps 9160080, FPS 2180, mean/median reward 14.6/2.0, min/max reward 0.0/70.0, entropy 0.93768, value loss 0.00891, policy loss 0.00520\n",
      "Updates 114600, num timesteps 9168080, FPS 2179, mean/median reward 10.7/2.0, min/max reward 0.0/65.0, entropy 0.96283, value loss 0.08060, policy loss 0.07473\n",
      "Updates 114700, num timesteps 9176080, FPS 2180, mean/median reward 8.4/2.0, min/max reward 0.0/65.0, entropy 0.97495, value loss 0.06227, policy loss -0.00718\n",
      "Updates 114800, num timesteps 9184080, FPS 2180, mean/median reward 16.3/3.0, min/max reward 0.0/65.0, entropy 0.92117, value loss 0.08543, policy loss 0.04420\n",
      "Updates 114900, num timesteps 9192080, FPS 2180, mean/median reward 20.2/9.0, min/max reward 0.0/86.0, entropy 1.11255, value loss 0.08514, policy loss -0.06375\n",
      "Updates 115000, num timesteps 9200080, FPS 2180, mean/median reward 13.3/7.0, min/max reward 0.0/65.0, entropy 1.01675, value loss 0.31701, policy loss -0.12723\n",
      "Updates 115100, num timesteps 9208080, FPS 2180, mean/median reward 26.0/7.0, min/max reward 0.0/76.0, entropy 1.03873, value loss 0.06120, policy loss 0.04600\n",
      "Updates 115200, num timesteps 9216080, FPS 2180, mean/median reward 25.6/23.0, min/max reward 0.0/76.0, entropy 0.85398, value loss 0.01796, policy loss -0.02150\n",
      "Updates 115300, num timesteps 9224080, FPS 2180, mean/median reward 24.9/9.0, min/max reward 0.0/93.0, entropy 0.95227, value loss 0.09921, policy loss -0.02541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 115400, num timesteps 9232080, FPS 2180, mean/median reward 25.8/5.0, min/max reward 0.0/99.0, entropy 0.99491, value loss 0.02802, policy loss 0.03407\n",
      "Updates 115500, num timesteps 9240080, FPS 2180, mean/median reward 16.3/2.0, min/max reward 0.0/90.0, entropy 0.90541, value loss 0.08655, policy loss -0.00077\n",
      "Updates 115600, num timesteps 9248080, FPS 2180, mean/median reward 11.0/2.0, min/max reward 0.0/42.0, entropy 0.96749, value loss 0.04544, policy loss 0.00986\n",
      "Updates 115700, num timesteps 9256080, FPS 2180, mean/median reward 18.9/10.0, min/max reward 0.0/50.0, entropy 0.79905, value loss 0.67114, policy loss -0.15880\n",
      "Updates 115800, num timesteps 9264080, FPS 2181, mean/median reward 24.8/14.0, min/max reward 0.0/100.0, entropy 1.15324, value loss 0.02172, policy loss -0.06898\n",
      "Updates 115900, num timesteps 9272080, FPS 2181, mean/median reward 10.5/3.0, min/max reward 0.0/37.0, entropy 0.96830, value loss 0.03060, policy loss 0.02087\n",
      "Updates 116000, num timesteps 9280080, FPS 2181, mean/median reward 14.3/2.0, min/max reward 0.0/102.0, entropy 0.84608, value loss 0.08711, policy loss -0.00730\n",
      "Updates 116100, num timesteps 9288080, FPS 2181, mean/median reward 11.2/2.0, min/max reward 0.0/87.0, entropy 0.78704, value loss 0.03299, policy loss 0.01207\n",
      "Updates 116200, num timesteps 9296080, FPS 2181, mean/median reward 12.5/1.0, min/max reward 0.0/79.0, entropy 0.99681, value loss 0.03728, policy loss -0.03105\n",
      "Updates 116300, num timesteps 9304080, FPS 2181, mean/median reward 19.3/6.0, min/max reward 0.0/96.0, entropy 1.00994, value loss 0.18553, policy loss 0.00220\n",
      "Updates 116400, num timesteps 9312080, FPS 2181, mean/median reward 18.4/3.0, min/max reward 0.0/83.0, entropy 1.03191, value loss 0.01289, policy loss -0.03198\n",
      "Updates 116500, num timesteps 9320080, FPS 2181, mean/median reward 13.6/6.0, min/max reward 0.0/105.0, entropy 0.88967, value loss 0.21224, policy loss -0.16011\n",
      "Updates 116600, num timesteps 9328080, FPS 2181, mean/median reward 17.5/5.0, min/max reward 0.0/105.0, entropy 0.94041, value loss 0.02612, policy loss -0.00030\n",
      "Updates 116700, num timesteps 9336080, FPS 2181, mean/median reward 8.9/3.0, min/max reward 0.0/28.0, entropy 0.81657, value loss 0.04556, policy loss 0.00485\n",
      "Updates 116800, num timesteps 9344080, FPS 2181, mean/median reward 30.0/15.0, min/max reward 0.0/101.0, entropy 0.93805, value loss 0.08299, policy loss 0.03338\n",
      "Updates 116900, num timesteps 9352080, FPS 2182, mean/median reward 23.9/10.0, min/max reward 0.0/78.0, entropy 0.99561, value loss 0.12285, policy loss -0.14715\n",
      "Updates 117000, num timesteps 9360080, FPS 2182, mean/median reward 29.8/15.0, min/max reward 0.0/91.0, entropy 0.90265, value loss 0.11109, policy loss -0.03494\n",
      "Updates 117100, num timesteps 9368080, FPS 2182, mean/median reward 31.0/14.0, min/max reward 0.0/91.0, entropy 0.97522, value loss 0.06872, policy loss -0.01933\n",
      "Updates 117200, num timesteps 9376080, FPS 2182, mean/median reward 16.5/9.0, min/max reward 0.0/96.0, entropy 1.01021, value loss 0.01589, policy loss 0.00445\n",
      "Updates 117300, num timesteps 9384080, FPS 2182, mean/median reward 14.5/4.0, min/max reward 0.0/61.0, entropy 0.99703, value loss 0.10944, policy loss 0.01603\n",
      "Updates 117400, num timesteps 9392080, FPS 2182, mean/median reward 18.8/12.0, min/max reward 0.0/71.0, entropy 0.84271, value loss 0.18529, policy loss -0.02013\n",
      "Updates 117500, num timesteps 9400080, FPS 2182, mean/median reward 10.7/5.0, min/max reward 0.0/48.0, entropy 0.89637, value loss 0.36557, policy loss -0.13950\n",
      "Updates 117600, num timesteps 9408080, FPS 2182, mean/median reward 13.1/8.0, min/max reward 0.0/73.0, entropy 0.88399, value loss 0.05342, policy loss 0.02947\n",
      "Updates 117700, num timesteps 9416080, FPS 2182, mean/median reward 22.5/11.0, min/max reward 0.0/88.0, entropy 0.91036, value loss 0.07101, policy loss -0.00541\n",
      "Updates 117800, num timesteps 9424080, FPS 2182, mean/median reward 12.3/11.0, min/max reward 0.0/59.0, entropy 0.84226, value loss 0.35258, policy loss 0.18403\n",
      "Updates 117900, num timesteps 9432080, FPS 2182, mean/median reward 17.1/11.0, min/max reward 0.0/59.0, entropy 0.86290, value loss 0.01202, policy loss -0.03039\n",
      "Updates 118000, num timesteps 9440080, FPS 2183, mean/median reward 14.2/10.0, min/max reward 0.0/62.0, entropy 0.89583, value loss 0.07780, policy loss -0.02271\n",
      "Updates 118100, num timesteps 9448080, FPS 2183, mean/median reward 9.1/5.0, min/max reward 0.0/38.0, entropy 0.88148, value loss 0.25489, policy loss -0.01865\n",
      "Updates 118200, num timesteps 9456080, FPS 2183, mean/median reward 14.9/2.0, min/max reward 0.0/108.0, entropy 0.77539, value loss 0.03067, policy loss 0.01946\n",
      "Updates 118300, num timesteps 9464080, FPS 2183, mean/median reward 16.1/4.0, min/max reward 0.0/71.0, entropy 1.07861, value loss 0.01973, policy loss -0.01789\n",
      "Updates 118400, num timesteps 9472080, FPS 2183, mean/median reward 20.6/7.0, min/max reward 0.0/92.0, entropy 1.02225, value loss 0.06497, policy loss 0.04745\n",
      "Updates 118500, num timesteps 9480080, FPS 2183, mean/median reward 11.1/5.0, min/max reward 0.0/44.0, entropy 0.89596, value loss 0.22061, policy loss 0.08024\n",
      "Updates 118600, num timesteps 9488080, FPS 2183, mean/median reward 25.4/15.0, min/max reward 3.0/95.0, entropy 0.80668, value loss 0.02526, policy loss 0.04097\n",
      "Updates 118700, num timesteps 9496080, FPS 2183, mean/median reward 16.0/9.0, min/max reward 3.0/64.0, entropy 0.99445, value loss 0.05944, policy loss -0.04316\n",
      "Updates 118800, num timesteps 9504080, FPS 2183, mean/median reward 24.7/15.0, min/max reward 0.0/93.0, entropy 0.86844, value loss 1.76544, policy loss -0.04225\n",
      "Updates 118900, num timesteps 9512080, FPS 2183, mean/median reward 33.8/10.0, min/max reward 0.0/99.0, entropy 0.99495, value loss 0.02570, policy loss -0.02193\n",
      "Updates 119000, num timesteps 9520080, FPS 2183, mean/median reward 23.2/4.0, min/max reward 0.0/93.0, entropy 0.96561, value loss 0.04099, policy loss 0.05042\n",
      "Updates 119100, num timesteps 9528080, FPS 2184, mean/median reward 20.5/4.0, min/max reward 0.0/79.0, entropy 1.03024, value loss 0.02094, policy loss -0.00242\n",
      "Updates 119200, num timesteps 9536080, FPS 2184, mean/median reward 14.7/7.0, min/max reward 0.0/79.0, entropy 1.00508, value loss 0.00691, policy loss -0.01512\n",
      "Updates 119300, num timesteps 9544080, FPS 2184, mean/median reward 12.9/7.0, min/max reward 2.0/53.0, entropy 1.06101, value loss 0.01917, policy loss -0.03073\n",
      "Updates 119400, num timesteps 9552080, FPS 2184, mean/median reward 16.6/10.0, min/max reward 2.0/51.0, entropy 0.99805, value loss 0.02343, policy loss 0.02342\n",
      "Updates 119500, num timesteps 9560080, FPS 2184, mean/median reward 13.3/4.0, min/max reward 0.0/62.0, entropy 0.99104, value loss 0.01874, policy loss 0.00837\n",
      "Updates 119600, num timesteps 9568080, FPS 2184, mean/median reward 18.9/10.0, min/max reward 1.0/82.0, entropy 0.97419, value loss 0.32085, policy loss -0.06990\n",
      "Updates 119700, num timesteps 9576080, FPS 2184, mean/median reward 14.0/10.0, min/max reward 1.0/39.0, entropy 0.97856, value loss 0.11379, policy loss -0.02423\n",
      "Updates 119800, num timesteps 9584080, FPS 2184, mean/median reward 19.4/11.0, min/max reward 0.0/102.0, entropy 1.09757, value loss 0.06293, policy loss -0.02049\n",
      "Updates 119900, num timesteps 9592080, FPS 2184, mean/median reward 17.7/3.0, min/max reward 0.0/82.0, entropy 0.93377, value loss 0.06393, policy loss -0.02210\n",
      "Updates 120000, num timesteps 9600080, FPS 2184, mean/median reward 24.3/17.0, min/max reward 1.0/92.0, entropy 0.96218, value loss 0.03898, policy loss 0.08253\n",
      "Updates 120100, num timesteps 9608080, FPS 2185, mean/median reward 20.9/21.0, min/max reward 0.0/71.0, entropy 1.01442, value loss 0.59994, policy loss 0.03334\n",
      "Updates 120200, num timesteps 9616080, FPS 2185, mean/median reward 14.4/5.0, min/max reward 0.0/71.0, entropy 0.86909, value loss 1.90439, policy loss -0.42132\n",
      "Updates 120300, num timesteps 9624080, FPS 2185, mean/median reward 16.1/4.0, min/max reward 0.0/68.0, entropy 1.04169, value loss 0.02716, policy loss 0.06618\n",
      "Updates 120400, num timesteps 9632080, FPS 2185, mean/median reward 17.7/9.0, min/max reward 0.0/82.0, entropy 1.03317, value loss 0.02698, policy loss 0.00082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 120500, num timesteps 9640080, FPS 2185, mean/median reward 15.6/7.0, min/max reward 0.0/75.0, entropy 1.16816, value loss 0.01599, policy loss 0.03873\n",
      "Updates 120600, num timesteps 9648080, FPS 2185, mean/median reward 8.1/5.0, min/max reward 0.0/29.0, entropy 0.91804, value loss 0.76804, policy loss 0.04362\n",
      "Updates 120700, num timesteps 9656080, FPS 2185, mean/median reward 13.5/5.0, min/max reward 0.0/56.0, entropy 1.00234, value loss 0.01911, policy loss -0.03609\n",
      "Updates 120800, num timesteps 9664080, FPS 2185, mean/median reward 12.5/5.0, min/max reward 0.0/78.0, entropy 0.93060, value loss 0.03610, policy loss 0.03270\n",
      "Updates 120900, num timesteps 9672080, FPS 2185, mean/median reward 11.2/4.0, min/max reward 0.0/45.0, entropy 0.96137, value loss 0.08656, policy loss 0.03911\n",
      "Updates 121000, num timesteps 9680080, FPS 2185, mean/median reward 19.4/4.0, min/max reward 0.0/86.0, entropy 0.99571, value loss 0.02908, policy loss -0.05409\n",
      "Updates 121100, num timesteps 9688080, FPS 2185, mean/median reward 24.6/4.0, min/max reward 0.0/79.0, entropy 1.00169, value loss 0.05037, policy loss -0.01528\n",
      "Updates 121200, num timesteps 9696080, FPS 2186, mean/median reward 17.4/5.0, min/max reward 0.0/99.0, entropy 1.00779, value loss 0.08628, policy loss -0.06276\n",
      "Updates 121300, num timesteps 9704080, FPS 2185, mean/median reward 11.5/3.0, min/max reward 0.0/83.0, entropy 1.05477, value loss 0.05772, policy loss 0.06600\n",
      "Updates 121400, num timesteps 9712080, FPS 2186, mean/median reward 18.3/3.0, min/max reward 0.0/91.0, entropy 0.98630, value loss 0.01747, policy loss -0.02814\n",
      "Updates 121500, num timesteps 9720080, FPS 2186, mean/median reward 16.1/10.0, min/max reward 0.0/50.0, entropy 1.08358, value loss 0.05501, policy loss 0.00371\n",
      "Updates 121600, num timesteps 9728080, FPS 2186, mean/median reward 18.4/6.0, min/max reward 0.0/90.0, entropy 0.94505, value loss 0.04098, policy loss -0.00980\n",
      "Updates 121700, num timesteps 9736080, FPS 2186, mean/median reward 14.8/4.0, min/max reward 0.0/69.0, entropy 0.86709, value loss 1.25815, policy loss -0.22517\n",
      "Updates 121800, num timesteps 9744080, FPS 2186, mean/median reward 16.1/4.0, min/max reward 0.0/72.0, entropy 0.98064, value loss 0.15572, policy loss 0.09330\n",
      "Updates 121900, num timesteps 9752080, FPS 2186, mean/median reward 17.1/4.0, min/max reward 0.0/62.0, entropy 0.96080, value loss 0.23163, policy loss -0.07005\n",
      "Updates 122000, num timesteps 9760080, FPS 2186, mean/median reward 11.8/4.0, min/max reward 0.0/56.0, entropy 0.92459, value loss 0.05997, policy loss 0.00080\n",
      "Updates 122100, num timesteps 9768080, FPS 2186, mean/median reward 15.1/4.0, min/max reward 0.0/94.0, entropy 0.91060, value loss 0.54687, policy loss -0.08112\n",
      "Updates 122200, num timesteps 9776080, FPS 2186, mean/median reward 30.1/17.0, min/max reward 0.0/103.0, entropy 0.82076, value loss 0.19442, policy loss -0.01774\n",
      "Updates 122300, num timesteps 9784080, FPS 2186, mean/median reward 17.8/13.0, min/max reward 0.0/94.0, entropy 0.87794, value loss 0.43886, policy loss -0.18666\n",
      "Updates 122400, num timesteps 9792080, FPS 2186, mean/median reward 22.1/15.0, min/max reward 0.0/81.0, entropy 1.09281, value loss 0.05819, policy loss -0.03597\n",
      "Updates 122500, num timesteps 9800080, FPS 2186, mean/median reward 16.9/6.0, min/max reward 0.0/81.0, entropy 1.03394, value loss 0.04886, policy loss 0.08658\n",
      "Updates 122600, num timesteps 9808080, FPS 2186, mean/median reward 22.3/15.0, min/max reward 0.0/81.0, entropy 0.87320, value loss 0.55667, policy loss -0.10737\n",
      "Updates 122700, num timesteps 9816080, FPS 2186, mean/median reward 19.9/10.0, min/max reward 0.0/71.0, entropy 1.11306, value loss 0.02217, policy loss -0.00088\n",
      "Updates 122800, num timesteps 9824080, FPS 2186, mean/median reward 15.4/6.0, min/max reward 1.0/66.0, entropy 1.01515, value loss 0.07217, policy loss 0.02655\n",
      "Updates 122900, num timesteps 9832080, FPS 2186, mean/median reward 10.8/6.0, min/max reward 0.0/44.0, entropy 0.95761, value loss 0.04809, policy loss -0.01132\n",
      "Updates 123000, num timesteps 9840080, FPS 2186, mean/median reward 15.7/4.0, min/max reward 0.0/95.0, entropy 1.13159, value loss 0.08403, policy loss 0.02412\n",
      "Updates 123100, num timesteps 9848080, FPS 2186, mean/median reward 12.1/4.0, min/max reward 1.0/39.0, entropy 1.01901, value loss 0.21278, policy loss -0.06946\n",
      "Updates 123200, num timesteps 9856080, FPS 2186, mean/median reward 13.8/6.0, min/max reward 0.0/39.0, entropy 0.97704, value loss 0.11348, policy loss 0.05028\n",
      "Updates 123300, num timesteps 9864080, FPS 2186, mean/median reward 10.8/3.0, min/max reward 0.0/57.0, entropy 0.88627, value loss 0.04711, policy loss -0.00076\n",
      "Updates 123400, num timesteps 9872080, FPS 2186, mean/median reward 8.2/2.0, min/max reward 0.0/40.0, entropy 1.02970, value loss 0.04063, policy loss -0.01964\n",
      "Updates 123500, num timesteps 9880080, FPS 2186, mean/median reward 15.8/2.0, min/max reward 1.0/85.0, entropy 0.95846, value loss 0.05613, policy loss -0.02952\n",
      "Updates 123600, num timesteps 9888080, FPS 2186, mean/median reward 19.1/8.0, min/max reward 2.0/85.0, entropy 1.05324, value loss 0.25100, policy loss -0.10042\n",
      "Updates 123700, num timesteps 9896080, FPS 2186, mean/median reward 16.5/6.0, min/max reward 1.0/48.0, entropy 0.98835, value loss 0.06855, policy loss 0.04037\n",
      "Updates 123800, num timesteps 9904080, FPS 2186, mean/median reward 13.8/8.0, min/max reward 0.0/46.0, entropy 1.00042, value loss 0.03855, policy loss 0.00584\n",
      "Updates 123900, num timesteps 9912080, FPS 2186, mean/median reward 10.6/2.0, min/max reward 0.0/51.0, entropy 0.83521, value loss 0.08981, policy loss 0.06357\n",
      "Updates 124000, num timesteps 9920080, FPS 2186, mean/median reward 25.7/13.0, min/max reward 0.0/95.0, entropy 1.02715, value loss 0.03618, policy loss 0.04552\n",
      "Updates 124100, num timesteps 9928080, FPS 2186, mean/median reward 8.7/3.0, min/max reward 0.0/51.0, entropy 0.92655, value loss 0.07874, policy loss -0.04137\n",
      "Updates 124200, num timesteps 9936080, FPS 2187, mean/median reward 9.2/4.0, min/max reward 0.0/51.0, entropy 1.04403, value loss 0.03682, policy loss -0.03930\n",
      "Updates 124300, num timesteps 9944080, FPS 2187, mean/median reward 31.4/9.0, min/max reward 0.0/98.0, entropy 1.01235, value loss 0.07503, policy loss 0.12759\n",
      "Updates 124400, num timesteps 9952080, FPS 2187, mean/median reward 24.9/9.0, min/max reward 0.0/98.0, entropy 1.05297, value loss 1.12885, policy loss -0.12828\n",
      "Updates 124500, num timesteps 9960080, FPS 2187, mean/median reward 16.4/4.0, min/max reward 0.0/98.0, entropy 1.06594, value loss 0.01585, policy loss -0.01877\n",
      "Updates 124600, num timesteps 9968080, FPS 2187, mean/median reward 8.7/5.0, min/max reward 0.0/33.0, entropy 1.01879, value loss 0.04144, policy loss 0.02696\n",
      "Updates 124700, num timesteps 9976080, FPS 2187, mean/median reward 18.3/8.0, min/max reward 0.0/84.0, entropy 0.93308, value loss 0.03152, policy loss -0.02187\n",
      "Updates 124800, num timesteps 9984080, FPS 2187, mean/median reward 14.1/9.0, min/max reward 0.0/50.0, entropy 0.96479, value loss 0.12337, policy loss -0.02549\n",
      "Updates 124900, num timesteps 9992080, FPS 2187, mean/median reward 15.6/9.0, min/max reward 0.0/59.0, entropy 1.10459, value loss 0.04179, policy loss 0.07910\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "start = time.time()\n",
    "for j in range(num_updates):\n",
    "    for step in range(num_steps):\n",
    "        # Sample actions\n",
    "        value, action, action_log_prob, states = actor_critic.act(Variable(rollouts.observations[step], volatile=True),\n",
    "                                                              Variable(rollouts.states[step], volatile=True),\n",
    "                                                              Variable(rollouts.masks[step], volatile=True))\n",
    "        cpu_actions = action.data.squeeze().cpu().numpy()\n",
    "        #print(cpu_action)\n",
    "        \n",
    "        # obser reward and next obs\n",
    "        obs, reward, done, info = envs.step(cpu_actions)\n",
    "        # stack: make sure that reward is a numpy array(convert list to ndarray)\n",
    "        reward = torch.from_numpy(np.expand_dims(np.stack(reward), 1)).float()\n",
    "        episode_rewards += reward\n",
    "        \n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1 - masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "        if cuda:\n",
    "            masks = masks.cuda()\n",
    "\n",
    "        if current_obs.dim() == 4:\n",
    "            current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "        else:\n",
    "            current_obs *= masks\n",
    "        \n",
    "        # update obs nad rollouts\n",
    "        update_current_obs(obs)\n",
    "        rollouts.insert(step, current_obs, states.data, action.data, action_log_prob.data, value.data, reward, masks)\n",
    "    \n",
    "    # compute current update's return\n",
    "    next_value = actor_critic(Variable(rollouts.observations[-1], volatile=True),\n",
    "                              Variable(rollouts.states[-1], volatile=True),\n",
    "                              Variable(rollouts.masks[-1], volatile=True))[0].data\n",
    "\n",
    "    rollouts.compute_returns(next_value, False, gamma, tau)\n",
    "    \n",
    "    # in a2c the values  were calculated twice\n",
    "    # the data in rollouts must be viewed, because the shape in rollouts is [num_steps, num_processes, x] which is [num,x] in actor_critic\n",
    "    values, action_log_probs, dist_entropy, states = actor_critic.evaluate_actions(Variable(rollouts.observations[:-1].view(-1, *obs_shape)),\n",
    "                                                                                   Variable(rollouts.states[0].view(-1, actor_critic.state_size)),\n",
    "                                                                                   Variable(rollouts.masks[:-1].view(-1, 1)),\n",
    "                                                                                   Variable(rollouts.actions.view(-1, action_shape)))\n",
    "    \n",
    "    # compute the loss\n",
    "    values = values.view(num_steps, num_processes, 1)\n",
    "    action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "    advantages = Variable(rollouts.returns[:-1]) - values\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "\n",
    "    action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "    \n",
    "    # update model\n",
    "    optimizer.zero_grad()\n",
    "    loss = value_loss * value_loss_coef + action_loss - dist_entropy * entropy_coef \n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    rollouts.after_update()\n",
    "    if j % log_interval == 0:\n",
    "        end = time.time()\n",
    "        total_num_steps = (j + 1) * num_processes * num_steps\n",
    "        print(\"Updates {}, num timesteps {}, FPS {}, mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, entropy {:.5f}, value loss {:.5f}, policy loss {:.5f}\".\n",
    "        format(j, total_num_steps,\n",
    "                int(total_num_steps / (end - start)),\n",
    "                final_rewards.mean(),\n",
    "                final_rewards.median(),\n",
    "                final_rewards.min(),\n",
    "                final_rewards.max(), dist_entropy.data[0],\n",
    "                value_loss.data[0], action_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(actor_critic, '../../models/'+env_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
